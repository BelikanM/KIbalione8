# ========================================
# Configuration pour supprimer les warnings TensorFlow et CUDA
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Supprime tous les logs sauf les erreurs fatales
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # D√©sactive les optimisations oneDNN
os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '3'  # Supprime les logs verbeux
os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices=false'  # D√©sactive XLA
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # D√©sactive compl√®tement CUDA pour TensorFlow
# Configuration pour acc√©l√©rer les t√©l√©chargements avec hf-transfer
# IMPORTANT: Doit √™tre d√©fini AVANT l'import de huggingface_hub
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"  # Active hf-transfer (bas√© sur Rust, pas aria2)
# Optimisation CPU - Limiter les threads pour √©viter surchauffe
os.environ['OMP_NUM_THREADS'] = '4'  # Limite OpenMP √† 4 threads
os.environ['MKL_NUM_THREADS'] = '4'  # Limite MKL √† 4 threads
os.environ['NUMEXPR_NUM_THREADS'] = '4'  # Limite NumExpr √† 4 threads
os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # D√©sactive parall√©lisme tokenizers

import math
import gc  # Garbage collector pour lib√©rer m√©moire
import fitz  # pymupdf
import osmium
import networkx as nx
import streamlit as st
import matplotlib.pyplot as plt
import numpy as np
import pickle
import json
from huggingface_hub import InferenceClient
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
from shapely.geometry import Point
import io
from PIL import Image
import cv2
import open3d as o3d
from io import BytesIO
import pandas as pd
from skimage import measure, segmentation
from sklearn.cluster import KMeans
import torch
# Nouveaux imports pour extraction PDF/OCR/YOLO
import pytesseract
import whisper
from gtts import gTTS
import speech_recognition as sr
from ultralytics import YOLO
import time
import shutil
# Optimisation CPU - Limiter les threads torch
torch.set_num_threads(4)  # Maximum 4 threads pour √©viter surchauffe
# Note: set_num_interop_threads retir√© car cause RuntimeError si appel√© apr√®s init parall√®le
from torchvision import models, transforms
from langchain_huggingface import HuggingFaceEndpoint
# Import des agents LangChain 1.0+ / LangGraph V1.0+
create_react_agent = None
try:
    # LangGraph V1.0+ : create_agent dans langchain.agents
    from langchain.agents import create_agent as create_react_agent
    print("‚úÖ Agents LangChain 1.0+ import√©s avec succ√®s")
except ImportError as e:
    print(f"‚ö†Ô∏è Agents non disponibles ({e}) - Mode simplifi√© activ√©")
    
from langchain_core.prompts import PromptTemplate
from langchain_core.tools import Tool
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_community.utilities import DuckDuckGoSearchAPIWrapper
from transformers import pipeline
import requests
from bs4 import BeautifulSoup
import time
import shutil
# Import conditionnel pour √©viter les conflits xformers/diffusers
try:
    from diffusers import DiffusionPipeline, AudioLDMPipeline, ShapEPipeline, ShapEImg2ImgPipeline
    DIFFUSERS_AVAILABLE = True
except Exception as e:
    print(f"‚ö†Ô∏è Diffusers non disponible (conflit xformers): {e}")
    DiffusionPipeline = None
    AudioLDMPipeline = None 
    ShapEPipeline = None
    ShapEImg2ImgPipeline = None
    DIFFUSERS_AVAILABLE = False
import imageio
import scipy.io.wavfile as wavfile
from tavily import TavilyClient
import os
from pathlib import Path
from dotenv import load_dotenv
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import MultinomialNB
import re
from transformers import AutoTokenizer, AutoModelForCausalLM
from langchain_huggingface import HuggingFacePipeline

# Configuration des tokens d√©j√† faite plus haut
# Charger le token depuis .env dans le dossier corrig√©
PROJECT_DIR = os.path.expanduser('~/RAG_ChatBot')
env_path = os.path.join(PROJECT_DIR, ".env")
if os.path.exists(env_path):
    load_dotenv(env_path)
else:
    # Essayer le r√©pertoire courant
    load_dotenv()

HF_TOKEN = os.getenv("HF_TOKEN")
if not HF_TOKEN:
    # Pour √©viter le crash, utiliser un token vide
    HF_TOKEN = ""
    print("‚ö†Ô∏è HF_TOKEN non trouv√© ! Certaines fonctionnalit√©s seront limit√©es")

TAVILY_API_KEY = os.getenv("TAVILY_API_KEY", "")

# D√©finir les variables d'environnement
os.environ["HF_TOKEN"] = HF_TOKEN
os.environ["HUGGINGFACE_HUB_TOKEN"] = HF_TOKEN

# Int√©gration du code ERT/Binary analysis
import struct, re, io
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from scipy import stats
import zlib
import math
import time
from collections import Counter
from safetensors.torch import load_file
import torch
from pathlib import Path
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
class SentenceTransformerEmbeddings:
    def __init__(self, model_name, device='cpu'):
        self.model = SentenceTransformer(model_name, device=device)
  
    def embed_documents(self, texts):
        return self.model.encode(texts, convert_to_numpy=True).tolist()
  
    def embed_query(self, text):
        return self.model.encode([text], convert_to_numpy=True)[0].tolist()
from langchain_community.vectorstores import FAISS
from langchain_tavily import TavilySearch as TavilySearchResults
from typing import Dict, Any
from transformers import AutoModelForCausalLM, AutoTokenizer
from langchain_core.documents import Document
from pdf2image import convert_from_path
import pytesseract
# Import des biblioth√®ques sp√©cialis√©es ERT
try:
    import pygimli as pg
    PYGIMLI_AVAILABLE = True
    print("‚úÖ PyGIMLI disponible pour analyses ERT avanc√©es")
except ImportError:
    PYGIMLI_AVAILABLE = False
    print("‚ö†Ô∏è PyGIMLI non disponible - analyses ERT limit√©es")
# ResIPy sera import√© seulement quand n√©cessaire pour √©viter les erreurs de compatibilit√© NumPy
RESIPY_AVAILABLE = False
from langchain.agents import create_agent
from langchain_core.prompts import PromptTemplate
from langchain_core.tools import Tool
from langchain_core.language_models import BaseChatModel
from typing import Optional, List, Any, Iterator
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage

# Classe ChatModel personnalis√©e pour LangChain utilisant Qwen2.5-1.5B
class QwenChatModel(BaseChatModel):
    tokenizer: AutoTokenizer = None
    model: AutoModelForCausalLM = None
    tools_available: bool = True
   
    def __init__(self, tokenizer, model):
        super().__init__()
        self.tokenizer = tokenizer
        self.model = model
        self.tools_available = True
       
    @property
    def _llm_type(self) -> str:
        return "qwen2.5-1.5b-local-enhanced"
    def _generate(self, messages, stop=None, run_manager=None, **kwargs):
        """Generate a response using tools and analyses."""
        # Extraire le contenu du message utilisateur
        user_message = ""
        for message in messages:
            if isinstance(message, HumanMessage):
                user_message = message.content
                break
       
        # D√©tecter si l'utilisateur demande une analyse
        needs_analysis = any(keyword in user_message.lower() for keyword in [
            "analyse", "resistivit√©", "ert", "recherche", "donn√©es", "mat√©riaux",
            "couleurs", "graphique", "tableau", "comparaison", "approfondie"
        ])
       
        if needs_analysis and self.tools_available:
            # Utiliser les outils disponibles pour une analyse compl√®te
            try:
                # Recherche web pour informations
                if any(keyword in user_message.lower() for keyword in ["recherche", "informations", "approfondie"]):
                    search_query = user_message.replace("fais maintenant une recherche plus approfondie pour obtenir toutes ces informations pr√©cises", "")
                    web_results = web_search_enhanced(search_query + " ERT electrical resistivity geophysics materials")
                   
                # Recherche RAG si disponible
                rag_results = ""
                if st.session_state.vectorstore:
                    rag_results = search_vectorstore(user_message)
               
                # G√©n√©ration de donn√©es et analyses si demand√©es
                analysis_results = ""
                if any(keyword in user_message.lower() for keyword in ["tableau", "graphique", "donn√©es"]):
                    # Simuler des donn√©es ERT pour d√©monstration
                    import numpy as np
                    sample_data = [0.05, 0.3, 10.0, 50.0, 200.0, 1000.0, 5000.0, 0.0000024, 1000000]
                    analysis_results = resistivity_color_analysis(sample_data)
               
                # Construire la r√©ponse enrichie avec outils
                enhanced_context = f"""
üîç ANALYSE COMPL√àTE AVEC OUTILS ACTIV√âS:
üåê RECHERCHE WEB EFFECTU√âE:
{web_results}
üìö RECHERCHE RAG:
{rag_results}
üìä ANALYSE ERT AVANC√âE:
{analysis_results}
CONTEXTE UTILISATEUR: {user_message}
"""
               
                # G√©n√©rer la r√©ponse avec le contexte enrichi
                enhanced_messages = [
                    {"role": "system", "content": """Tu es un expert en g√©ophysique ERT avec acc√®s √† des outils puissants.
                    Tu DOIS utiliser les donn√©es fournies pour cr√©er des analyses d√©taill√©es, tableaux, graphiques et comparaisons.
                    R√©ponds toujours avec des donn√©es concr√®tes et des analyses approfondies bas√©es sur les outils utilis√©s.
                    Ne dis JAMAIS que tu n'as pas acc√®s aux outils - utilise les r√©sultats fournis."""},
                    {"role": "user", "content": enhanced_context}
                ]
            except Exception as e:
                print(f"Erreur outils: {e}")
                enhanced_messages = [
                    {"role": "system", "content": "Tu es un expert en analyse de donn√©es ERT."},
                    {"role": "user", "content": user_message}
                ]
        else:
            # Messages standard
            enhanced_messages = []
            for message in messages:
                if isinstance(message, SystemMessage):
                    enhanced_messages.append({"role": "system", "content": message.content})
                elif isinstance(message, HumanMessage):
                    enhanced_messages.append({"role": "user", "content": message.content})
                elif isinstance(message, AIMessage):
                    enhanced_messages.append({"role": "assistant", "content": message.content})
       
        # G√©n√©ration avec les messages enrichis
        inputs = self.tokenizer.apply_chat_template(
            enhanced_messages,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(self.model.device)
       
        attention_mask = (inputs != self.tokenizer.pad_token_id).long()
       
        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                attention_mask=attention_mask,
                max_new_tokens=2000, # Plus de tokens pour analyses d√©taill√©es
                temperature=0.6,
                do_sample=True,
                top_p=0.9,
                pad_token_id=self.tokenizer.eos_token_id
            )
       
        response = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)
       
        if stop:
            for stop_token in stop:
                if stop_token in response:
                    response = response.split(stop_token)[0]
                    break
       
        return AIMessage(content=response)
    def _stream(self, messages, stop=None, run_manager=None, **kwargs) -> Iterator:
        """Streaming is not implemented for simplicity."""
        yield self._generate(messages, stop, run_manager, **kwargs)

# Chargement du mod√®le LLM compact avec d√©tection GPU optimis√©e
@st.cache_resource
def load_llm_model():
    model_name = "Qwen/Qwen2.5-1.5B-Instruct"
   
    # R√©cup√©rer le token depuis les variables d'environnement
    hf_token = os.getenv("HF_TOKEN", "")
    
    # D√©tection GPU optimis√©e
    device = 'cpu'
    gpu_info = ""
    if torch.cuda.is_available():
        device = 'cuda'
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        gpu_info = f"GPU: {gpu_name} ({gpu_memory:.1f}GB VRAM)"
        print(f"üöÄ GPU d√©tect√©: {gpu_info}")
    else:
        print("üñ•Ô∏è Utilisation du CPU")
   
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True,
        token=hf_token if hf_token else None,
        use_fast=True  # Tokenizer rapide pour r√©duire CPU
    )
    # Corriger le probl√®me du pad_token = eos_token pour √©viter les warnings
    if tokenizer.pad_token is None or tokenizer.pad_token == tokenizer.eos_token:
        tokenizer.pad_token = tokenizer.eos_token
   
    # Configuration optimis√©e selon le device
    if device == 'cuda':
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            torch_dtype=torch.float16, # Optimisation GPU
            trust_remote_code=True,
            token=hf_token if hf_token else None,
            low_cpu_mem_usage=True  # R√©duire utilisation m√©moire CPU
        )
    else:
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float32, # CPU n√©cessite float32
            trust_remote_code=True,
            token=hf_token if hf_token else None,
            low_cpu_mem_usage=True  # R√©duire utilisation m√©moire CPU
        ).to(device)
   
    return tokenizer, model, device, gpu_info

# Chargement au d√©marrage
if "model_loaded" not in st.session_state:
    with st.spinner("üîÑ Chargement du mod√®le LLM (Qwen2.5-1.5B ~1.5GB)..."):
        tokenizer, model, device, gpu_info = load_llm_model()
        # Stocker dans session_state pour acc√®s global
        st.session_state.tokenizer = tokenizer
        st.session_state.model = model
        st.session_state.device = device
        st.session_state.gpu_info = gpu_info
        st.session_state.model_loaded = True
        # Cr√©er l'instance ChatModel pour LangChain
        qwen_llm = QwenChatModel(tokenizer, model)
        st.session_state.qwen_llm = qwen_llm
        success_msg = f"‚úÖ Mod√®le charg√© sur {device.upper()}"
        if gpu_info:
            success_msg += f" - {gpu_info}"
        st.success(success_msg)
else:
    # R√©cup√©rer depuis session_state
    tokenizer = st.session_state.tokenizer
    model = st.session_state.model
    device = st.session_state.device
    gpu_info = st.session_state.gpu_info
    qwen_llm = st.session_state.qwen_llm

# ========================================
# MOD√àLES IA SP√âCIALIS√âS L√âGERS (1-2GB)
# ========================================

@st.cache_resource
def load_code_specialist():
    """Charge un mod√®le sp√©cialis√© en codage (DeepSeek-Coder-1.3B)"""
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        model_name = "deepseek-ai/deepseek-coder-1.3b-instruct"
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if device == 'cuda' else torch.float32,
            device_map="auto" if device == 'cuda' else None,
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        if device == 'cpu':
            model = model.to(device)
        
        print(f"‚úÖ Code Specialist charg√© sur {device}")
        return tokenizer, model, device
    except Exception as e:
        print(f"‚ö†Ô∏è Code Specialist non disponible: {e}")
        return None, None, None

@st.cache_resource
def load_plot_specialist():
    """Charge un mod√®le sp√©cialis√© en g√©n√©ration de code Python pour graphiques"""
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        # Utiliser un mod√®le l√©ger optimis√© pour Python/Data Science
        model_name = "Salesforce/codegen-350M-mono"  # 350MB - Tr√®s l√©ger
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if device == 'cuda' else torch.float32,
            device_map="auto" if device == 'cuda' else None,
            low_cpu_mem_usage=True
        )
        if device == 'cpu':
            model = model.to(device)
            
        print(f"‚úÖ Plot Specialist charg√© sur {device}")
        return tokenizer, model, device
    except Exception as e:
        print(f"‚ö†Ô∏è Plot Specialist non disponible: {e}")
        return None, None, None

# Charger les mod√®les sp√©cialis√©s
if "code_specialist" not in st.session_state:
    code_tok, code_model, code_device = load_code_specialist()
    st.session_state.code_specialist = {
        'tokenizer': code_tok,
        'model': code_model,
        'device': code_device
    }

if "plot_specialist" not in st.session_state:
    plot_tok, plot_model, plot_device = load_plot_specialist()
    st.session_state.plot_specialist = {
        'tokenizer': plot_tok,
        'model': plot_model,
        'device': plot_device
    }

# Fonctions outils utilisant les mod√®les sp√©cialis√©s
def generate_code_with_ai(prompt: str) -> str:
    """G√©n√®re du code avec l'IA sp√©cialis√©e DeepSeek-Coder"""
    specialist = st.session_state.code_specialist
    if specialist['model'] is None:
        return "‚ùå Code Specialist non disponible"
    
    try:
        tokenizer = specialist['tokenizer']
        model = specialist['model']
        device = specialist['device']
        
        full_prompt = f"### Instruction:\n{prompt}\n### Response:\n"
        inputs = tokenizer(full_prompt, return_tensors="pt").to(device)
        
        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.2,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        code = tokenizer.decode(outputs[0], skip_special_tokens=True)
        code = code.split("### Response:")[-1].strip()
        
        return f"```python\n{code}\n```"
    except Exception as e:
        return f"‚ùå Erreur: {e}"

def generate_plot_code(data_description: str, plot_type: str = "auto") -> str:
    """G√©n√®re du code matplotlib/seaborn pour cr√©er un graphique"""
    specialist = st.session_state.plot_specialist
    if specialist['model'] is None:
        return "‚ùå Plot Specialist non disponible"
    
    try:
        tokenizer = specialist['tokenizer']
        model = specialist['model']
        device = specialist['device']
        
        prompt = f"# Create a {plot_type} plot for: {data_description}\nimport matplotlib.pyplot as plt\nimport numpy as np\n"
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.3,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        code = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        return f"```python\n{code}\n```"
    except Exception as e:
        return f"‚ùå Erreur: {e}"

# Outils avanc√©s pour l'agent LangChain (Analyse scientifique)
def entropy_analysis(file_bytes: bytes) -> str:
    """Calcule l'entropie de Shannon pour d√©tecter la compression/randomness"""
    from collections import Counter
    import math
    if not file_bytes:
        return "Fichier vide"
    # Calcul de la fr√©quence des bytes
    freq = Counter(file_bytes)
    total = len(file_bytes)
    # Entropie de Shannon
    entropy = -sum((count/total) * math.log2(count/total) for count in freq.values())
    # Classification
    if entropy < 3:
        classification = "Donn√©es structur√©es/compress√©es"
    elif entropy < 6:
        classification = "Donn√©es mixtes"
    else:
        classification = "Donn√©es al√©atoires/crypt√©es"
    return f"Entropie: {entropy:.2f}/8 bits. Classification: {classification}"
def statistical_analysis(numbers: list) -> str:
    """Analyse statistique avanc√©e des nombres extraits"""
    if not numbers:
        return "Aucun nombre extrait"
    import numpy as np
    from scipy import stats
    arr = np.array(numbers)
    analysis = {
        "Moyenne": np.mean(arr),
        "M√©diane": np.median(arr),
        "√âcart-type": np.std(arr),
        "Skewness": stats.skew(arr),
        "Kurtosis": stats.kurtosis(arr),
        "Min/Max": f"{np.min(arr)} / {np.max(arr)}",
        "IQR": stats.iqr(arr),
        "Distribution": "Normale" if -1 < stats.skew(arr) < 1 else "Asym√©trique"
    }
    return "\n".join([f"{k}: {v:.3f}" if isinstance(v, float) else f"{k}: {v}" for k, v in analysis.items()])
def pattern_recognition(file_bytes: bytes) -> str:
    """D√©tecte des patterns connus (headers, signatures, etc.)"""
    patterns = {
        b'\x89PNG': "Fichier PNG",
        b'\xFF\xD8\xFF': "Fichier JPEG",
        b'\x25\x50\x44\x46': "Fichier PDF",
        b'\x50\x4B\x03\x04': "Fichier ZIP",
        b'\x7FELF': "Fichier ELF (Linux executable)",
        b'\x4D\x5A': "Fichier PE (Windows executable)",
        b'\xCA\xFE\xBA\xBE': "Fichier Java class",
        b'\x52\x61\x72\x21': "Fichier RAR"
    }
    detected = []
    for signature, file_type in patterns.items():
        if signature in file_bytes[:100]: # Check first 100 bytes
            detected.append(file_type)
    if detected:
        return f"Patterns d√©tect√©s: {', '.join(detected)}"
    else:
        return "Aucun pattern connu d√©tect√© dans les premiers bytes"
def frequency_analysis(file_bytes: bytes) -> str:
    """Analyse de fr√©quence des bytes (comme analyse cryptographique)"""
    from collections import Counter
    freq = Counter(file_bytes)
    total = len(file_bytes)
    # Les 10 bytes les plus fr√©quents
    most_common = freq.most_common(10)
    analysis = "Top 10 bytes fr√©quents:\n"
    for byte_val, count in most_common:
        percentage = (count / total) * 100
        analysis += f"0x{byte_val:02X}: {count} ({percentage:.2f}%)\n"
    # D√©tection de patterns p√©riodiques simples
    if len(file_bytes) > 100:
        # Recherche de r√©p√©titions tous les N bytes
        for period in [4, 8, 16, 32]:
            if len(file_bytes) >= period * 3:
                pattern_score = 0
                for i in range(period, min(len(file_bytes), period * 10), period):
                    if file_bytes[i:i+period] == file_bytes[i-period:i]:
                        pattern_score += 1
                if pattern_score > 3:
                    analysis += f"\nPattern p√©riodique d√©tect√© (p√©riode {period} bytes)"
    return analysis
def correlation_analysis(numbers: list) -> str:
    """Analyse de corr√©lation entre valeurs successives"""
    if len(numbers) < 3:
        return "Pas assez de donn√©es pour l'analyse de corr√©lation"
    import numpy as np
    arr = np.array(numbers)
    # Corr√©lation avec le d√©calage
    correlations = []
    for lag in range(1, min(10, len(arr)//2)):
        corr = np.corrcoef(arr[:-lag], arr[lag:])[0, 1]
        correlations.append(f"Lag {lag}: {corr:.3f}")
    # Test de stationnarit√© simple
    diffs = np.diff(arr)
    mean_diff = np.mean(diffs)
    std_diff = np.std(diffs)
    result = "Analyses de corr√©lation:\n" + "\n".join(correlations)
    result += f"\n\nStationnarit√© (diff√©rences):\nMoyenne: {mean_diff:.3f}\n√âcart-type: {std_diff:.3f}"
    return result
def compression_ratio(file_bytes: bytes) -> str:
    """Estime le taux de compression possible"""
    import zlib
    try:
        compressed = zlib.compress(file_bytes)
        ratio = len(compressed) / len(file_bytes)
        percentage = (1 - ratio) * 100
        if ratio < 0.3:
            assessment = "Tr√®s compressible (texte/structur√©)"
        elif ratio < 0.7:
            assessment = "Mod√©r√©ment compressible"
        else:
            assessment = "Peu compressible (d√©j√† compress√©/al√©atoire)"
        return f"Taux de compression: {ratio:.3f} ({percentage:.1f}% de r√©duction)\n√âvaluation: {assessment}"
    except:
        return "Impossible de calculer le taux de compression"
def dimensionality_analysis(numbers: list) -> str:
    """Analyse de dimensionalit√© et r√©duction (PCA simple)"""
    if len(numbers) < 10:
        return "Pas assez de donn√©es pour l'analyse de dimensionalit√©"
    import numpy as np
    from sklearn.decomposition import PCA
    # Reshape en matrice 2D
    n_samples = len(numbers) // 5 # Groupes de 5 valeurs
    if n_samples < 2:
        return "Pas assez d'√©chantillons pour PCA"
    X = np.array(numbers[:n_samples*5]).reshape(n_samples, 5)
    pca = PCA(n_components=min(3, X.shape[1]))
    X_pca = pca.fit_transform(X)
    explained_variance = pca.explained_variance_ratio_
    result = f"Analyse PCA ({X.shape[0]} √©chantillons, {X.shape[1]} dimensions):\n"
    result += "\n".join([f"Composante {i+1}: {var:.3f} variance expliqu√©e" for i, var in enumerate(explained_variance)])
    result += f"\n\nVariance totale expliqu√©e: {sum(explained_variance):.3f}"
    return result
def anomaly_detection(numbers: list) -> str:
    """D√©tection d'anomalies statistiques"""
    if len(numbers) < 10:
        return "Pas assez de donn√©es pour la d√©tection d'anomalies"
    import numpy as np
    from scipy import stats
    arr = np.array(numbers)
    # Z-score pour d√©tecter les outliers
    z_scores = np.abs(stats.zscore(arr))
    outliers = np.where(z_scores > 3)[0]
    # IQR method
    Q1 = np.percentile(arr, 25)
    Q3 = np.percentile(arr, 75)
    IQR = Q3 - Q1
    iqr_outliers = np.where((arr < Q1 - 1.5 * IQR) | (arr > Q3 + 1.5 * IQR))[0]
    result = f"D√©tection d'anomalies:\n"
    result += f"Z-score (>3œÉ): {len(outliers)} anomalies d√©tect√©es\n"
    result += f"IQR method: {len(iqr_outliers)} anomalies d√©tect√©es\n"
    if len(outliers) > 0:
        result += f"Valeurs anormales (Z-score): {arr[outliers][:5].tolist()}..." if len(outliers) > 5 else f"Valeurs anormales: {arr[outliers].tolist()}"
    return result
def spectral_analysis(numbers: list) -> str:
    """Analyse spectrale (FFT) pour d√©tecter des fr√©quences"""
    if len(numbers) < 32:
        return "Pas assez de donn√©es pour l'analyse spectrale"
    import numpy as np
    arr = np.array(numbers)
    # FFT
    fft = np.fft.fft(arr)
    freqs = np.fft.ffreq(len(arr))
    # Magnitude du spectre
    magnitude = np.abs(fft)
    # Fr√©quences dominantes (top 5)
    top_indices = np.argsort(magnitude)[::-1][:5]
    dominant_freqs = freqs[top_indices]
    dominant_magnitudes = magnitude[top_indices]
    result = "Analyse spectrale (FFT):\n"
    result += "Fr√©quences dominantes:\n"
    for i, (freq, mag) in enumerate(zip(dominant_freqs, dominant_magnitudes)):
        result += f"Freq {i+1}: {freq:.6f} Hz, Magnitude: {mag:.3f}\n"
    # D√©tection de p√©riodicit√©
    if len(arr) > 100:
        autocorr = np.correlate(arr, arr, mode='full')[len(arr)-1:]
        peaks = np.where(autocorr > np.mean(autocorr) + 2*np.std(autocorr))[0]
        if len(peaks) > 1:
            periods = np.diff(peaks[:5]) # Top 5 p√©riodes
            result += f"\n\nP√©riodes d√©tect√©es: {periods.tolist()}"
    return result
def metadata_extraction(file_bytes: bytes) -> str:
    """Extraction de m√©tadonn√©es et informations structurelles"""
    import struct
    result = f"Taille totale: {len(file_bytes)} bytes ({len(file_bytes)/1024:.1f} KB)\n"
    # Analyse de l'ent√™te (premiers 64 bytes)
    header = file_bytes[:64]
    result += f"Ent√™te (64 premiers bytes):\n{header.hex()}\n"
    # Recherche de cha√Ænes ASCII
    ascii_strings = []
    current_string = ""
    for byte in file_bytes:
        if 32 <= byte <= 126: # Caract√®res ASCII imprimables
            current_string += chr(byte)
        else:
            if len(current_string) >= 4: # Cha√Ænes d'au moins 4 caract√®res
                ascii_strings.append(current_string)
            current_string = ""
    if ascii_strings:
        result += f"\nCha√Ænes ASCII trouv√©es ({len(ascii_strings)}):\n"
        result += "\n".join(ascii_strings[:10]) # Top 10
        if len(ascii_strings) > 10:
            result += f"\n... et {len(ascii_strings)-10} autres"
    # Analyse de l'endianness (little/big endian)
    try:
        if len(file_bytes) >= 4:
            little_endian = struct.unpack('<I', file_bytes[:4])[0]
            big_endian = struct.unpack('>I', file_bytes[:4])[0]
            result += f"\n\nAnalyse endianness:\nLittle-endian (Intel): 0x{little_endian:08X}\nBig-endian (Motorola): 0x{big_endian:08X}"
    except:
        pass
    return result
def search_vectorstore(query: str) -> str:
    """Recherche dans la base vectorielle FAISS des documents PDF index√©s pour enrichir l'analyse"""
    if not st.session_state.vectorstore:
        return "‚ùå Aucune base vectorielle disponible. Veuillez d'abord indexer des PDFs."
    try:
        retriever = st.session_state.vectorstore.as_retriever(search_kwargs={"k": 5})
        docs = retriever.get_relevant_documents(query)
        if not docs:
            return "‚ÑπÔ∏è Aucun document pertinent trouv√© dans la base de connaissances."
        context = "\n\n".join([
            f"üìÑ Document {i+1} (Source: {doc.metadata.get('source', 'Unknown')}):\n{doc.page_content[:500]}..."
            for i, doc in enumerate(docs)
        ])
        return f"‚úÖ {len(docs)} documents pertinents trouv√©s dans la base RAG:\n{context}"
    except Exception as e:
        return f"‚ùå Erreur lors de la recherche RAG: {str(e)}"
def web_search_enhanced(query: str, search_type="general") -> str:
    """Recherche web avanc√©e avec Tavily pour contextualiser l'analyse ERT"""
    try:
        tool = TavilySearchResults(api_key=TAVILY_API_KEY, max_results=5)
      
        # Enrichir la requ√™te pour ERT si n√©cessaire
        if any(keyword in query.lower() for keyword in ["ert", "r√©sistivit√©", "electrical resistivity", "tomography"]):
            enhanced_query = f"{query} ERT electrical resistivity tomography geophysics subsurface"
        else:
            enhanced_query = query
          
        web_results = tool.invoke(enhanced_query)
        if not web_results:
            return "‚ÑπÔ∏è Aucune information trouv√©e sur le web."
        
        # V√©rifier si web_results est une string (erreur) ou une liste
        if isinstance(web_results, str):
            return f"‚ÑπÔ∏è R√©sultat inattendu: {web_results[:200]}"
        
        # Assurer que web_results est une liste de dicts
        if not isinstance(web_results, list):
            return f"‚ÑπÔ∏è Format inattendu des r√©sultats web"
        
        context = "\n\n".join([
            f"üåê Source {i+1}: {result.get('title', 'Sans titre') if isinstance(result, dict) else 'Sans titre'}\n{result.get('content', '')[:400] if isinstance(result, dict) else str(result)[:400]}..."
            for i, result in enumerate(web_results)
        ])
        return f"‚úÖ {len(web_results)} r√©sultats de recherche web:\n{context}"
    except Exception as e:
        return f"‚ùå Erreur lors de la recherche web: {str(e)}"
def mathematical_calculator(expression: str) -> str:
    """Outil de calcul math√©matique avanc√© pour analyses statistiques et num√©riques"""
    try:
        # Imports s√©curis√©s pour les calculs
        import numpy as np
        import math
        from scipy import stats, special
        # Environnement s√©curis√© pour les calculs
        safe_dict = {
            "np": np,
            "math": math,
            "stats": stats,
            "special": special,
            "sqrt": math.sqrt,
            "log": math.log,
            "exp": math.exp,
            "sin": math.sin,
            "cos": math.cos,
            "pi": math.pi,
            "e": math.e
        }
        # √âvaluation s√©curis√©e
        result = eval(expression, {"__builtins__": {}}, safe_dict)
        # Formatage du r√©sultat
        if isinstance(result, (int, float)):
            return f"‚úÖ R√©sultat: {result:.6f}"
        elif isinstance(result, np.ndarray):
            return f"‚úÖ R√©sultat array: {result.shape}\n{result}"
        else:
            return f"‚úÖ R√©sultat: {result}"
    except Exception as e:
        return f"‚ùå Erreur de calcul: {str(e)}\nExpression: {expression}"
def rag_enhanced_analysis(query: str, file_context: str = "", ert_data: dict = None) -> str:
    """Analyse RAG enrichie combinant connaissances locales et recherche web pour ERT"""
    try:
        # Recherche dans la base RAG
        rag_results = search_vectorstore(query)
        # Recherche web sp√©cialis√©e ERT
        if ert_data and any(keyword in query.lower() for keyword in ["ert", "r√©sistivit√©", "electrical", "tomography"]):
            # Enrichir la requ√™te avec les valeurs ERT d√©tect√©es
            mean_val = ert_data.get('mean', 0)
            enhanced_query = f"{query} ERT r√©sistivit√© {mean_val:.1f} Ohm.m interpr√©tation g√©ophysique"
            web_results = web_search_enhanced(enhanced_query, "ert_specialized")
        else:
            web_results = web_search_enhanced(query)
        # Combinaison intelligente avec contexte ERT
        combined_context = f"""
üìö ANALYSE RAG ENRICHIE - SP√âCIALIS√âE ERT
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üîç Query: {query}
üìÑ CONNAISSANCES LOCALES (RAG):
{rag_results}
üåê RECHERCHE WEB SP√âCIALIS√âE:
{web_results}
üí° Analyse crois√©e:
- Documents RAG: {len(rag_results.split('Document'))-1 if 'Document' in rag_results else 0} sources internes
- Recherche web: {len(web_results.split('Source'))-1 if 'Source' in web_results else 0} sources externes
üî¨ CONTEXTE FICHIER ANALYS√â:
{file_context}
üéØ DONN√âES ERT D√âTECT√âES:
{ert_data if ert_data else "Aucune donn√©e ERT sp√©cifique"}
"""
        return combined_context
    except Exception as e:
        return f"‚ùå Erreur dans l'analyse RAG enrichie: {str(e)}"
def ert_data_detection(file_bytes: bytes, numbers: list) -> str:
    """D√©tection sp√©cialis√©e de donn√©es ERT (Electrical Resistivity Tomography)"""
    if not numbers:
        return "‚ùå Aucune donn√©e num√©rique trouv√©e pour l'analyse ERT"
    import numpy as np
    arr = np.array(numbers)
    # Crit√®res typiques des donn√©es ERT
    analysis = "üîç ANALYSE SP√âCIALIS√âE ERT (R√©sistivit√© √âlectrique)\n"
    analysis += "=" * 50 + "\n\n"
    # 1. Analyse des valeurs de r√©sistivit√© (g√©n√©ralement 0.1 - 10000 Ohm.m)
    resistivity_range = f"Valeurs r√©sistivit√©: {np.min(arr):.3f} - {np.max(arr):.3f}"
    if 0.1 <= np.min(arr) and np.max(arr) <= 10000:
        resistivity_range += " ‚úÖ Plage typique ERT"
    else:
        resistivity_range += " ‚ö†Ô∏è Hors plage typique ERT"
    analysis += f"üìä {resistivity_range}\n\n"
    # 2. Analyse de la distribution (souvent log-normale)
    mean_val = np.mean(arr)
    std_val = np.std(arr)
    cv = std_val / mean_val if mean_val != 0 else float('inf') # Coefficient de variation
    analysis += f"üìà Statistiques:\n"
    analysis += f" ‚Ä¢ Moyenne: {mean_val:.3f}\n"
    analysis += f" ‚Ä¢ √âcart-type: {std_val:.3f}\n"
    analysis += f" ‚Ä¢ Coefficient de variation: {cv:.3f}\n"
    analysis += f" ‚Ä¢ M√©diane: {np.median(arr):.3f}\n\n"
    # 3. Test de distribution log-normale (caract√©ristique ERT)
    try:
        log_data = np.log(arr[arr > 0]) # √âviter log(0)
        from scipy import stats
        _, p_value = stats.shapiro(log_data[:min(5000, len(log_data))]) # Test Shapiro-Wilk
        if p_value > 0.05:
            analysis += f"üìä Distribution: Log-normale (p={p_value:.3f}) ‚úÖ Typique ERT\n\n"
        else:
            analysis += f"üìä Distribution: Non log-normale (p={p_value:.3f}) ‚ö†Ô∏è Peu commun ERT\n\n"
    except:
        analysis += f"üìä Distribution: Test impossible\n\n"
    # 4. Analyse de patterns spatiaux (si donn√©es organis√©es)
    if len(arr) > 100:
        # Recherche de patterns r√©p√©t√©s (√©lectrodes)
        unique_vals = len(np.unique(arr))
        analysis += f"üéØ Unicit√© des valeurs: {unique_vals}/{len(arr)} ({unique_vals/len(arr)*100:.1f}%)\n"
        # Analyse de clustering spatial simul√©
        if len(arr) >= 50:
            from sklearn.cluster import KMeans
            # Clustering simple pour d√©tecter groupes de r√©sistivit√©
            kmeans = KMeans(n_clusters=min(5, len(arr)//10), random_state=42, n_init=10)
            clusters = kmeans.fit_predict(arr.reshape(-1, 1))
            cluster_centers = kmeans.cluster_centers_.flatten()
            analysis += f"üéØ Clustering r√©sistivit√© ({len(np.unique(clusters))} groupes):\n"
            for i, center in enumerate(sorted(cluster_centers)):
                count = np.sum(clusters == i)
                analysis += f" ‚Ä¢ Groupe {i+1}: {center:.3f} Ohm.m ({count} valeurs)\n"
            analysis += "\n"
    # 5. D√©tection de format de donn√©es ERT connu
    ert_formats = {
        "RES2DINV": "Format ASCII RES2DINV (r√©sistivit√© 2D)",
        "ERTLab": "Format ERTLab (syst√®me IRIS)",
        "Syscal": "Format Syscal (syst√®me fran√ßais)",
        "ABEM": "Format ABEM (syst√®me su√©dois)"
    }
    detected_format = "Format non reconnu"
    if len(file_bytes) > 100:
        header = file_bytes[:200].decode('utf-8', errors='ignore').lower()
        for fmt, desc in ert_formats.items():
            if fmt.lower() in header:
                detected_format = desc
                break
    analysis += f"üìã Format d√©tect√©: {detected_format}\n\n"
    # 6. Recommandations d'analyse
    analysis += f"üí° RECOMMANDATIONS:\n"
    if PYGIMLI_AVAILABLE:
        analysis += f" ‚Ä¢ Inversion possible avec PyGIMLI\n"
    if RESIPY_AVAILABLE:
        analysis += f" ‚Ä¢ Inversion possible avec ResIPy\n"
    analysis += f" ‚Ä¢ Visualisation 2D/3D recommand√©e\n"
    analysis += f" ‚Ä¢ Analyse de sensibilit√© possible\n"
    analysis += f" ‚Ä¢ Pour fichiers .dat ERT: Utilisez les formules de calcul de r√©sistivit√© apparente via mathematical_calculator (Schlumberger: pi*(L**2 - l**2)/(2*l) * V/I, etc.)\n\n"
    # 7. Classification finale
    if 0.1 <= np.min(arr) <= 10000 and cv > 0.5: # CV √©lev√© = h√©t√©rog√©n√©it√© typique ERT
        confidence = "√âLEV√âE"
        analysis += f"üéØ CONCLUSION: Donn√©es tr√®s probablement ERT (confiance: {confidence})\n"
    elif 0.1 <= np.min(arr) <= 10000:
        confidence = "MOYENNE"
        analysis += f"üéØ CONCLUSION: Donn√©es probablement ERT (confiance: {confidence})\n"
    else:
        confidence = "FAIBLE"
        analysis += f"üéØ CONCLUSION: Donn√©es peu caract√©ristiques ERT (confiance: {confidence})\n"
    return analysis
def ert_inversion_analysis(numbers: list) -> str:
    """Analyse d'inversion ERT sp√©cialis√©e utilisant PyGIMLI/ResIPy si disponible"""
    if not numbers:
        return "‚ùå Aucune donn√©e pour l'inversion ERT"
    import numpy as np
    analysis = "üî¨ ANALYSE D'INVERSION ERT\n"
    analysis += "=" * 40 + "\n\n"
    arr = np.array(numbers)
    # Simulation d'inversion simple (sans vraie inversion g√©ophysique)
    analysis += f"üìä Param√®tres d'inversion simul√©s:\n"
    analysis += f" ‚Ä¢ Nombre de donn√©es: {len(arr)}\n"
    analysis += f" ‚Ä¢ R√©sistivit√© moyenne: {np.mean(arr):.3f} Ohm.m\n"
    analysis += f" ‚Ä¢ Contraste: {np.max(arr)/np.min(arr):.1f}\n\n"
    # Analyse de r√©solution th√©orique
    if len(arr) > 10:
        # Estimation de la r√©solution bas√©e sur la variance
        variance = np.var(arr)
        resolution = 1.0 / (1.0 + variance / np.mean(arr)**2)
        analysis += f"üéØ R√©solution estim√©e: {resolution:.3f}\n\n"
    # Recommandations d'inversion
    analysis += f"üí° RECOMMANDATIONS D'INVERSION:\n"
    if PYGIMLI_AVAILABLE:
        analysis += f" ‚úÖ PyGIMLI disponible - Inversion compl√®te possible\n"
        analysis += f" ‚Ä¢ M√©thodes: Gauss-Newton, Quasi-Newton\n"
        analysis += f" ‚Ä¢ R√©gularisation: L2, L1, TV\n"
    else:
        analysis += f" ‚ö†Ô∏è PyGIMLI non install√© - Inversion limit√©e\n"
    # Test d'import ResIPy seulement ici
    try:
        import resipy
        resipy_available = True
    except ImportError:
        resipy_available = False
    if resipy_available:
        analysis += f" ‚úÖ ResIPy disponible - Interface graphique possible\n"
        analysis += f" ‚Ä¢ Support multi-√©lectrodes\n"
        analysis += f" ‚Ä¢ Visualisation 3D\n"
    else:
        analysis += f" ‚ö†Ô∏è ResIPy non disponible (compatibilit√© NumPy)\n"
    analysis += f" ‚Ä¢ Donn√©es suffisantes: {'Oui' if len(arr) > 50 else 'Non'} (min 50 mesures)\n"
    analysis += f" ‚Ä¢ Qualit√© des donn√©es: {'Bonne' if np.std(arr)/np.mean(arr) > 0.1 else 'Faible contraste'}\n"
    return analysis
def get_resistivity_color(rho: float) -> str:
    """Retourne un code couleur et description pour une valeur de r√©sistivit√© en Ohm.m"""
    if rho < 10:
        color_hex = "#0000FF" # Bleu
        desc = "Faible r√©sistivit√© - mat√©riaux conducteurs (argile, eau sal√©e, m√©taux)"
        nature = "Nature: Couches satur√©es en eau, pollution potentielle"
        depth_est = "Profondeur estim√©e: Superficielle (0-5 m)"
    elif 10 <= rho < 100:
        color_hex = "#00FF00" # Vert
        desc = "R√©sistivit√© moyenne - sols typiques (sable humide, limon)"
        nature = "Nature: Zone vadose, aquif√®res non salins"
        depth_est = "Profondeur estim√©e: Moyenne (5-20 m)"
    elif 100 <= rho < 1000:
        color_hex = "#FFFF00" # Jaune
        desc = "R√©sistivit√© √©lev√©e - mat√©riaux semi-r√©sistants (gr√®s, calcaire)"
        nature = "Nature: Roches s√©dimentaires, fractures partielles"
        depth_est = "Profondeur estim√©e: Profonde (20-50 m)"
    else:
        color_hex = "#FF0000" # Rouge
        desc = "Tr√®s haute r√©sistivit√© - mat√©riaux r√©sistants (granite, air, vides)"
        nature = "Nature: Substratum rocheux, cavit√©s ou zones s√®ches"
        depth_est = "Profondeur estim√©e: Tr√®s profonde (>50 m)"
  
    return f"Couleur: {color_hex} ({desc})\nNature: {nature}\nProfondeur: {depth_est}\nAutres: Couleur indicative pour visualisation ERT (colormap g√©ophysique standard)"
def fetch_material_resistivities(category: str) -> str:
    """Recherche dynamique sur internet des plages de r√©sistivit√© pour une cat√©gorie de mat√©riaux"""
    query = f"typical electrical resistivity ranges {category} liquids minerals soils rocks geophysics Ohm.m values categories comparison"
    return web_search_enhanced(query, "ert_materials")

def create_minerals_database():
    """
    Cr√©e une base de donn√©es √©tendue des r√©sistivit√©s de min√©raux, roches et liquides
    Bas√©e sur recherches g√©ophysiques pour exploration mini√®re ERT
    """
    import pandas as pd
    
    materials_data = [
        # LIQUIDES
        {"Cat√©gorie": "Liquides", "Type": "Eau de mer", "Plage Min (Œ©m)": 0.05, "Plage Max (Œ©m)": 0.3, "Notes": "Haute conductivit√© due √† la salinit√©"},
        {"Cat√©gorie": "Liquides", "Type": "Eau saum√¢tre", "Plage Min (Œ©m)": 1, "Plage Max (Œ©m)": 10, "Notes": "Salinit√© mod√©r√©e"},
        {"Cat√©gorie": "Liquides", "Type": "Eau douce", "Plage Min (Œ©m)": 10, "Plage Max (Œ©m)": 100, "Notes": "Faible salinit√©, eaux de surface ou souterraines"},
        {"Cat√©gorie": "Liquides", "Type": "Eau min√©rale/mine", "Plage Min (Œ©m)": 0.1, "Plage Max (Œ©m)": 1, "Notes": "Haute concentration en min√©raux dissous"},
        {"Cat√©gorie": "Liquides", "Type": "P√©trole/Hydrocarbures", "Plage Min (Œ©m)": 1000, "Plage Max (Œ©m)": 100000000, "Notes": "Tr√®s r√©sistif, isolant"},
        
        # MINERAIS (√©tendu pour exploration mini√®re)
        {"Cat√©gorie": "Minerais", "Type": "Graphite", "Plage Min (Œ©m)": 0.000008, "Plage Max (Œ©m)": 0.0001, "Notes": "Tr√®s conducteur, carbone pur"},
        {"Cat√©gorie": "Minerais", "Type": "Pyrite pure", "Plage Min (Œ©m)": 0.00003, "Plage Max (Œ©m)": 0.001, "Notes": "Sulfure de fer, tr√®s conducteur"},
        {"Cat√©gorie": "Minerais", "Type": "Pyrite (impure)", "Plage Min (Œ©m)": 0.001, "Plage Max (Œ©m)": 10, "Notes": "Avec impuret√©s cuivre, anomalie ERT"},
        {"Cat√©gorie": "Minerais", "Type": "Galena", "Plage Min (Œ©m)": 0.001, "Plage Max (Œ©m)": 100, "Notes": "Sulfure de plomb, conducteur"},
        {"Cat√©gorie": "Minerais", "Type": "Magn√©tite", "Plage Min (Œ©m)": 0.01, "Plage Max (Œ©m)": 1000, "Notes": "Oxyde de fer, magn√©tique, variable"},
        {"Cat√©gorie": "Minerais", "Type": "H√©matite", "Plage Min (Œ©m)": 10, "Plage Max (Œ©m)": 10000, "Notes": "Oxyde de fer, presque isolant"},
        {"Cat√©gorie": "Minerais", "Type": "Chalcopyrite", "Plage Min (Œ©m)": 0.001, "Plage Max (Œ©m)": 10, "Notes": "Sulfure cuivre-fer, minerai Cu"},
        {"Cat√©gorie": "Minerais", "Type": "Bornite", "Plage Min (Œ©m)": 0.001, "Plage Max (Œ©m)": 10, "Notes": "Sulfure cuivre-fer, paon ore"},
        {"Cat√©gorie": "Minerais", "Type": "Sphalerite (Zinc)", "Plage Min (Œ©m)": 100, "Plage Max (Œ©m)": 10000, "Notes": "Sulfure de zinc, mod√©r√©ment r√©sistif"},
        {"Cat√©gorie": "Minerais", "Type": "Cassit√©rite (√âtain)", "Plage Min (Œ©m)": 1000, "Plage Max (Œ©m)": 10000, "Notes": "Oxyde d'√©tain, r√©sistif"},
        {"Cat√©gorie": "Minerais", "Type": "Molybd√©nite", "Plage Min (Œ©m)": 0.001, "Plage Max (Œ©m)": 1, "Notes": "Sulfure molybd√®ne, tr√®s conducteur"},
        {"Cat√©gorie": "Minerais", "Type": "Or (natif)", "Plage Min (Œ©m)": 0.000001, "Plage Max (Œ©m)": 0.00001, "Notes": "M√©tal pur, ultra-conducteur"},
        {"Cat√©gorie": "Minerais", "Type": "Or (veines quartz)", "Plage Min (Œ©m)": 1, "Plage Max (Œ©m)": 1000, "Notes": "Variable, sulfures associ√©s"},
        {"Cat√©gorie": "Minerais", "Type": "Fer (minerai)", "Plage Min (Œ©m)": 0.01, "Plage Max (Œ©m)": 1000, "Notes": "Magn√©tite/h√©matite m√©lang√©e"},
        {"Cat√©gorie": "Minerais", "Type": "Quartz", "Plage Min (Œ©m)": 10000000000, "Plage Max (Œ©m)": 100000000000000, "Notes": "Silicate, ultra-isolant"},
        {"Cat√©gorie": "Minerais", "Type": "Cuivre (natif)", "Plage Min (Œ©m)": 0.0000017, "Plage Max (Œ©m)": 0.000002, "Notes": "M√©tal pur, excellent conducteur"},
        {"Cat√©gorie": "Minerais", "Type": "Argent (natif)", "Plage Min (Œ©m)": 0.0000016, "Plage Max (Œ©m)": 0.000002, "Notes": "Meilleur conducteur naturel"},
        
        # ROCHES
        {"Cat√©gorie": "Roches", "Type": "Argile (humide)", "Plage Min (Œ©m)": 1, "Plage Max (Œ©m)": 100, "Notes": "Faible r√©sistivit√©, eau et ions"},
        {"Cat√©gorie": "Roches", "Type": "Schiste", "Plage Min (Œ©m)": 20, "Plage Max (Œ©m)": 2000, "Notes": "Variable avec humidit√©"},
        {"Cat√©gorie": "Roches", "Type": "Gr√®s", "Plage Min (Œ©m)": 30, "Plage Max (Œ©m)": 10000, "Notes": "Sec √† satur√©"},
        {"Cat√©gorie": "Roches", "Type": "Calcaire", "Plage Min (Œ©m)": 50, "Plage Max (Œ©m)": 10000000, "Notes": "Variable, haut si sec"},
        {"Cat√©gorie": "Roches", "Type": "Granite", "Plage Min (Œ©m)": 5000, "Plage Max (Œ©m)": 1000000, "Notes": "Igneuse, r√©sistif si sec"},
        {"Cat√©gorie": "Roches", "Type": "Basalte", "Plage Min (Œ©m)": 10, "Plage Max (Œ©m)": 13000000, "Notes": "Igneuse, tr√®s variable"},
        {"Cat√©gorie": "Roches", "Type": "Alluvions", "Plage Min (Œ©m)": 1, "Plage Max (Œ©m)": 1000, "Notes": "S√©diments non consolid√©s"},
        {"Cat√©gorie": "Roches", "Type": "Gravier", "Plage Min (Œ©m)": 100, "Plage Max (Œ©m)": 2500, "Notes": "Sec, bonne perm√©abilit√©"},
    ]
    
    return pd.DataFrame(materials_data)

def create_real_mineral_correspondence_table(numbers: list, file_name: str = "unknown", depths: list = None, full_size: bool = False) -> tuple:
    """
    üéØ TABLEAU DE CORRESPONDANCES R√âELLES - Donn√©es mesur√©es vs Min√©raux g√©ophysiques
    
    Cr√©e un tableau dynamique matplotlib avec UNIQUEMENT les min√©raux r√©ellement d√©tect√©s
    bas√© sur les valeurs de r√©sistivit√© mesur√©es dans le fichier .dat
    
    Args:
        numbers: Liste des valeurs de r√©sistivit√© mesur√©es (Œ©¬∑m)
        file_name: Nom du fichier analys√©
        depths: Liste optionnelle des profondeurs correspondantes (m)
        full_size: Mode grand format (True = 24√ó16", False = 16√ó12")
    
    Returns:
        tuple: (figure matplotlib, DataFrame des correspondances, texte rapport)
    """
    import numpy as np
    import matplotlib.pyplot as plt
    from matplotlib.patches import Rectangle
    
    if not numbers or len(numbers) < 5:
        return None, None, "‚ùå Donn√©es insuffisantes pour analyse (minimum 5 mesures)"
    
    arr = np.array(numbers)
    minerals_db = create_minerals_database()
    
    # Tailles adaptatives
    if full_size:
        figsize = (24, 16)
        title_fontsize = 18
        header_fontsize = 12
        cell_fontsize = 10
        scatter_markersize = 120
    else:
        figsize = (16, 12)
        title_fontsize = 14
        header_fontsize = 10
        cell_fontsize = 8
        scatter_markersize = 80
    
    # Si pas de profondeurs, estimer selon la r√©sistivit√©
    if depths is None:
        depths = []
        for rho in arr:
            if rho < 1:
                depths.append(np.random.uniform(0, 20))  # Zone superficielle conductrice
            elif rho < 100:
                depths.append(np.random.uniform(5, 50))  # Zone moyenne
            elif rho < 1000:
                depths.append(np.random.uniform(20, 100))  # Zone transition
            else:
                depths.append(np.random.uniform(50, 200))  # Zone profonde
        depths = np.array(depths)
    else:
        depths = np.array(depths)
    
    # Cr√©er DataFrame des mesures r√©elles
    real_data = []
    
    for i, (rho, depth) in enumerate(zip(arr, depths)):
        # Trouver correspondances dans la base de donn√©es
        matches = minerals_db[
            (minerals_db["Plage Min (Œ©m)"] <= rho) & 
            (minerals_db["Plage Max (Œ©m)"] >= rho)
        ]
        
        if not matches.empty:
            for _, match in matches.iterrows():
                real_data.append({
                    "Mesure #": i + 1,
                    "Profondeur (m)": depth,
                    "R√©sistivit√© mesur√©e (Œ©¬∑m)": rho,
                    "Mat√©riau d√©tect√©": match["Type"],
                    "Cat√©gorie": match["Cat√©gorie"],
                    "Plage DB (Œ©¬∑m)": f"{match['Plage Min (Œ©m)']} - {match['Plage Max (Œ©m)']}",
                    "Confiance": calculate_confidence(rho, match["Plage Min (Œ©m)"], match["Plage Max (Œ©m)"]),
                    "Notes": match["Notes"]
                })
    
    if not real_data:
        return None, None, "‚ö†Ô∏è Aucune correspondance trouv√©e dans la base de donn√©es min√©ralogique"
    
    df_correspondances = pd.DataFrame(real_data)
    
    # Trier par profondeur
    df_correspondances = df_correspondances.sort_values("Profondeur (m)")
    
    # Limiter le nombre de lignes pour √©viter decompression bomb
    max_rows_display = min(100, len(df_correspondances))
    
    # üìä CR√âER TABLEAU MATPLOTLIB DYNAMIQUE avec taille responsive
    # Limiter la taille pour √©viter decompression bomb (max 20 pouces de hauteur)
    fig_height = min(20, max(8, max_rows_display * 0.15))
    fig, (ax_table, ax_depth) = plt.subplots(1, 2, figsize=(figsize[0], fig_height))
    
    # Augmenter la limite de pixels pour matplotlib
    from PIL import Image
    Image.MAX_IMAGE_PIXELS = 200000000  # 200 millions de pixels max
    
    # TABLEAU GAUCHE: Correspondances d√©taill√©es
    ax_table.axis('tight')
    ax_table.axis('off')
    
    # Grouper par profondeur pour affichage condens√©
    depth_groups = df_correspondances.groupby(df_correspondances["Profondeur (m)"].round(1))
    
    table_data = []
    row_colors = []
    
    # Limiter √† 50 groupes max pour le tableau
    max_groups = min(50, len(depth_groups))
    group_count = 0
    
    for depth, group in depth_groups:
        if group_count >= max_groups:
            break
        group_count += 1
        
        materials = group["Mat√©riau d√©tect√©"].unique()
        rho_values = group["R√©sistivit√© mesur√©e (Œ©¬∑m)"].values
        categories = group["Cat√©gorie"].unique()
        confidence = group["Confiance"].mean()
        
        # D√©terminer couleur selon cat√©gorie dominante
        if "Minerais" in categories:
            color = '#FFD700' if any('Or' in m for m in materials) else '#FF6B6B'
        elif "Liquides" in categories:
            color = '#4ECDC4'
        else:
            color = '#95E1D3'
        
        table_data.append([
            f"{depth:.1f}m",
            f"{rho_values.min():.4f} - {rho_values.max():.4f}",
            "\n".join(materials[:3]),  # Max 3 mat√©riaux
            f"{confidence:.0%}"
        ])
        row_colors.append(color)
    
    table = ax_table.table(
        cellText=table_data,
        colLabels=["Profondeur", "R√©sistivit√© (Œ©¬∑m)", "Mat√©riaux d√©tect√©s", "Confiance"],
        cellLoc='left',
        loc='center',
        colWidths=[0.15, 0.25, 0.45, 0.15]
    )
    
    table.auto_set_font_size(False)
    table.set_fontsize(cell_fontsize)
    table.scale(1, 2)
    
    # Colorer les lignes
    for i, color in enumerate(row_colors):
        for j in range(4):
            table[(i+1, j)].set_facecolor(color)
            table[(i+1, j)].set_alpha(0.3)
    
    # Header
    for j in range(4):
        table[(0, j)].set_facecolor('#2C3E50')
        table[(0, j)].set_text_props(weight='bold', color='white', fontsize=header_fontsize)
    
    ax_table.set_title(f"üìä Correspondances R√©elles: {file_name}\n{len(real_data)} d√©tections", 
                       fontsize=title_fontsize, weight='bold', pad=20)
    
    # GRAPHIQUE DROITE: Profil profondeur vs r√©sistivit√©
    ax_depth.invert_yaxis()  # Profondeur croissante vers le bas
    
    # Grouper par type de mat√©riau pour l√©gende
    material_types = df_correspondances.groupby("Mat√©riau d√©tect√©")
    
    colors_map = {
        "Eau de mer": "#FF0000",
        "Eau sal√©e (nappe)": "#FF6B00",
        "Eau douce": "#00FF00",
        "Eau tr√®s pure": "#0000FF",
        "Or (natif)": "#FFD700",
        "Argent (natif)": "#C0C0C0",
        "Pyrite pure": "#FF4500",
        "Chalcopyrite": "#FF8C00",
        "Galena": "#696969",
        "Magn√©tite": "#8B4513",
        "Graphite": "#000000",
    }
    
    plotted_materials = set()
    
    # Limiter le nombre de points affich√©s pour √©viter surcharge
    max_points_per_material = 200
    
    for material, group in material_types:
        color = colors_map.get(material, "#888888")
        marker = 'o' if group["Cat√©gorie"].iloc[0] == "Minerais" else 's'
        
        # Sous-√©chantillonner si trop de points
        if len(group) > max_points_per_material:
            group_sample = group.sample(n=max_points_per_material, random_state=42)
        else:
            group_sample = group
        
        ax_depth.scatter(
            group_sample["R√©sistivit√© mesur√©e (Œ©¬∑m)"],
            group_sample["Profondeur (m)"],
            c=color,
            marker=marker,
            s=scatter_markersize,
            alpha=0.7,
            label=material if material not in plotted_materials else "",
            edgecolors='black',
            linewidth=1
        )
        plotted_materials.add(material)
    
    ax_depth.set_xlabel("R√©sistivit√© (Œ©¬∑m)", fontsize=header_fontsize, weight='bold')
    ax_depth.set_ylabel("Profondeur (m)", fontsize=header_fontsize, weight='bold')
    ax_depth.set_xscale('log')
    ax_depth.grid(True, alpha=0.3, linestyle='--')
    ax_depth.legend(loc='best', fontsize=cell_fontsize, framealpha=0.9)
    ax_depth.tick_params(labelsize=cell_fontsize)
    ax_depth.set_title("Profil G√©ophysique R√©el", fontsize=12, weight='bold')
    
    # Ajouter zones de r√©f√©rence
    ax_depth.axhspan(0, 20, alpha=0.1, color='red', label='_Zone superficielle')
    ax_depth.axhspan(20, 100, alpha=0.1, color='yellow', label='_Zone interm√©diaire')
    ax_depth.axhspan(100, max(depths) if len(depths) > 0 else 200, alpha=0.1, color='blue', label='_Zone profonde')
    
    plt.tight_layout()
    
    # üìù G√âN√âRER RAPPORT TEXTUEL D√âTAILL√â
    rapport = "üéØ TABLEAU DE CORRESPONDANCES R√âELLES - DONN√âES ERT vs MIN√âRAUX\n"
    rapport += "=" * 80 + "\n\n"
    
    rapport += f"üìÅ Fichier: {file_name}\n"
    rapport += f"üìä Mesures analys√©es: {len(arr)}\n"
    rapport += f"‚úÖ Correspondances trouv√©es: {len(real_data)}\n"
    rapport += f"üìà Plage r√©sistivit√©: {arr.min():.6f} - {arr.max():.2f} Œ©¬∑m\n"
    rapport += f"üìè Plage profondeur: {depths.min():.1f} - {depths.max():.1f} m\n\n"
    
    rapport += "üîç D√âTECTION PAR PROFONDEUR:\n"
    rapport += "‚îÄ" * 80 + "\n"
    
    for depth, group in depth_groups:
        rapport += f"\nüìç PROFONDEUR: {depth:.1f} m\n"
        rapport += f"   R√©sistivit√© mesur√©e: {group['R√©sistivit√© mesur√©e (Œ©¬∑m)'].min():.4f} - {group['R√©sistivit√© mesur√©e (Œ©¬∑m)'].max():.4f} Œ©¬∑m\n"
        rapport += f"   Mat√©riaux d√©tect√©s ({len(group)}):\n"
        
        for _, row in group.iterrows():
            rapport += f"      ‚Ä¢ {row['Mat√©riau d√©tect√©']} ({row['Cat√©gorie']})\n"
            rapport += f"        - Confiance: {row['Confiance']:.0%}\n"
            rapport += f"        - Plage DB: {row['Plage DB (Œ©¬∑m)']}\n"
            rapport += f"        - Notes: {row['Notes']}\n"
    
    # Statistiques par cat√©gorie
    rapport += "\nüìä STATISTIQUES PAR CAT√âGORIE:\n"
    rapport += "‚îÄ" * 80 + "\n"
    
    category_stats = df_correspondances.groupby("Cat√©gorie").agg({
        "Mat√©riau d√©tect√©": lambda x: x.nunique(),
        "Profondeur (m)": ["min", "max", "mean"],
        "R√©sistivit√© mesur√©e (Œ©¬∑m)": ["min", "max"],
        "Confiance": "mean"
    })
    
    for cat, stats in category_stats.iterrows():
        rapport += f"\n{cat}:\n"
        rapport += f"  ‚Ä¢ Mat√©riaux uniques: {stats[('Mat√©riau d√©tect√©', '<lambda>')]}\n"
        rapport += f"  ‚Ä¢ Profondeur: {stats[('Profondeur (m)', 'min')]:.1f} - {stats[('Profondeur (m)', 'max')]:.1f} m (moy: {stats[('Profondeur (m)', 'mean')]:.1f} m)\n"
        rapport += f"  ‚Ä¢ R√©sistivit√©: {stats[('R√©sistivit√© mesur√©e (Œ©¬∑m)', 'min')]:.4f} - {stats[('R√©sistivit√© mesur√©e (Œ©¬∑m)', 'max')]:.2f} Œ©¬∑m\n"
        rapport += f"  ‚Ä¢ Confiance moyenne: {stats[('Confiance', 'mean')]:.0%}\n"
    
    # Min√©raux d'int√©r√™t √©conomique
    rapport += "\nüíé MIN√âRAUX D'INT√âR√äT √âCONOMIQUE D√âTECT√âS:\n"
    rapport += "‚îÄ" * 80 + "\n"
    
    economic_minerals = df_correspondances[df_correspondances["Mat√©riau d√©tect√©"].str.contains(
        "Or|Argent|Cuivre|Pyrite|Chalcopyrite|Galena|Molybd√©nite|Cassit√©rite", 
        case=False, 
        na=False
    )]
    
    if not economic_minerals.empty:
        for _, row in economic_minerals.iterrows():
            rapport += f"‚≠ê {row['Mat√©riau d√©tect√©']}\n"
            rapport += f"   ‚Ä¢ Profondeur: {row['Profondeur (m)']:.1f} m\n"
            rapport += f"   ‚Ä¢ R√©sistivit√©: {row['R√©sistivit√© mesur√©e (Œ©¬∑m)']:.6f} Œ©¬∑m\n"
            rapport += f"   ‚Ä¢ Confiance: {row['Confiance']:.0%}\n"
            rapport += f"   ‚Ä¢ Recommandation: Forage cibl√© pour validation\n\n"
    else:
        rapport += "‚ö†Ô∏è Aucun min√©ral d'int√©r√™t √©conomique majeur d√©tect√©\n\n"
    
    rapport += "=" * 80 + "\n"
    
    return fig, df_correspondances, rapport

def calculate_confidence(measured_rho: float, min_rho: float, max_rho: float) -> float:
    """
    Calcule le niveau de confiance de la correspondance
    Bas√© sur la position dans la plage de r√©sistivit√©
    """
    if min_rho == max_rho:
        return 1.0 if measured_rho == min_rho else 0.0
    
    # Distance au centre de la plage (normalis√©e)
    center = (min_rho + max_rho) / 2
    range_width = max_rho - min_rho
    
    distance_from_center = abs(measured_rho - center)
    normalized_distance = distance_from_center / (range_width / 2)
    
    # Confiance = 100% au centre, diminue vers les bords
    confidence = max(0.0, 1.0 - (normalized_distance * 0.3))  # Max 30% de p√©nalit√©
    
    return confidence

def create_ert_professional_sections(numbers: list, file_name: str = "unknown", depths: list = None, distances: list = None, full_size: bool = False) -> tuple:
    """
    üé® CR√âATION DE 5 GRAPHIQUES ERT PROFESSIONNELS
    Style Res2DInv/RES3DINV avec coupes repr√©sentatives et palette de couleurs
    
    Les 5 graphiques:
    1. Pseudosection de r√©sistivit√© apparente (donn√©es brutes)
    2. Section invers√©e avec mod√®le de r√©sistivit√© (interpr√©tation)
    3. Coupe verticale avec √©chelle de couleurs g√©ologique
    4. Histogramme de distribution + palette de couleurs
    5. Profil 1D comparatif (profondeur vs r√©sistivit√©)
    
    Args:
        numbers: Valeurs de r√©sistivit√© mesur√©es (Œ©¬∑m)
        file_name: Nom du fichier
        depths: Profondeurs (m) - si None, g√©n√©r√© automatiquement
        distances: Distances horizontales (m) - si None, g√©n√©r√© automatiquement
        full_size: Mode grand format (True = 30x36", False = 20x24")
    
    Returns:
        tuple: (figure matplotlib, donn√©es_grille, texte_rapport)
    """
    import numpy as np
    import matplotlib.pyplot as plt
    from matplotlib import cm
    from matplotlib.colors import LogNorm, ListedColormap
    from scipy.interpolate import griddata
    
    if not numbers or len(numbers) < 10:
        return None, None, "‚ùå Donn√©es insuffisantes (minimum 10 mesures)"
    
    arr = np.array(numbers)
    n_points = len(arr)
    
    # G√©n√©rer grille si pas fournie
    if depths is None:
        # Estimer profondeurs selon r√©sistivit√© (0-100m typique)
        depths = np.array([estimate_depth_value(rho) for rho in arr])
    else:
        depths = np.array(depths)
    
    if distances is None:
        # Espacer uniform√©ment les mesures sur 100m
        distances = np.linspace(0, 100, n_points)
    else:
        distances = np.array(distances)
    
    # Cr√©er grille interpol√©e pour visualisation (style Res2DInv)
    grid_x = np.linspace(distances.min(), distances.max(), 100)
    grid_y = np.linspace(0, depths.max(), 50)
    grid_X, grid_Y = np.meshgrid(grid_x, grid_y)
    
    # Interpolation des valeurs sur la grille
    grid_rho = griddata((distances, depths), arr, (grid_X, grid_Y), method='cubic', fill_value=arr.mean())
    
    # Cr√©er palette de couleurs ERT standard (Res2DInv style)
    colors_ert = [
        '#000080',  # Bleu fonc√© - Tr√®s r√©sistif (>1000)
        '#0000FF',  # Bleu - R√©sistif (100-1000)
        '#00FFFF',  # Cyan - Mod√©r√©ment r√©sistif (10-100)
        '#00FF00',  # Vert - Neutre (1-10)
        '#FFFF00',  # Jaune - L√©g√®rement conducteur (0.1-1)
        '#FFA500',  # Orange - Conducteur (0.01-0.1)
        '#FF0000',  # Rouge - Tr√®s conducteur (0.001-0.01)
        '#8B0000',  # Rouge fonc√© - Ultra-conducteur (<0.001)
    ]
    cmap_ert = ListedColormap(colors_ert)
    
    # Cr√©er figure avec taille responsive
    if full_size:
        figsize = (30, 36)  # Grand format pour visualisation d√©taill√©e
        title_fontsize = 18
        label_fontsize = 14
        tick_fontsize = 11
    else:
        figsize = (20, 24)  # Taille standard
        title_fontsize = 14
        label_fontsize = 12
        tick_fontsize = 10
    
    fig = plt.figure(figsize=figsize)
    gs = fig.add_gridspec(5, 2, height_ratios=[1, 1, 1, 0.8, 1], hspace=0.3, wspace=0.3)
    
    # ========== GRAPHIQUE 1: PSEUDOSECTION R√âSISTIVIT√â APPARENTE ==========
    ax1 = fig.add_subplot(gs[0, :])
    
    # Utiliser √©chelle logarithmique
    im1 = ax1.contourf(grid_X, grid_Y, grid_rho, levels=20, cmap=cmap_ert, 
                       norm=LogNorm(vmin=max(arr.min(), 0.0001), vmax=arr.max()))
    ax1.scatter(distances, depths, c='black', s=20, marker='v', label='Points de mesure', zorder=10)
    
    ax1.set_xlabel('Distance (m)', fontsize=label_fontsize, weight='bold')
    ax1.set_ylabel('Profondeur (m)', fontsize=label_fontsize, weight='bold')
    ax1.set_title(f'1Ô∏è‚É£ PSEUDOSECTION - R√©sistivit√© Apparente\n{file_name}', 
                  fontsize=title_fontsize, weight='bold', pad=15)
    ax1.invert_yaxis()
    ax1.grid(True, alpha=0.3, linestyle='--')
    ax1.legend(loc='upper right', fontsize=tick_fontsize)
    ax1.tick_params(labelsize=tick_fontsize)
    
    # Colorbar avec labels g√©ologiques
    cbar1 = plt.colorbar(im1, ax=ax1, orientation='vertical', pad=0.02)
    cbar1.set_label('R√©sistivit√© (Œ©¬∑m)', fontsize=label_fontsize-1, weight='bold')
    cbar1.ax.tick_params(labelsize=tick_fontsize)
    
    # ========== GRAPHIQUE 2: MOD√àLE INVERS√â (avec contours) ==========
    ax2 = fig.add_subplot(gs[1, :])
    
    # Contours remplis + lignes de contour
    im2 = ax2.contourf(grid_X, grid_Y, grid_rho, levels=15, cmap=cmap_ert,
                       norm=LogNorm(vmin=max(arr.min(), 0.0001), vmax=arr.max()), alpha=0.9)
    contours = ax2.contour(grid_X, grid_Y, grid_rho, levels=10, colors='black', 
                          linewidths=0.5, alpha=0.4)
    ax2.clabel(contours, inline=True, fontsize=tick_fontsize-2, fmt='%.2f')
    
    ax2.set_xlabel('Distance (m)', fontsize=label_fontsize, weight='bold')
    ax2.set_ylabel('Profondeur (m)', fontsize=label_fontsize, weight='bold')
    ax2.set_title('2Ô∏è‚É£ MOD√àLE INVERS√â - Section avec Contours', 
                  fontsize=title_fontsize, weight='bold', pad=15)
    ax2.invert_yaxis()
    ax2.grid(True, alpha=0.2, linestyle=':')
    ax2.tick_params(labelsize=tick_fontsize)
    
    cbar2 = plt.colorbar(im2, ax=ax2, orientation='vertical', pad=0.02)
    cbar2.set_label('R√©sistivit√© (Œ©¬∑m)', fontsize=label_fontsize-1, weight='bold')
    cbar2.ax.tick_params(labelsize=tick_fontsize)
    
    # ========== GRAPHIQUE 3: COUPE VERTICALE COLOR√âE (style g√©ologique) ==========
    ax3 = fig.add_subplot(gs[2, :])
    
    # Version sans contours, couleurs pleines (style g√©ologique)
    im3 = ax3.imshow(grid_rho, aspect='auto', cmap=cmap_ert, 
                     norm=LogNorm(vmin=max(arr.min(), 0.0001), vmax=arr.max()),
                     extent=[distances.min(), distances.max(), depths.max(), 0],
                     interpolation='bilinear')
    
    # Ajouter annotations pour zones int√©ressantes
    # Trouver zones ultra-conductrices (m√©taux, sulfures)
    ultra_cond = arr < 1
    if ultra_cond.any():
        ultra_idx = np.where(ultra_cond)[0]
        for idx in ultra_idx[:5]:  # Max 5 annotations
            ax3.annotate('‚≠ê Anomalie', 
                        xy=(distances[idx], depths[idx]),
                        xytext=(distances[idx]+5, depths[idx]-5),
                        fontsize=tick_fontsize+1, color='white', weight='bold',
                        bbox=dict(boxstyle='round,pad=0.3', facecolor='red', alpha=0.7),
                        arrowprops=dict(arrowstyle='->', color='white', lw=1.5))
    
    ax3.set_xlabel('Distance (m)', fontsize=label_fontsize, weight='bold')
    ax3.set_ylabel('Profondeur (m)', fontsize=label_fontsize, weight='bold')
    ax3.set_title('3Ô∏è‚É£ COUPE G√âOLOGIQUE - Interpr√©tation Visuelle', 
                  fontsize=title_fontsize, weight='bold', pad=15)
    ax3.grid(True, alpha=0.3, color='white', linestyle='--')
    ax3.tick_params(labelsize=tick_fontsize)
    
    cbar3 = plt.colorbar(im3, ax=ax3, orientation='vertical', pad=0.02)
    cbar3.set_label('R√©sistivit√© (Œ©¬∑m)', fontsize=label_fontsize-1, weight='bold')
    cbar3.ax.tick_params(labelsize=tick_fontsize)
    
    # ========== GRAPHIQUE 4: HISTOGRAMME + PALETTE DE COULEURS ==========
    ax4a = fig.add_subplot(gs[3, 0])
    ax4b = fig.add_subplot(gs[3, 1])
    
    # Histogramme logarithmique
    log_rho = np.log10(arr)
    ax4a.hist(log_rho, bins=30, color='steelblue', edgecolor='black', alpha=0.7)
    ax4a.axvline(np.median(log_rho), color='red', linestyle='--', linewidth=2, label=f'M√©diane: {10**np.median(log_rho):.2f} Œ©¬∑m')
    ax4a.axvline(np.mean(log_rho), color='orange', linestyle='--', linewidth=2, label=f'Moyenne: {10**np.mean(log_rho):.2f} Œ©¬∑m')
    
    ax4a.set_xlabel('log‚ÇÅ‚ÇÄ(R√©sistivit√©)', fontsize=label_fontsize-1, weight='bold')
    ax4a.set_ylabel('Fr√©quence', fontsize=label_fontsize-1, weight='bold')
    ax4a.set_title('4Ô∏è‚É£a DISTRIBUTION des Valeurs', fontsize=title_fontsize-2, weight='bold')
    ax4a.legend(loc='upper right', fontsize=tick_fontsize-1)
    ax4a.grid(True, alpha=0.3)
    ax4a.tick_params(labelsize=tick_fontsize)
    
    # Palette de couleurs avec plages
    resistivity_ranges = [
        (0.0001, 0.001, '#8B0000', 'Ultra-conducteur\n(M√©taux natifs)'),
        (0.001, 0.01, '#FF0000', 'Tr√®s conducteur\n(Sulfures)'),
        (0.01, 0.1, '#FFA500', 'Conducteur\n(Eau sal√©e)'),
        (0.1, 1, '#FFFF00', 'L√©g√®rement cond.\n(Argiles humides)'),
        (1, 10, '#00FF00', 'Neutre\n(Eau douce)'),
        (10, 100, '#00FFFF', 'Mod√©r√©ment r√©s.\n(Sables/Graviers)'),
        (100, 1000, '#0000FF', 'R√©sistif\n(Roches s√®ches)'),
        (1000, 10000, '#000080', 'Tr√®s r√©sistif\n(Granite/Quartz)'),
    ]
    
    ax4b.axis('off')
    y_pos = 0.95
    palette_fontsize = tick_fontsize if not full_size else tick_fontsize + 2
    for rho_min, rho_max, color, label in resistivity_ranges:
        # Compter combien de mesures dans cette plage
        count = np.sum((arr >= rho_min) & (arr < rho_max))
        percentage = (count / len(arr)) * 100
        
        # Rectangle de couleur
        rect = plt.Rectangle((0.1, y_pos-0.08), 0.15, 0.08, facecolor=color, 
                            edgecolor='black', linewidth=1.5, transform=ax4b.transAxes)
        ax4b.add_patch(rect)
        
        # Texte
        ax4b.text(0.27, y_pos-0.04, f'{rho_min}-{rho_max} Œ©¬∑m', 
                 fontsize=palette_fontsize-1, va='center', weight='bold', transform=ax4b.transAxes)
        ax4b.text(0.65, y_pos-0.04, label, 
                 fontsize=palette_fontsize-2, va='center', transform=ax4b.transAxes)
        ax4b.text(0.92, y_pos-0.04, f'{percentage:.1f}%', 
                 fontsize=palette_fontsize-2, va='center', weight='bold', ha='right', transform=ax4b.transAxes)
        
        y_pos -= 0.12
    
    ax4b.set_title('4Ô∏è‚É£b PALETTE DE COULEURS ERT', fontsize=title_fontsize-2, weight='bold', pad=10)
    ax4b.set_xlim(0, 1)
    ax4b.set_ylim(0, 1)
    
    # ========== GRAPHIQUE 5: PROFIL 1D VERTICAL ==========
    ax5 = fig.add_subplot(gs[4, :])
    
    # Profil moyen par tranche de profondeur
    depth_bins = np.linspace(0, depths.max(), 20)
    depth_centers = (depth_bins[:-1] + depth_bins[1:]) / 2
    rho_profile = []
    
    for i in range(len(depth_bins)-1):
        mask = (depths >= depth_bins[i]) & (depths < depth_bins[i+1])
        if mask.any():
            rho_profile.append(np.mean(arr[mask]))
        else:
            rho_profile.append(np.nan)
    
    rho_profile = np.array(rho_profile)
    
    # Profil principal
    ax5.plot(rho_profile, depth_centers, 'b-o', linewidth=2, markersize=8, 
            label='Profil moyen', zorder=5)
    
    # Plage min-max (enveloppe)
    rho_min_profile = []
    rho_max_profile = []
    for i in range(len(depth_bins)-1):
        mask = (depths >= depth_bins[i]) & (depths < depth_bins[i+1])
        if mask.any():
            rho_min_profile.append(np.min(arr[mask]))
            rho_max_profile.append(np.max(arr[mask]))
        else:
            rho_min_profile.append(np.nan)
            rho_max_profile.append(np.nan)
    
    ax5.fill_betweenx(depth_centers, rho_min_profile, rho_max_profile, 
                      alpha=0.3, color='blue', label='Plage min-max')
    
    # Zones g√©ologiques
    ax5.axhspan(0, 20, alpha=0.1, color='red', label='Zone superficielle')
    ax5.axhspan(20, 50, alpha=0.1, color='yellow', label='Zone interm√©diaire')
    ax5.axhspan(50, depths.max(), alpha=0.1, color='blue', label='Zone profonde')
    
    ax5.set_xlabel('R√©sistivit√© moyenne (Œ©¬∑m)', fontsize=label_fontsize, weight='bold')
    ax5.set_ylabel('Profondeur (m)', fontsize=label_fontsize, weight='bold')
    ax5.set_title('5Ô∏è‚É£ PROFIL 1D - Variation avec la Profondeur', 
                  fontsize=title_fontsize, weight='bold', pad=15)
    ax5.set_xscale('log')
    ax5.invert_yaxis()
    ax5.grid(True, alpha=0.3, linestyle='--', which='both')
    ax5.legend(loc='best', fontsize=tick_fontsize)
    ax5.tick_params(labelsize=tick_fontsize)
    
    # Titre g√©n√©ral
    suptitle_fontsize = title_fontsize + 2 if full_size else title_fontsize + 2
    fig.suptitle(f'üìä ANALYSE ERT COMPL√àTE - {file_name}\n'
                f'{len(arr)} mesures | Plage: {arr.min():.4f} - {arr.max():.2f} Œ©¬∑m', 
                fontsize=suptitle_fontsize, weight='bold', y=0.995)
    
    plt.tight_layout(rect=[0, 0, 1, 0.99])
    
    # G√©n√©rer rapport textuel
    format_text = "GRAND FORMAT (30√ó36\")" if full_size else "STANDARD (20√ó24\")"
    rapport = f"""
üé® RAPPORT GRAPHIQUES ERT PROFESSIONNELS
{'='*80}

üìÅ Fichier: {file_name}
üìä Nombre de mesures: {len(arr)}
üìè Profondeur max: {depths.max():.1f} m
üìê Distance totale: {distances.max():.1f} m
üìà R√©sistivit√©: {arr.min():.6f} - {arr.max():.2f} Œ©¬∑m
üñºÔ∏è  Format: {format_text}

üé® GRAPHIQUES G√âN√âR√âS:

1Ô∏è‚É£ PSEUDOSECTION - R√©sistivit√© apparente
   ‚Ä¢ Donn√©es brutes interpol√©es sur grille 100x50
   ‚Ä¢ Points de mesure affich√©s
   ‚Ä¢ √âchelle logarithmique
   ‚Ä¢ Palette Res2DInv standard

2Ô∏è‚É£ MOD√àLE INVERS√â - Section avec contours
   ‚Ä¢ 15 niveaux de remplissage
   ‚Ä¢ 10 lignes de contour annot√©es
   ‚Ä¢ Interpr√©tation g√©ophysique

3Ô∏è‚É£ COUPE G√âOLOGIQUE - Interpr√©tation visuelle
   ‚Ä¢ Couleurs pleines (style g√©ologique)
   ‚Ä¢ Annotations des anomalies conductrices
   ‚Ä¢ Interpolation bilin√©aire

4Ô∏è‚É£ DISTRIBUTION & PALETTE
   ‚Ä¢ Histogramme logarithmique (30 bins)
   ‚Ä¢ Statistiques: m√©diane={10**np.median(log_rho):.2f} Œ©¬∑m, moyenne={10**np.mean(log_rho):.2f} Œ©¬∑m
   ‚Ä¢ Palette 8 couleurs avec r√©partition (%)

5Ô∏è‚É£ PROFIL 1D VERTICAL
   ‚Ä¢ 20 tranches de profondeur
   ‚Ä¢ Profil moyen + enveloppe min-max
   ‚Ä¢ Zones g√©ologiques identifi√©es

{'='*80}
"""
    
    grid_data = {
        'grid_X': grid_X,
        'grid_Y': grid_Y,
        'grid_rho': grid_rho,
        'distances': distances,
        'depths': depths,
        'resistivities': arr
    }
    
    return fig, grid_data, rapport

def estimate_depth_value(rho: float) -> float:
    """
    Estime une profondeur typique bas√©e sur la r√©sistivit√©
    Utilis√© pour g√©n√©rer profondeur si non fournie
    """
    if rho < 1:
        return np.random.uniform(0, 20)  # Zone superficielle conductrice
    elif rho < 10:
        return np.random.uniform(5, 40)  # Zone moyenne
    elif rho < 100:
        return np.random.uniform(15, 60)  # Zone transition
    elif rho < 1000:
        return np.random.uniform(30, 80)  # Zone profonde mod√©r√©e
    else:
        return np.random.uniform(50, 100)  # Zone tr√®s profonde r√©sistive


def generate_professional_ert_report(
    numbers: list,
    file_name: str,
    mineral_report: str = "",
    df_corr: pd.DataFrame = None,
    fig_ert: plt.Figure = None,
    fig_corr: plt.Figure = None,
    grid_data: dict = None,
    output_path: str = None
) -> bytes:
    """
    üé® G√âN√âRATION RAPPORT PDF PROFESSIONNEL COMPLET
    
    Cr√©e un rapport PDF avec:
    - Page de garde avec logo et titre color√©
    - R√©sum√© ex√©cutif
    - Graphiques ERT int√©gr√©s (5 coupes)
    - Tableau de correspondances
    - Interpr√©tation g√©ologique d√©taill√©e
    - Recommandations
    - Annexes techniques
    
    Args:
        numbers: Valeurs de r√©sistivit√©
        file_name: Nom du fichier analys√©
        mineral_report: Texte du rapport min√©ralogique
        df_corr: DataFrame des correspondances
        fig_ert: Figure matplotlib des 5 graphiques ERT
        fig_corr: Figure matplotlib du tableau
        grid_data: Donn√©es de grille interpol√©e
        output_path: Chemin de sauvegarde (si None, retourne bytes)
    
    Returns:
        bytes: Contenu du PDF
    """
    from reportlab.lib.pagesizes import A4, landscape
    from reportlab.lib.units import cm, mm
    from reportlab.lib import colors
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.platypus import (
        SimpleDocTemplate, Paragraph, Spacer, Image as RLImage,
        Table, TableStyle, PageBreak, KeepTogether
    )
    from reportlab.lib.enums import TA_CENTER, TA_JUSTIFY, TA_LEFT, TA_RIGHT
    from reportlab.pdfgen import canvas
    from datetime import datetime
    import io
    import tempfile
    
    # Buffer pour le PDF
    buffer = io.BytesIO()
    
    # Cr√©er document
    if output_path:
        doc = SimpleDocTemplate(output_path, pagesize=A4,
                               topMargin=2*cm, bottomMargin=2*cm,
                               leftMargin=2*cm, rightMargin=2*cm)
    else:
        doc = SimpleDocTemplate(buffer, pagesize=A4,
                               topMargin=2*cm, bottomMargin=2*cm,
                               leftMargin=2*cm, rightMargin=2*cm)
    
    # Styles
    styles = getSampleStyleSheet()
    
    # Style titre principal (rouge)
    title_style = ParagraphStyle(
        'CustomTitle',
        parent=styles['Heading1'],
        fontSize=24,
        textColor=colors.HexColor('#8B0000'),
        spaceAfter=30,
        alignment=TA_CENTER,
        fontName='Helvetica-Bold'
    )
    
    # Style sous-titre (bleu)
    subtitle_style = ParagraphStyle(
        'CustomSubtitle',
        parent=styles['Heading2'],
        fontSize=18,
        textColor=colors.HexColor('#000080'),
        spaceAfter=20,
        spaceBefore=20,
        alignment=TA_CENTER,
        fontName='Helvetica-Bold'
    )
    
    # Style section (vert fonc√©)
    section_style = ParagraphStyle(
        'CustomSection',
        parent=styles['Heading2'],
        fontSize=16,
        textColor=colors.HexColor('#006400'),
        spaceAfter=12,
        spaceBefore=20,
        fontName='Helvetica-Bold',
        borderWidth=2,
        borderColor=colors.HexColor('#006400'),
        borderPadding=5,
        backColor=colors.HexColor('#F0FFF0')
    )
    
    # Style paragraphe justifi√©
    justified_style = ParagraphStyle(
        'Justified',
        parent=styles['BodyText'],
        fontSize=11,
        alignment=TA_JUSTIFY,
        spaceAfter=12,
        leading=14
    )
    
    # Style liste
    bullet_style = ParagraphStyle(
        'Bullet',
        parent=styles['BodyText'],
        fontSize=10,
        leftIndent=20,
        bulletIndent=10,
        spaceAfter=6
    )
    
    # Statistiques
    arr = np.array(numbers)
    stats = {
        'n_mesures': len(arr),
        'min': arr.min(),
        'max': arr.max(),
        'mean': arr.mean(),
        'median': np.median(arr),
        'std': arr.std()
    }
    
    # Contenu du PDF
    story = []
    
    # ========== PAGE DE GARDE ==========
    story.append(Spacer(1, 3*cm))
    
    story.append(Paragraph("RAPPORT D'INVESTIGATION", title_style))
    story.append(Paragraph("TOMOGRAPHIE DE R√âSISTIVIT√â √âLECTRIQUE (ERT)", subtitle_style))
    
    story.append(Spacer(1, 2*cm))
    
    # Bo√Æte d'information
    info_data = [
        ['Fichier analys√©:', file_name],
        ['Date du rapport:', datetime.now().strftime('%d/%m/%Y %H:%M')],
        ['Nombre de mesures:', f"{stats['n_mesures']}"],
        ['Plage de r√©sistivit√©:', f"{stats['min']:.4f} - {stats['max']:.2f} Œ©¬∑m"],
        ['Type d\'analyse:', 'Investigation compl√®te avec IA']
    ]
    
    info_table = Table(info_data, colWidths=[7*cm, 9*cm])
    info_table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (0, -1), colors.HexColor('#E6F3FF')),
        ('BACKGROUND', (1, 0), (1, -1), colors.white),
        ('TEXTCOLOR', (0, 0), (-1, -1), colors.black),
        ('ALIGN', (0, 0), (0, -1), 'RIGHT'),
        ('ALIGN', (1, 0), (1, -1), 'LEFT'),
        ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),
        ('FONTNAME', (1, 0), (1, -1), 'Helvetica'),
        ('FONTSIZE', (0, 0), (-1, -1), 11),
        ('GRID', (0, 0), (-1, -1), 1, colors.HexColor('#4682B4')),
        ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
        ('TOPPADDING', (0, 0), (-1, -1), 8),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 8),
    ]))
    
    story.append(info_table)
    story.append(Spacer(1, 2*cm))
    
    # Logo/watermark
    story.append(Paragraph(
        "<font color='#808080' size=10><i>G√©n√©r√© par Kibali AI - Syst√®me Expert ERT</i></font>",
        ParagraphStyle('footer', parent=styles['Normal'], alignment=TA_CENTER)
    ))
    
    story.append(PageBreak())
    
    # ========== R√âSUM√â EX√âCUTIF ==========
    story.append(Paragraph("1. R√âSUM√â EX√âCUTIF", section_style))
    
    # D√©terminer interpr√©tation principale
    if stats['mean'] < 1:
        interpretation = "zone fortement conductrice sugg√©rant la pr√©sence de sulfures m√©talliques, graphite ou argiles satur√©es"
        color_indicator = "üî¥"
    elif stats['mean'] < 10:
        interpretation = "zone conductrice typique d'eau sal√©e, argiles humides ou min√©raux hydrat√©s"
        color_indicator = "üü†"
    elif stats['mean'] < 100:
        interpretation = "zone de r√©sistivit√© mod√©r√©e caract√©ristique d'eau douce, sables ou roches alt√©r√©es"
        color_indicator = "üü¢"
    else:
        interpretation = "zone r√©sistive indiquant des roches consolid√©es, granite ou calcaire"
        color_indicator = "üîµ"
    
    executive_summary = f"""
    L'investigation g√©ophysique par tomographie de r√©sistivit√© √©lectrique (ERT) du site <b>{file_name}</b> 
    a permis d'acqu√©rir <b>{stats['n_mesures']} mesures</b> sur le terrain. L'analyse r√©v√®le une {interpretation}.
    <br/><br/>
    {color_indicator} <b>R√©sistivit√© moyenne: {stats['mean']:.2f} Œ©¬∑m</b> (√©cart-type: {stats['std']:.2f})
    <br/><br/>
    Les valeurs varient de <b>{stats['min']:.4f} Œ©¬∑m</b> (minimum) √† <b>{stats['max']:.2f} Œ©¬∑m</b> (maximum), 
    avec une m√©diane de <b>{stats['median']:.2f} Œ©¬∑m</b>. Cette distribution statistique permet d'identifier 
    plusieurs horizons g√©ologiques distincts et de localiser des anomalies significatives pour l'exploration mini√®re.
    """
    
    story.append(Paragraph(executive_summary, justified_style))
    story.append(Spacer(1, 0.5*cm))
    
    # ========== STATISTIQUES CL√âS ==========
    story.append(Paragraph("2. STATISTIQUES DESCRIPTIVES", section_style))
    
    stats_data = [
        ['<b>Param√®tre</b>', '<b>Valeur</b>', '<b>Interpr√©tation</b>'],
        ['Nombre de mesures', f"{stats['n_mesures']}", 'Excellente couverture spatiale'],
        ['Minimum', f"{stats['min']:.6f} Œ©¬∑m", 'Zone ultra-conductrice d√©tect√©e'],
        ['Maximum', f"{stats['max']:.2f} Œ©¬∑m", 'Zone r√©sistive identifi√©e'],
        ['Moyenne', f"{stats['mean']:.2f} Œ©¬∑m", 'Valeur centrale de la distribution'],
        ['M√©diane', f"{stats['median']:.2f} Œ©¬∑m", 'Valeur m√©diane (50e percentile)'],
        ['√âcart-type', f"{stats['std']:.2f} Œ©¬∑m", 'Variabilit√© mod√©r√©e du sous-sol'],
    ]
    
    stats_table = Table(stats_data, colWidths=[5*cm, 4*cm, 7*cm])
    stats_table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#006400')),
        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
        ('FONTSIZE', (0, 0), (-1, 0), 12),
        ('FONTSIZE', (0, 1), (-1, -1), 10),
        ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
        ('GRID', (0, 0), (-1, -1), 1, colors.black),
        ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
        ('TOPPADDING', (0, 0), (-1, -1), 8),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 8),
    ]))
    
    story.append(stats_table)
    story.append(Spacer(1, 0.5*cm))
    
    story.append(PageBreak())
    
    # ========== GRAPHIQUES ERT (5 COUPES) ==========
    if fig_ert is not None:
        story.append(Paragraph("3. COUPES ERT PROFESSIONNELLES", section_style))
        
        # Explication des graphiques
        ert_explanation = """
        Les cinq graphiques suivants pr√©sentent une analyse compl√®te de la distribution de r√©sistivit√© 
        dans le sous-sol. Chaque repr√©sentation offre une perspective compl√©mentaire pour l'interpr√©tation 
        g√©ologique et la localisation des cibles d'exploration.
        """
        story.append(Paragraph(ert_explanation, justified_style))
        story.append(Spacer(1, 0.3*cm))
        
        # Descriptions des graphiques
        graph_descriptions = [
            ("<b>1Ô∏è‚É£ Pseudosection</b>", "Repr√©sentation de la r√©sistivit√© apparente mesur√©e sur le terrain. "
             "Les points noirs indiquent les positions des √©lectrodes. Cette vue montre les donn√©es brutes avant inversion."),
            
            ("<b>2Ô∏è‚É£ Mod√®le invers√©</b>", "Section apr√®s traitement par inversion g√©ophysique. "
             "Les lignes de contour annot√©es facilitent la lecture quantitative des valeurs de r√©sistivit√©."),
            
            ("<b>3Ô∏è‚É£ Coupe g√©ologique</b>", "Interpr√©tation visuelle avec annotations des anomalies majeures (‚≠ê). "
             "Les zones ultra-conductrices (<1 Œ©¬∑m) sont marqu√©es pour investigation prioritaire."),
            
            ("<b>4Ô∏è‚É£ Distribution statistique</b>", "Histogramme logarithmique montrant la fr√©quence des valeurs. "
             "La palette de 8 couleurs correspond aux standards Res2DInv avec pourcentages de distribution."),
            
            ("<b>5Ô∏è‚É£ Profil vertical 1D</b>", "√âvolution de la r√©sistivit√© avec la profondeur. "
             "L'enveloppe min-max montre la variabilit√© lat√©rale. Les zones g√©ologiques sont color√©es par profondeur.")
        ]
        
        for title, desc in graph_descriptions:
            story.append(Paragraph(f"‚Ä¢ {title}: {desc}", bullet_style))
        
        story.append(Spacer(1, 0.5*cm))
        
        # Sauvegarder figure ERT en haute r√©solution
        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp_ert:
            fig_ert.savefig(tmp_ert.name, format='png', dpi=200, bbox_inches='tight')
            tmp_ert_path = tmp_ert.name
        
        # Ins√©rer image (mode paysage pour les 5 graphiques)
        story.append(PageBreak())
        ert_img = RLImage(tmp_ert_path, width=18*cm, height=21*cm)
        story.append(ert_img)
        story.append(Spacer(1, 0.3*cm))
        story.append(Paragraph(
            "<i>Figure 1: Ensemble complet des 5 coupes ERT professionnelles (style Res2DInv)</i>",
            ParagraphStyle('caption', parent=styles['Normal'], fontSize=9, alignment=TA_CENTER, textColor=colors.HexColor('#666666'))
        ))
        
        os.unlink(tmp_ert_path)  # Nettoyer fichier temporaire
    
    story.append(PageBreak())
    
    # ========== TABLEAU DE CORRESPONDANCES ==========
    if df_corr is not None and not df_corr.empty:
        story.append(Paragraph("4. CORRESPONDANCES MIN√âRALES", section_style))
        
        corr_explanation = """
        Le tableau suivant √©tablit les correspondances entre les valeurs de r√©sistivit√© mesur√©es et les 
        mat√©riaux g√©ologiques potentiels. Le niveau de confiance (0-100%) refl√®te la position de la mesure 
        dans la plage de r√©sistivit√© caract√©ristique de chaque min√©ral.
        """
        story.append(Paragraph(corr_explanation, justified_style))
        story.append(Spacer(1, 0.5*cm))
        
        if fig_corr is not None:
            # Ins√©rer graphique scatter + table
            with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp_corr:
                fig_corr.savefig(tmp_corr.name, format='png', dpi=200, bbox_inches='tight')
                tmp_corr_path = tmp_corr.name
            
            corr_img = RLImage(tmp_corr_path, width=17*cm, height=13*cm)
            story.append(corr_img)
            story.append(Spacer(1, 0.3*cm))
            story.append(Paragraph(
                "<i>Figure 2: Tableau de correspondances et scatter plot des mesures r√©elles</i>",
                ParagraphStyle('caption', parent=styles['Normal'], fontSize=9, alignment=TA_CENTER, textColor=colors.HexColor('#666666'))
            ))
            
            os.unlink(tmp_corr_path)
        
        # Top 10 correspondances en tableau
        story.append(Spacer(1, 0.5*cm))
        story.append(Paragraph("<b>Top 10 Correspondances Identifi√©es:</b>", ParagraphStyle('bold', parent=styles['Normal'], fontName='Helvetica-Bold')))
        story.append(Spacer(1, 0.2*cm))
        
        top10 = df_corr.nlargest(10, 'Confiance')[['Mat√©riau', 'R√©sistivit√© mesur√©e (Œ©¬∑m)', 'Confiance', 'Profondeur (m)']]
        
        table_data = [['<b>Mat√©riau</b>', '<b>R√©sistivit√© (Œ©¬∑m)</b>', '<b>Confiance</b>', '<b>Profondeur (m)</b>']]
        for _, row in top10.iterrows():
            table_data.append([
                row['Mat√©riau'],
                f"{row['R√©sistivit√© mesur√©e (Œ©¬∑m)']:.4f}",
                f"{row['Confiance']*100:.0f}%",
                f"{row['Profondeur (m)']:.1f}"
            ])
        
        corr_table = Table(table_data, colWidths=[6*cm, 4*cm, 3*cm, 3*cm])
        corr_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#8B0000')),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
            ('FONTSIZE', (0, 0), (-1, 0), 10),
            ('FONTSIZE', (0, 1), (-1, -1), 9),
            ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.HexColor('#F5F5F5')]),
            ('GRID', (0, 0), (-1, -1), 0.5, colors.grey),
            ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
            ('TOPPADDING', (0, 0), (-1, -1), 6),
            ('BOTTOMPADDING', (0, 0), (-1, -1), 6),
        ]))
        
        story.append(corr_table)
    
    story.append(PageBreak())
    
    # ========== INTERPR√âTATION G√âOLOGIQUE ==========
    story.append(Paragraph("5. INTERPR√âTATION G√âOLOGIQUE D√âTAILL√âE", section_style))
    
    # Analyse par plages de r√©sistivit√©
    ranges_analysis = [
        (0, 1, "Ultra-conducteur", "Sulfures m√©talliques, graphite, argiles satur√©es en eau sal√©e"),
        (1, 10, "Fortement conducteur", "Eau sal√©e, argiles humides, schistes graphiteux"),
        (10, 100, "Mod√©r√©ment conducteur", "Eau douce, sables satur√©s, roches alt√©r√©es"),
        (100, 1000, "Mod√©r√©ment r√©sistif", "Sables secs, graviers, roches consolid√©es"),
        (1000, float('inf'), "Tr√®s r√©sistif", "Granite, quartz, calcaire compact, roches ign√©es")
    ]
    
    story.append(Paragraph("<b>5.1 Analyse par horizons de r√©sistivit√©:</b>", ParagraphStyle('subsection', parent=styles['Heading3'], fontSize=12, textColor=colors.HexColor('#00008B'))))
    story.append(Spacer(1, 0.3*cm))
    
    for rho_min, rho_max, label, materials in ranges_analysis:
        count = np.sum((arr >= rho_min) & (arr < rho_max))
        percentage = (count / len(arr)) * 100
        
        if count > 0:
            range_text = f"""
            <b>{label} ({rho_min}-{rho_max} Œ©¬∑m)</b>: {count} mesures ({percentage:.1f}%)
            <br/>
            <i>Mat√©riaux probables: {materials}</i>
            """
            story.append(Paragraph(range_text, bullet_style))
            story.append(Spacer(1, 0.2*cm))
    
    # Anomalies d√©tect√©es
    story.append(Spacer(1, 0.3*cm))
    story.append(Paragraph("<b>5.2 Anomalies g√©ophysiques majeures:</b>", ParagraphStyle('subsection', parent=styles['Heading3'], fontSize=12, textColor=colors.HexColor('#00008B'))))
    story.append(Spacer(1, 0.3*cm))
    
    anomalies = []
    
    # Anomalie conductrice
    ultra_cond = arr < 1
    if ultra_cond.any():
        n_ultra = ultra_cond.sum()
        anomalies.append(f"üî¥ <b>{n_ultra} zones ultra-conductrices</b> (œÅ < 1 Œ©¬∑m) - Cibles prioritaires pour exploration mini√®re (sulfures, or associ√©)")
    
    # Anomalie r√©sistive
    ultra_res = arr > 1000
    if ultra_res.any():
        n_ultra_res = ultra_res.sum()
        anomalies.append(f"üîµ <b>{n_ultra_res} zones tr√®s r√©sistives</b> (œÅ > 1000 Œ©¬∑m) - Roches cristallines, granite, quartz massif")
    
    # Zones interm√©diaires
    water_zone = (arr >= 10) & (arr <= 100)
    if water_zone.any():
        n_water = water_zone.sum()
        anomalies.append(f"üü¢ <b>{n_water} zones aquif√®res potentielles</b> (10-100 Œ©¬∑m) - Eau douce, sables satur√©s")
    
    if not anomalies:
        anomalies.append("‚ÑπÔ∏è Aucune anomalie majeure d√©tect√©e - Distribution homog√®ne")
    
    for anomaly in anomalies:
        story.append(Paragraph(f"‚Ä¢ {anomaly}", bullet_style))
        story.append(Spacer(1, 0.2*cm))
    
    story.append(PageBreak())
    
    # ========== RECOMMANDATIONS ==========
    story.append(Paragraph("6. RECOMMANDATIONS ET PERSPECTIVES", section_style))
    
    recommendations = f"""
    Sur la base de l'analyse g√©ophysique ERT, les recommandations suivantes sont propos√©es:
    <br/><br/>
    <b>6.1 Investigations compl√©mentaires:</b>
    <br/>
    ‚Ä¢ Sondages carott√©s aux emplacements des anomalies ultra-conductrices (œÅ < 1 Œ©¬∑m)
    <br/>
    ‚Ä¢ Prospection g√©ochimique (√©chantillonnage sol) sur les zones √† fort potentiel
    <br/>
    ‚Ä¢ Polarisation provoqu√©e (IP) pour confirmer la pr√©sence de sulfures m√©talliques
    <br/>
    ‚Ä¢ Lev√© magn√©tique pour compl√©ter la signature g√©ophysique
    <br/><br/>
    <b>6.2 Ciblage minier:</b>
    <br/>
    ‚Ä¢ Priorit√© 1: Zones œÅ < 1 Œ©¬∑m (potentiel sulfures massifs)
    <br/>
    ‚Ä¢ Priorit√© 2: Transitions brusques de r√©sistivit√© (contacts lithologiques)
    <br/>
    ‚Ä¢ Priorit√© 3: Zones 10-100 Œ©¬∑m si contexte aquif√®re recherch√©
    <br/><br/>
    <b>6.3 Mod√©lisation 3D:</b>
    <br/>
    ‚Ä¢ Extension du profil 2D vers une couverture surfacique (grille 3D)
    <br/>
    ‚Ä¢ Inversion 3D pour mod√®le volum√©trique complet du sous-sol
    <br/>
    ‚Ä¢ Corr√©lation avec donn√©es g√©ologiques de surface et forages existants
    """
    
    story.append(Paragraph(recommendations, justified_style))
    
    story.append(PageBreak())
    
    # ========== ANNEXES TECHNIQUES ==========
    story.append(Paragraph("7. ANNEXES TECHNIQUES", section_style))
    
    story.append(Paragraph("<b>7.1 M√©thodologie ERT:</b>", ParagraphStyle('subsection', parent=styles['Heading3'], fontSize=12)))
    story.append(Spacer(1, 0.2*cm))
    
    methodology = """
    La tomographie de r√©sistivit√© √©lectrique (ERT) est une m√©thode g√©ophysique non-invasive qui mesure 
    la r√©sistivit√© √©lectrique du sous-sol. Des √©lectrodes sont implant√©es selon un profil lin√©aire, et des 
    mesures de r√©sistance sont effectu√©es entre diff√©rentes combinaisons d'√©lectrodes (dispositif Wenner, 
    Schlumberger, dip√¥le-dip√¥le, etc.). Les donn√©es sont ensuite invers√©es pour obtenir un mod√®le 2D de 
    distribution de r√©sistivit√© en profondeur.
    """
    story.append(Paragraph(methodology, justified_style))
    
    story.append(Spacer(1, 0.5*cm))
    story.append(Paragraph("<b>7.2 Param√®tres d'acquisition:</b>", ParagraphStyle('subsection', parent=styles['Heading3'], fontSize=12)))
    story.append(Spacer(1, 0.2*cm))
    
    acq_data = [
        ['Nombre de mesures:', f"{stats['n_mesures']}"],
        ['Plage de mesure:', f"{stats['min']:.6f} - {stats['max']:.2f} Œ©¬∑m"],
        ['Espacement √©lectrodes:', '√Ä d√©terminer selon fichier .dat'],
        ['Dispositif utilis√©:', '√Ä d√©terminer (Wenner/Schlumberger/DD)'],
        ['Profondeur investigation:', f"Estim√©e: {max(50, stats['n_mesures']*0.2):.0f} m"],
    ]
    
    acq_table = Table(acq_data, colWidths=[8*cm, 8*cm])
    acq_table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (0, -1), colors.HexColor('#E6E6FA')),
        ('ALIGN', (0, 0), (0, -1), 'RIGHT'),
        ('ALIGN', (1, 0), (1, -1), 'LEFT'),
        ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),
        ('FONTSIZE', (0, 0), (-1, -1), 10),
        ('GRID', (0, 0), (-1, -1), 0.5, colors.grey),
        ('TOPPADDING', (0, 0), (-1, -1), 6),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 6),
    ]))
    
    story.append(acq_table)
    
    story.append(Spacer(1, 0.5*cm))
    story.append(Paragraph("<b>7.3 Palette de couleurs standard:</b>", ParagraphStyle('subsection', parent=styles['Heading3'], fontSize=12)))
    story.append(Spacer(1, 0.2*cm))
    
    palette_info = """
    Les graphiques utilisent la palette standard Res2DInv √† 8 couleurs:
    Rouge fonc√© (#8B0000) ‚Üí Rouge ‚Üí Orange ‚Üí Jaune ‚Üí Vert ‚Üí Cyan ‚Üí Bleu ‚Üí Bleu fonc√© (#000080).
    L'√©chelle logarithmique permet de visualiser efficacement la large gamme de r√©sistivit√©s (0.0001 - 10000 Œ©¬∑m).
    """
    story.append(Paragraph(palette_info, justified_style))
    
    # Footer final
    story.append(Spacer(1, 2*cm))
    story.append(Paragraph(
        "‚îÄ" * 80,
        ParagraphStyle('line', parent=styles['Normal'], alignment=TA_CENTER)
    ))
    story.append(Paragraph(
        f"<font color='#808080' size=8>Rapport g√©n√©r√© automatiquement le {datetime.now().strftime('%d/%m/%Y √† %H:%M:%S')}<br/>"
        "Kibali AI - Syst√®me Expert d'Investigation G√©ophysique ERT<br/>"
        "Pour toute question technique: support@kibali-ai.local</font>",
        ParagraphStyle('footer', parent=styles['Normal'], alignment=TA_CENTER, fontSize=8)
    ))
    
    # G√©n√©rer PDF
    doc.build(story)
    
    if output_path:
        with open(output_path, 'rb') as f:
            return f.read()
    else:
        buffer.seek(0)
        return buffer.getvalue()

def analyze_minerals_from_resistivity(numbers: list, file_name: str = "unknown") -> str:
    """
    Analyse compl√®te des min√©raux pr√©sents bas√©e sur les valeurs de r√©sistivit√©
    G√©n√®re un rapport d√©taill√© avec clustering, interpr√©tation g√©ologique et calculs
    """
    if not numbers or len(numbers) < 10:
        return "‚ùå Donn√©es insuffisantes pour analyse min√©rale (minimum 10 mesures requises)"
    
    import numpy as np
    from sklearn.cluster import KMeans
    
    arr = np.array(numbers)
    minerals_db = create_minerals_database()
    
    report = "üî¨ RAPPORT COMPLET D'ANALYSE MIN√âRALE ERT\n"
    report += "=" * 80 + "\n\n"
    
    report += f"üìÅ Fichier analys√©: {file_name}\n"
    report += f"üìä Nombre de mesures: {len(arr)}\n"
    report += f"üìà Plage de r√©sistivit√©: {np.min(arr):.4f} - {np.max(arr):.2f} Œ©¬∑m\n\n"
    
    # Ajouter le tableau de r√©f√©rence de l'eau
    report += get_water_resistivity_color_table() + "\n\n"
    
    # 1Ô∏è‚É£ CLUSTERING AUTOMATIQUE
    report += "1Ô∏è‚É£ CLUSTERING K-MEANS DES R√âSISTIVIT√âS\n"
    report += "‚îÄ" * 80 + "\n"
    
    # D√©terminer nombre optimal de clusters (2-6 bas√© sur variance)
    n_clusters = min(5, max(2, int(np.sqrt(len(arr) / 20))))
    
    try:
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        clusters = kmeans.fit_predict(arr.reshape(-1, 1))
        cluster_centers = kmeans.cluster_centers_.flatten()
        
        report += f"‚úÖ {n_clusters} clusters identifi√©s\n\n"
        
        # Trier par r√©sistivit√©
        sorted_indices = np.argsort(cluster_centers)
        
        for i, idx in enumerate(sorted_indices):
            center = cluster_centers[idx]
            count = np.sum(clusters == idx)
            percentage = (count / len(arr)) * 100
            
            report += f"üéØ Cluster {i+1} (œÅ moyenne = {center:.3f} Œ©¬∑m)\n"
            report += f"   ‚Ä¢ Nombre de mesures: {count} ({percentage:.1f}%)\n"
            report += f"   ‚Ä¢ R√©sistivit√©: {arr[clusters == idx].min():.3f} - {arr[clusters == idx].max():.3f} Œ©¬∑m\n"
            
            # Correspondance min√©raux
            matches = minerals_db[
                (minerals_db["Plage Min (Œ©m)"] <= center) & 
                (minerals_db["Plage Max (Œ©m)"] >= center)
            ]
            
            if not matches.empty:
                report += f"   ‚Ä¢ Min√©raux/Mat√©riaux compatibles:\n"
                for _, match in matches.iterrows():
                    report += f"     - {match['Type']} ({match['Cat√©gorie']}): {match['Notes']}\n"
            else:
                report += f"   ‚Ä¢ ‚ö†Ô∏è Aucune correspondance exacte dans la base\n"
            
            # Calculs g√©ophysiques
            conductivity = 1000 / center if center > 0 else float('inf')  # mS/m
            report += f"   ‚Ä¢ Conductivit√© calcul√©e: {conductivity:.2f} mS/m\n"
            report += f"   ‚Ä¢ Profondeur estim√©e: {estimate_depth_from_rho(center)}\n\n"
        
    except Exception as e:
        report += f"‚ùå Erreur clustering: {str(e)}\n\n"
    
    # 2Ô∏è‚É£ ANALYSE PAR CAT√âGORIE
    report += "2Ô∏è‚É£ CLASSIFICATION PAR CAT√âGORIE G√âOPHYSIQUE\n"
    report += "‚îÄ" * 80 + "\n"
    
    # Cat√©gories bas√©es sur r√©sistivit√© avec codes couleur
    ultra_conductors = arr[arr < 0.01]
    conductors = arr[(arr >= 0.01) & (arr < 10)]
    semi_conductors = arr[(arr >= 10) & (arr < 100)]
    resistive = arr[(arr >= 100) & (arr < 1000)]
    highly_resistive = arr[arr >= 1000]
    
    categories = [
        ("Ultra-conducteurs (<0.01 Œ©¬∑m)", ultra_conductors, "M√©taux natifs (or, argent, cuivre), graphite", "üü£ Violet/Noir"),
        ("Conducteurs (0.01-10 Œ©¬∑m)", conductors, "Sulfures (pyrite, galena, chalcopyrite), eau sal√©e, nappes", "üî¥ Rouge/üü† Orange"),
        ("Semi-conducteurs (10-100 Œ©¬∑m)", semi_conductors, "Argile humide, eau douce, certains oxydes", "üü° Jaune/üü¢ Vert"),
        ("R√©sistifs (100-1000 Œ©¬∑m)", resistive, "Gr√®s, calcaire, sphalerite", "üîµ Bleu clair"),
        ("Tr√®s r√©sistifs (>1000 Œ©¬∑m)", highly_resistive, "Granite, quartz, air/vides, eau tr√®s pure", "üîµ Bleu fonc√©")
    ]
    
    for cat_name, cat_data, typical_materials, color_code in categories:
        count = len(cat_data)
        percentage = (count / len(arr)) * 100
        
        if count > 0:
            report += f"üìä {cat_name} - {color_code}\n"
            report += f"   ‚Ä¢ Mesures: {count} ({percentage:.1f}%)\n"
            report += f"   ‚Ä¢ Moyenne: {np.mean(cat_data):.3f} Œ©¬∑m\n"
            report += f"   ‚Ä¢ Mat√©riaux typiques: {typical_materials}\n\n"
    
    # üìä ANALYSE SP√âCIFIQUE DE L'EAU
    report += "üíß ANALYSE D√âTAILL√âE DES TYPES D'EAU\n"
    report += "‚îÄ" * 80 + "\n"
    
    water_categories = [
        {
            "type": "Eau de mer",
            "range": (0.1, 1.0),
            "color": "üî¥ Rouge vif / üü† Orange",
            "description": "Haute conductivit√©, salinit√© >35 g/L",
            "applications": "Zones c√¥ti√®res, intrusions salines"
        },
        {
            "type": "Eau sal√©e (nappe)",
            "range": (1.0, 10.0),
            "color": "üü† Jaune / üü† Orange",
            "description": "Salinit√© mod√©r√©e 1-10 g/L",
            "applications": "Nappes contamin√©es, zones arides"
        },
        {
            "type": "Eau douce",
            "range": (10.0, 100.0),
            "color": "üü¢ Vert / üîµ Bleu clair",
            "description": "Eau potable, faible salinit√© <1 g/L",
            "applications": "Aquif√®res exploitables, rivi√®res"
        },
        {
            "type": "Eau tr√®s pure",
            "range": (100.0, float('inf')),
            "color": "üîµ Bleu fonc√©",
            "description": "Eau d√©min√©ralis√©e, pluie r√©cente",
            "applications": "Zones non satur√©es, pr√©cipitations"
        }
    ]
    
    water_detected = False
    for water_cat in water_categories:
        water_zone = arr[(arr >= water_cat["range"][0]) & (arr < water_cat["range"][1])]
        count = len(water_zone)
        percentage = (count / len(arr)) * 100
        
        if count > 0:
            water_detected = True
            report += f"üíß **{water_cat['type']}** ({water_cat['range'][0]}-{water_cat['range'][1]} Œ©¬∑m) - {water_cat['color']}\n"
            report += f"   ‚Ä¢ Mesures: {count} ({percentage:.1f}%)\n"
            report += f"   ‚Ä¢ Moyenne: {np.mean(water_zone):.3f} Œ©¬∑m\n"
            report += f"   ‚Ä¢ Description: {water_cat['description']}\n"
            report += f"   ‚Ä¢ Applications: {water_cat['applications']}\n\n"
    
    if not water_detected:
        report += "‚ö†Ô∏è Aucune signature d'eau claire d√©tect√©e dans les mesures\n"
        report += "   Possible: Zone tr√®s s√®che, substrat rocheux, ou min√©ralisation dominante\n\n"
    else:
        report += "‚úÖ Signatures hydriques identifi√©es - Possible nappe phr√©atique ou circulation d'eau\n\n"
    
    # 3Ô∏è‚É£ D√âTECTION D'ANOMALIES MIN√âRALES
    report += "3Ô∏è‚É£ D√âTECTION D'ANOMALIES POUR EXPLORATION MINI√àRE\n"
    report += "‚îÄ" * 80 + "\n"
    
    anomalies_detected = []
    
    # Anomalie sulfures (tr√®s conducteurs)
    sulfure_zone = arr[arr < 1]
    if len(sulfure_zone) > 0:
        anomalies_detected.append({
            "type": "Zone sulfur√©e potentielle",
            "count": len(sulfure_zone),
            "rho_range": f"{np.min(sulfure_zone):.4f} - {np.max(sulfure_zone):.3f} Œ©¬∑m",
            "minerals": "Pyrite, Chalcopyrite, Galena, Bornite",
            "interest": "‚≠ê‚≠ê‚≠ê HAUT - Exploration Cu, Pb, Zn, Au associ√©"
        })
    
    # Anomalie m√©taux pr√©cieux
    metal_zone = arr[arr < 0.01]
    if len(metal_zone) > 0:
        anomalies_detected.append({
            "type": "Zone m√©taux natifs potentielle",
            "count": len(metal_zone),
            "rho_range": f"{np.min(metal_zone):.6f} - {np.max(metal_zone):.4f} Œ©¬∑m",
            "minerals": "Or natif, Argent, Cuivre, Graphite",
            "interest": "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê TR√àS HAUT - Exploration m√©taux pr√©cieux"
        })
    
    # Anomalie oxydes de fer
    iron_zone = arr[(arr >= 10) & (arr <= 1000)]
    if len(iron_zone) > len(arr) * 0.1:  # >10% des mesures
        anomalies_detected.append({
            "type": "Zone oxydes de fer",
            "count": len(iron_zone),
            "rho_range": f"{np.min(iron_zone):.2f} - {np.max(iron_zone):.2f} Œ©¬∑m",
            "minerals": "Magn√©tite, H√©matite",
            "interest": "‚≠ê‚≠ê MOYEN - Exploration fer, indicateur alt√©ration"
        })
    
    if anomalies_detected:
        for i, anomaly in enumerate(anomalies_detected, 1):
            report += f"üéØ Anomalie {i}: {anomaly['type']}\n"
            report += f"   ‚Ä¢ Mesures affect√©es: {anomaly['count']} ({anomaly['count']/len(arr)*100:.1f}%)\n"
            report += f"   ‚Ä¢ Plage de r√©sistivit√©: {anomaly['rho_range']}\n"
            report += f"   ‚Ä¢ Min√©raux probables: {anomaly['minerals']}\n"
            report += f"   ‚Ä¢ Int√©r√™t √©conomique: {anomaly['interest']}\n\n"
    else:
        report += "‚ö†Ô∏è Aucune anomalie min√©rale majeure d√©tect√©e\n\n"
    
    # 4Ô∏è‚É£ RECOMMANDATIONS D'EXPLORATION
    report += "4Ô∏è‚É£ RECOMMANDATIONS POUR EXPLORATION\n"
    report += "‚îÄ" * 80 + "\n"
    
    if len(sulfure_zone) > 0:
        report += "‚úÖ PRIORIT√â 1: Forage cibl√© sur zones sulfur√©es (<1 Œ©¬∑m)\n"
        report += "   ‚Ä¢ Profondeur recommand√©e: 50-200m\n"
        report += "   ‚Ä¢ Analyses g√©ochimiques: Cu, Pb, Zn, Au, Ag\n"
        report += "   ‚Ä¢ M√©thodes compl√©mentaires: IP (Polarisation Induite), Magn√©tom√©trie\n\n"
    
    if len(metal_zone) > 0:
        report += "‚úÖ PRIORIT√â 2: Investigation m√©taux pr√©cieux (<0.01 Œ©¬∑m)\n"
        report += "   ‚Ä¢ Technique: √âchantillonnage par tranch√©es\n"
        report += "   ‚Ä¢ Analyses: Fire assay pour Au, ICP-MS pour √©l√©ments traces\n\n"
    
    # Recommandations hydrog√©ologiques
    water_conductors = arr[(arr >= 0.1) & (arr <= 100)]
    if len(water_conductors) > 0:
        report += "üíß HYDROG√âOLOGIE: Investigation ressources en eau\n"
        report += "   ‚Ä¢ Zones identifi√©es avec signature hydrique\n"
        
        sea_water = arr[(arr >= 0.1) & (arr <= 1.0)]
        brackish_water = arr[(arr >= 1.0) & (arr <= 10.0)]
        fresh_water = arr[(arr >= 10.0) & (arr <= 100.0)]
        
        if len(sea_water) > 0:
            report += f"   ‚Ä¢ ‚ö†Ô∏è Eau sal√©e d√©tect√©e ({len(sea_water)} mesures): Risque intrusion marine\n"
        if len(brackish_water) > 0:
            report += f"   ‚Ä¢ üü° Eau saum√¢tre ({len(brackish_water)} mesures): Qualit√© mod√©r√©e\n"
        if len(fresh_water) > 0:
            report += f"   ‚Ä¢ ‚úÖ Eau douce ({len(fresh_water)} mesures): Aquif√®re potentiellement exploitable\n"
        
        report += "   ‚Ä¢ Recommandations:\n"
        report += "     - Forages de reconnaissance (30-150m)\n"
        report += "     - Analyses hydrochimiques (pH, TDS, ions majeurs)\n"
        report += "     - Essais de pompage pour transmissivit√©\n"
        report += "     - Monitoring pi√©zom√©trique temporel\n\n"
    
    report += "üìã M√©thodes ERT compl√©mentaires recommand√©es:\n"
    report += "   ‚Ä¢ Inversion 3D pour cartographie volum√©trique\n"
    report += "   ‚Ä¢ Mesures IP pour discrimination sulfures/oxydes\n"
    report += "   ‚Ä¢ Profils serr√©s (espacement <2m) sur anomalies\n"
    report += "   ‚Ä¢ Time-lapse ERT pour suivi temporel\n"
    report += "   ‚Ä¢ TDEM (Time Domain EM) pour profondeurs >200m\n\n"
    
    # 5Ô∏è‚É£ STATISTIQUES GLOBALES
    report += "5Ô∏è‚É£ STATISTIQUES GLOBALES DU FICHIER\n"
    report += "‚îÄ" * 80 + "\n"
    
    report += f"üìä R√©sistivit√© moyenne: {np.mean(arr):.3f} Œ©¬∑m\n"
    report += f"üìä M√©diane: {np.median(arr):.3f} Œ©¬∑m\n"
    report += f"üìä √âcart-type: {np.std(arr):.3f} Œ©¬∑m\n"
    report += f"üìä Coefficient de variation: {np.std(arr)/np.mean(arr):.3f}\n"
    report += f"üìä Range log: {np.log10(np.max(arr)/np.min(arr)):.2f} d√©cades\n\n"
    
    # Distribution g√©ologique probable
    mean_rho = np.median(arr)  # M√©diane plus robuste que moyenne
    if mean_rho < 10:
        geo_context = "Environnement conducteur: zone satur√©e, sulfures, alt√©ration hydrothermale"
    elif mean_rho < 100:
        geo_context = "Environnement mixte: sols, roches alt√©r√©es, transition vadose-phr√©atique"
    elif mean_rho < 1000:
        geo_context = "Environnement r√©sistif: roches compactes, zone non satur√©e"
    else:
        geo_context = "Environnement tr√®s r√©sistif: substratum cristallin, zones s√®ches"
    
    report += f"üåç Contexte g√©ologique probable: {geo_context}\n"
    
    report += "\n" + "=" * 80 + "\n"
    report += "‚úÖ ANALYSE MIN√âRALE COMPL√àTE TERMIN√âE\n"
    
    return report

def estimate_depth_from_rho(rho: float) -> str:
    """Estime la profondeur typique bas√©e sur la r√©sistivit√©"""
    if rho < 1:
        return "0-20m (zone conductrice superficielle ou min√©ralisation)"
    elif rho < 100:
        return "0-50m (zone vadose ou alt√©r√©e)"
    elif rho < 1000:
        return "20-100m (zone de transition ou roche fractur√©e)"
    else:
        return ">50m (substratum profond ou zone s√®che)"

def get_water_resistivity_color_table() -> str:
    """
    Retourne un tableau de r√©f√©rence des r√©sistivit√©s de l'eau avec codes couleur
    Bas√© sur les standards g√©ophysiques internationaux
    """
    table = """
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë         TABLEAU DE R√âF√âRENCE - R√âSISTIVIT√â DE L'EAU (Œ©¬∑m)                   ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë Type d'eau          ‚îÇ R√©sistivit√© (Œ©¬∑m)  ‚îÇ Couleur associ√©e                 ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë **Eau de mer**      ‚îÇ 0.1 - 1 Œ©¬∑m        ‚îÇ üî¥ Rouge vif / üü† Orange         ‚ïë
‚ïë                     ‚îÇ                    ‚îÇ (Haute conductivit√©)             ‚ïë
‚ï†‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï£
‚ïë **Eau sal√©e (nappe)**‚îÇ 1 - 10 Œ©¬∑m        ‚îÇ üü† Jaune / üü† Orange             ‚ïë
‚ïë                     ‚îÇ                    ‚îÇ (Salinit√© mod√©r√©e)               ‚ïë
‚ï†‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï£
‚ïë **Eau douce**       ‚îÇ 10 - 100 Œ©¬∑m       ‚îÇ üü¢ Vert / üîµ Bleu clair          ‚ïë
‚ïë                     ‚îÇ                    ‚îÇ (Potable, exploitable)           ‚ïë
‚ï†‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï£
‚ïë **Eau tr√®s pure**   ‚îÇ > 100 Œ©¬∑m          ‚îÇ üîµ Bleu fonc√©                    ‚ïë
‚ïë                     ‚îÇ                    ‚îÇ (D√©min√©ralis√©e)                  ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Notes:
‚Ä¢ Les couleurs sont indicatives et d√©pendent de la palette utilis√©e (Res2DInv, etc.)
‚Ä¢ La r√©sistivit√© de l'eau varie avec: temp√©rature, salinit√©, pH, min√©raux dissous
‚Ä¢ Eau de mer: ~0.2 Œ©¬∑m (35 g/L sel) vs Eau pure: >1000 Œ©¬∑m (<1 mg/L TDS)
‚Ä¢ Zone de transition douce/sal√©e: 10-30 Œ©¬∑m (m√©lange, interface)
"""
    return table

# ========================================
# EXTRACTION PDF & OCR POUR RAPPORTS ERT
# ========================================

def generate_annotations_with_ocr(image_path: str, label_output_path: str, preview: bool = False) -> bool:
    """
    G√©n√®re des annotations YOLO √† partir d'OCR sur une image
    D√©tecte texte, valeurs de r√©sistivit√©, l√©gendes min√©rales
    """
    image = cv2.imread(image_path)
    if image is None:
        return False

    h, w, _ = image.shape
    try:
        data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)
    except Exception as e:
        st.warning(f"Erreur OCR: {e}")
        return False

    found = False
    resistivity_values = []
    
    for i in range(len(data['text'])):
        text = data['text'][i].strip()
        if not text:
            continue
        
        # D√©tecter valeurs de r√©sistivit√© (patterns: "123.45", "0.001", etc.)
        try:
            value = float(text.replace(',', '.'))
            if 0.000001 <= value <= 1e15:  # Plage r√©sistivit√© valide
                resistivity_values.append(value)
        except:
            pass
        
        x, y, bw, bh = data['left'][i], data['top'][i], data['width'][i], data['height'][i]
        x_center = (x + bw / 2) / w
        y_center = (y + bh / 2) / h
        bw_norm, bh_norm = bw / w, bh / h
        
        with open(label_output_path, "a") as label_file:
            label_file.write(f"0 {x_center:.6f} {y_center:.6f} {bw_norm:.6f} {bh_norm:.6f}\n")
        found = True
        
        if preview:
            cv2.rectangle(image, (x, y), (x + bw, y + bh), (0, 255, 0), 2)

    if found and preview:
        st.image(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), caption=f"OCR: {os.path.basename(image_path)}")
    
    return found, resistivity_values

def extract_captions_near_images(pdf, page, page_num: int, pdf_name: str, output_dir: str) -> list:
    """
    Extrait les l√©gendes textuelles proches des images dans un PDF
    Utile pour r√©cup√©rer descriptions de profils ERT, annotations g√©ologiques
    """
    try:
        blocks = page.get_text("blocks")
        images = page.get_images(full=True)
        captions = []

        for img in images:
            xref = img[0]
            bbox = page.get_image_bbox(xref)
            
            # Chercher texte proche de l'image (dans un rayon de 80 pixels)
            for b in blocks:
                bx0, by0, bx1, by1, text, *_ = b
                # Texte en dessous ou au-dessus de l'image
                if abs(by0 - bbox.y1) < 80 or abs(bbox.y0 - by1) < 80:
                    if len(text.strip()) > 5:
                        captions.append({
                            "text": text.strip(),
                            "position": (bx0, by0, bx1, by1),
                            "image_bbox": (bbox.x0, bbox.y0, bbox.x1, bbox.y1)
                        })

        if captions:
            caption_file = os.path.join(output_dir, f"{pdf_name}_page{page_num+1}_captions.txt")
            with open(caption_file, "w", encoding="utf-8") as f:
                for cap in captions:
                    f.write(cap["text"] + "\n")
            
        return captions
    except Exception as e:
        st.warning(f"Erreur extraction l√©gendes page {page_num+1}: {e}")
        return []

def extract_image_map(pdf, page, page_num: int, pdf_name: str, output_dir: str) -> dict:
    """
    G√©n√®re une carte JSON des images pr√©sentes dans la page
    Utile pour indexer positions de profils ERT, cartes de localisation
    """
    try:
        images = page.get_images(full=True)
        map_data = []
        
        for img in images:
            xref = img[0]
            bbox = page.get_image_bbox(xref)
            map_data.append({
                "xref": xref,
                "bbox": [bbox.x0, bbox.y0, bbox.x1, bbox.y1],
                "page": page_num + 1,
                "width": bbox.x1 - bbox.x0,
                "height": bbox.y1 - bbox.y0
            })
        
        if map_data:
            map_file = os.path.join(output_dir, f"{pdf_name}_page{page_num+1}_map.json")
            with open(map_file, "w") as f:
                json.dump(map_data, f, indent=2)
        
        return map_data
    except Exception as e:
        st.warning(f"Erreur g√©n√©ration carte d'images: {e}")
        return []

def extract_drawings(pdf, page, page_num: int, pdf_name: str, output_dir: str) -> bool:
    """
    Extrait les √©l√©ments vectoriels (graphiques, courbes, croquis)
    Utile pour extraire profils ERT vectoriels, graphiques de r√©sistivit√©
    """
    try:
        drawings = page.get_drawings()
        if drawings:
            # Convertir page en image pour sauvegarder les dessins
            pix = page.get_pixmap()
            drawing_file = os.path.join(output_dir, f"{pdf_name}_page{page_num+1}_drawings.png")
            pix.save(drawing_file)
            return True
        return False
    except Exception as e:
        st.warning(f"Erreur extraction dessins: {e}")
        return False

def extract_ert_report_from_pdf(pdf_path: str, output_base_dir: str = None) -> dict:
    """
    üî¨ EXTRACTION COMPL√àTE DE RAPPORT ERT DEPUIS PDF
    
    Extrait automatiquement:
    - Profils de r√©sistivit√© (images)
    - L√©gendes min√©rales/g√©ologiques  
    - Cartes de localisation
    - Tableaux de mesures
    - Valeurs de r√©sistivit√© par OCR
    - Graphiques vectoriels
    
    Returns:
        dict avec chemins des fichiers extraits et m√©tadonn√©es
    """
    if output_base_dir is None:
        output_base_dir = "/tmp/ert_extracted"
    
    os.makedirs(output_base_dir, exist_ok=True)
    
    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]
    
    # Cr√©er dossiers de sortie
    images_dir = os.path.join(output_base_dir, "images")
    text_dir = os.path.join(output_base_dir, "text")
    data_dir = os.path.join(output_base_dir, "data")
    
    for d in [images_dir, text_dir, data_dir]:
        os.makedirs(d, exist_ok=True)
    
    extraction_results = {
        "pdf_name": pdf_name,
        "images": [],
        "captions": [],
        "maps": [],
        "drawings": [],
        "resistivity_values": [],
        "full_text": ""
    }
    
    try:
        pdf = fitz.open(pdf_path)
        st.info(f"üìÑ Extraction PDF: {pdf_name} ({len(pdf)} pages)")
        
        full_text = ""
        progress_bar = st.progress(0)
        
        for page_num in range(len(pdf)):
            page = pdf[page_num]
            
            # 1Ô∏è‚É£ Extraire texte complet
            page_text = page.get_text()
            full_text += f"\n=== Page {page_num+1} ===\n{page_text}"
            
            # 2Ô∏è‚É£ Extraire l√©gendes proches des images
            captions = extract_captions_near_images(pdf, page, page_num, pdf_name, text_dir)
            extraction_results["captions"].extend(captions)
            
            # 3Ô∏è‚É£ G√©n√©rer carte des images
            image_map = extract_image_map(pdf, page, page_num, pdf_name, data_dir)
            extraction_results["maps"].append(image_map)
            
            # 4Ô∏è‚É£ Extraire dessins vectoriels
            has_drawings = extract_drawings(pdf, page, page_num, pdf_name, images_dir)
            if has_drawings:
                extraction_results["drawings"].append(f"page_{page_num+1}")
            
            # 5Ô∏è‚É£ Extraire images et appliquer OCR
            images = page.get_images(full=True)
            for img_index, img in enumerate(images):
                xref = img[0]
                base_image = pdf.extract_image(xref)
                image_bytes = base_image["image"]
                
                image_filename = f"{pdf_name}_page{page_num+1}_img{img_index}.png"
                image_path = os.path.join(images_dir, image_filename)
                
                with open(image_path, "wb") as img_file:
                    img_file.write(image_bytes)
                
                extraction_results["images"].append(image_path)
                
                # OCR pour extraire valeurs de r√©sistivit√©
                label_path = os.path.join(data_dir, f"{pdf_name}_page{page_num+1}_img{img_index}.txt")
                found, resistivity_vals = generate_annotations_with_ocr(image_path, label_path, preview=False)
                
                if resistivity_vals:
                    extraction_results["resistivity_values"].extend(resistivity_vals)
            
            progress_bar.progress((page_num + 1) / len(pdf))
        
        # Sauvegarder texte complet
        text_file = os.path.join(text_dir, f"{pdf_name}_full_text.txt")
        with open(text_file, "w", encoding="utf-8") as f:
            f.write(full_text)
        
        extraction_results["full_text"] = full_text
        
        # Sauvegarder m√©tadonn√©es JSON
        metadata_file = os.path.join(output_base_dir, f"{pdf_name}_metadata.json")
        with open(metadata_file, "w") as f:
            json.dump({
                "pdf_name": pdf_name,
                "total_pages": len(pdf),
                "total_images": len(extraction_results["images"]),
                "total_captions": len(extraction_results["captions"]),
                "resistivity_values_found": len(extraction_results["resistivity_values"]),
                "output_dir": output_base_dir
            }, f, indent=2)
        
        pdf.close()
        
        st.success(f"‚úÖ Extraction termin√©e: {len(extraction_results['images'])} images, {len(extraction_results['resistivity_values'])} valeurs de r√©sistivit√©")
        
        return extraction_results
        
    except Exception as e:
        st.error(f"‚ùå Erreur extraction PDF: {e}")
        return extraction_results

def process_audio_transcription(audio_path: str, output_text_path: str = None) -> str:
    """
    üé§ Transcription audio avec Whisper
    Utile pour notes vocales de g√©ologues sur le terrain
    """
    try:
        st.info(f"üé§ Transcription audio en cours...")
        model = whisper.load_model("base")
        result = model.transcribe(audio_path)
        
        transcription = result["text"]
        
        if output_text_path:
            with open(output_text_path, "w", encoding="utf-8") as f:
                f.write(transcription)
        
        st.success(f"‚úÖ Audio transcrit: {len(transcription)} caract√®res")
        return transcription
        
    except Exception as e:
        st.error(f"‚ùå Erreur transcription audio: {e}")
        return ""

def deep_binary_investigation(file_bytes: bytes, file_name: str = "unknown") -> str:
    """
    üîç FOUILLE INTELLIGENTE DE FICHIER BINAIRE
    Combine Hex+ASCII Dump + Base Vectorielle RAG + Base ERT pour interpr√©tation scientifique compl√®te
    Similaire √† l'agent VSCode avec todo list mais pour l'analyse binaire
    """
    investigation_report = "üî¨ RAPPORT D'INVESTIGATION BINAIRE APPROFONDIE\n"
    investigation_report += "=" * 80 + "\n\n"
    
    # 1Ô∏è‚É£ EXTRACTION INITIALE (Hex + ASCII)
    investigation_report += "1Ô∏è‚É£ PHASE 1: EXTRACTION HEX + ASCII\n"
    investigation_report += "‚îÄ" * 80 + "\n"
    hex_dump = hex_ascii_view(file_bytes, bytes_per_line=16, max_lines=100)
    investigation_report += f"üìú Dump hexad√©cimal ({len(file_bytes)} bytes):\n"
    investigation_report += f"{hex_dump[:500]}...\n\n"
    
    # Extraction des nombres
    numbers = extract_numbers(file_bytes)
    investigation_report += f"üî¢ Nombres extraits: {len(numbers)} valeurs\n"
    if numbers:
        import numpy as np
        arr = np.array(numbers)
        investigation_report += f"   ‚Ä¢ Range: {np.min(arr):.3f} - {np.max(arr):.3f}\n"
        investigation_report += f"   ‚Ä¢ Moyenne: {np.mean(arr):.3f} ¬± {np.std(arr):.3f}\n"
        investigation_report += f"   ‚Ä¢ M√©diane: {np.median(arr):.3f}\n\n"
    
    # 2Ô∏è‚É£ ANALYSES TECHNIQUES (entropie, patterns, m√©tadonn√©es)
    investigation_report += "2Ô∏è‚É£ PHASE 2: ANALYSES TECHNIQUES\n"
    investigation_report += "‚îÄ" * 80 + "\n"
    
    entropy_result = entropy_analysis(file_bytes)
    pattern_result = pattern_recognition(file_bytes)
    metadata_result = metadata_extraction(file_bytes)
    compression_result = compression_ratio(file_bytes)
    frequency_result = frequency_analysis(file_bytes)
    
    investigation_report += f"üìä Entropie: {entropy_result}\n"
    investigation_report += f"üéØ Patterns: {pattern_result}\n"
    investigation_report += f"üìã M√©tadonn√©es: {metadata_result}\n"
    investigation_report += f"üóúÔ∏è Compression: {compression_result}\n"
    investigation_report += f"üìà Fr√©quences: {frequency_result}\n\n"
    
    # 3Ô∏è‚É£ FOUILLE DANS LA BASE VECTORIELLE RAG
    investigation_report += "3Ô∏è‚É£ PHASE 3: FOUILLE BASE VECTORIELLE RAG\n"
    investigation_report += "‚îÄ" * 80 + "\n"
    
    rag_queries = []
    # Construire des requ√™tes intelligentes bas√©es sur les patterns d√©tect√©s
    if "ELF" in pattern_result or "executable" in pattern_result.lower():
        rag_queries.append("analyse fichier ex√©cutable binaire ELF format Linux s√©curit√©")
    if "JPEG" in pattern_result or "PNG" in pattern_result:
        rag_queries.append("format image JPEG PNG m√©tadonn√©es EXIF analyse forensique")
    if "PDF" in pattern_result:
        rag_queries.append("structure PDF analyse document m√©tadonn√©es forensique")
    if numbers and len(numbers) > 10:
        import numpy as np
        arr = np.array(numbers)
        if 0.1 <= np.min(arr) <= 10000:
            rag_queries.append("ERT electrical resistivity tomography geophysics data interpretation")
            rag_queries.append("r√©sistivit√© √©lectrique tomographie g√©ophysique inversion subsurface")
    
    # Requ√™te g√©n√©rique bas√©e sur l'entropie
    if "haute" in entropy_result.lower() or "high" in entropy_result.lower():
        rag_queries.append("fichier chiffr√© crypt√© haute entropie analyse cryptographique")
    else:
        rag_queries.append("fichier donn√©es structur√©es format binaire analyse")
    
    # Fouiller dans RAG pour chaque requ√™te
    rag_findings = ""
    
    # V√©rifier si la base vectorielle existe et est initialis√©e
    has_vectorstore = False
    try:
        has_vectorstore = hasattr(st.session_state, 'vectorstore') and st.session_state.vectorstore is not None
    except:
        has_vectorstore = False
    
    if has_vectorstore:
        investigation_report += f"‚úÖ Base vectorielle d√©tect√©e - {len(rag_queries)} requ√™tes planifi√©es\n\n"
        for i, query in enumerate(rag_queries[:3], 1):  # Limiter √† 3 requ√™tes pour performance
            try:
                result = search_vectorstore(query)
                if result and len(result) > 50:  # √âviter r√©sultats vides
                    rag_findings += f"üîç Requ√™te {i}/3: '{query[:60]}...'\n"
                    rag_findings += f"   üìÑ R√©sultat: {result[:300]}...\n\n"
                else:
                    rag_findings += f"üîç Requ√™te {i}/3: '{query[:60]}...'\n"
                    rag_findings += f"   ‚ö†Ô∏è Aucun r√©sultat pertinent\n\n"
            except Exception as e:
                rag_findings += f"üîç Requ√™te {i}/3: '{query[:60]}...'\n"
                rag_findings += f"   ‚ùå Erreur: {str(e)}\n\n"
        
        if rag_findings:
            investigation_report += rag_findings
        else:
            investigation_report += "‚ö†Ô∏è Aucun r√©sultat trouv√© dans la base RAG\n\n"
    else:
        investigation_report += "‚ö†Ô∏è Base vectorielle RAG non disponible\n"
        investigation_report += "üí° Conseil: Uploadez et indexez des PDFs dans la sidebar pour enrichir l'analyse\n\n"
    
    
    # 4Ô∏è‚É£ FOUILLE SP√âCIALIS√âE ERT (si donn√©es num√©riques d√©tect√©es)
    investigation_report += "4Ô∏è‚É£ PHASE 4: FOUILLE SP√âCIALIS√âE ERT\n"
    investigation_report += "‚îÄ" * 80 + "\n"
    
    mineral_report = ""
    if numbers and len(numbers) > 10:
        import numpy as np
        arr = np.array(numbers)
        ert_detection = ert_data_detection(file_bytes, numbers)
        investigation_report += ert_detection + "\n"
        
        # üÜï ANALYSE MIN√âRALE COMPL√àTE
        investigation_report += "\nüî¨ ANALYSE MIN√âRALE APPROFONDIE\n"
        investigation_report += "‚îÄ" * 80 + "\n"
        
        try:
            mineral_report = analyze_minerals_from_resistivity(numbers, file_name)
            investigation_report += mineral_report + "\n"
        except Exception as e:
            investigation_report += f"‚ùå Erreur lors de l'analyse min√©rale: {str(e)}\n\n"
        
        # üÜï TABLEAU DE CORRESPONDANCES R√âELLES
        investigation_report += "\nüìä TABLEAU DE CORRESPONDANCES R√âELLES\n"
        investigation_report += "‚îÄ" * 80 + "\n"
        
        try:
            # Option mode grand format pour le tableau
            st.markdown("### üìä Tableau de Correspondances Min√©rales")
            col_tbl1, col_tbl2 = st.columns([1, 1])
            with col_tbl1:
                use_fullsize_table = st.checkbox("üìà Mode GRAND FORMAT Tableau", value=False, 
                                                help="Agrandit le tableau et le scatter plot pour meilleure lisibilit√©")
            
            fig_corr, df_corr, rapport_corr = create_real_mineral_correspondence_table(
                numbers, 
                file_name,
                full_size=use_fullsize_table
            )
            
            if fig_corr and df_corr is not None:
                # Affichage responsive du graphique
                st.pyplot(fig_corr, use_container_width=True)
                
                # Boutons t√©l√©chargement pour le graphique tableau
                col_dl1, col_dl2, col_dl3 = st.columns(3)
                with col_dl1:
                    import io
                    buf_table_png = io.BytesIO()
                    fig_corr.savefig(buf_table_png, format='png', dpi=300, bbox_inches='tight')
                    buf_table_png.seek(0)
                    st.download_button(
                        label="üì• Tableau PNG 300 DPI",
                        data=buf_table_png,
                        file_name=f"{file_name}_correspondances_300dpi.png",
                        mime="image/png",
                        key="dl_table_png"
                    )
                
                with col_dl2:
                    buf_table_pdf = io.BytesIO()
                    fig_corr.savefig(buf_table_pdf, format='pdf', bbox_inches='tight')
                    buf_table_pdf.seek(0)
                    st.download_button(
                        label="üìÑ Tableau PDF",
                        data=buf_table_pdf,
                        file_name=f"{file_name}_correspondances.pdf",
                        mime="application/pdf",
                        key="dl_table_pdf"
                    )
                
                with col_dl3:
                    # CSV du dataframe
                    csv_data = df_corr.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        label="üì• CSV Donn√©es",
                        data=csv_data,
                        file_name=f"{file_name}_correspondances.csv",
                        mime="text/csv",
                        key="dl_csv"
                    )
                
                plt.close(fig_corr)
                
                # Afficher les donn√©es en plusieurs tableaux pour √©viter scroll excessif
                st.markdown("#### üìã Donn√©es Tabulaires - Organis√©es par Profondeur")
                
                # Corriger les pourcentages de confiance (convertir de 0-1 √† 0-100%)
                df_corr_display = df_corr.copy()
                if 'Confiance' in df_corr_display.columns:
                    # Si les valeurs sont entre 0 et 1, convertir en pourcentage
                    if df_corr_display['Confiance'].max() <= 1:
                        df_corr_display['Confiance (%)'] = (df_corr_display['Confiance'] * 100).round(1)
                    else:
                        df_corr_display['Confiance (%)'] = df_corr_display['Confiance'].round(1)
                    df_corr_display = df_corr_display.drop('Confiance', axis=1)
                
                # Organiser en 5 tableaux selon la profondeur
                total_rows = len(df_corr_display)
                if total_rows > 20:
                    # Diviser en 5 groupes de profondeur
                    depth_col = 'Profondeur (m)' if 'Profondeur (m)' in df_corr_display.columns else df_corr_display.columns[0]
                    df_sorted = df_corr_display.sort_values(depth_col)
                    
                    # Cr√©er 5 quantiles de profondeur
                    quantiles = [0, 0.2, 0.4, 0.6, 0.8, 1.0]
                    depth_ranges = df_sorted[depth_col].quantile(quantiles).values
                    
                    for i in range(5):
                        min_depth = depth_ranges[i]
                        max_depth = depth_ranges[i+1]
                        
                        # Filtrer les donn√©es dans cette plage de profondeur
                        if i == 4:  # Dernier groupe, inclure la valeur max
                            mask = (df_sorted[depth_col] >= min_depth) & (df_sorted[depth_col] <= max_depth)
                        else:
                            mask = (df_sorted[depth_col] >= min_depth) & (df_sorted[depth_col] < max_depth)
                        
                        df_section = df_sorted[mask]
                        
                        if len(df_section) > 0:
                            with st.expander(f"üìä Tableau {i+1}/5 - Profondeur: {min_depth:.1f} √† {max_depth:.1f} m ({len(df_section)} d√©tections)", expanded=(i==0)):
                                st.dataframe(
                                    df_section,
                                    use_container_width=True,
                                    column_config={
                                        "Confiance (%)": st.column_config.NumberColumn(
                                            "Confiance (%)",
                                            format="%.1f%%",
                                            help="Niveau de confiance de la correspondance (0-100%)"
                                        ),
                                        "R√©sistivit√© mesur√©e (Œ©¬∑m)": st.column_config.NumberColumn(
                                            "R√©sistivit√© mesur√©e (Œ©¬∑m)",
                                            format="%.6f"
                                        ),
                                        "Profondeur (m)": st.column_config.NumberColumn(
                                            "Profondeur (m)",
                                            format="%.1f"
                                        )
                                    },
                                    height=min(400, len(df_section) * 35 + 38)  # Hauteur adaptative
                                )
                                
                                # Statistiques du tableau
                                st.caption(f"üìà Stats: R√©sistivit√© moy. {df_section['R√©sistivit√© mesur√©e (Œ©¬∑m)'].mean():.4f} Œ©¬∑m | "
                                          f"Confiance moy. {df_section['Confiance (%)'].mean():.1f}%")
                else:
                    # Si moins de 20 lignes, afficher en un seul tableau
                    st.dataframe(
                        df_corr_display,
                        use_container_width=True,
                        column_config={
                            "Confiance (%)": st.column_config.NumberColumn(
                                "Confiance (%)",
                                format="%.1f%%",
                                help="Niveau de confiance de la correspondance (0-100%)"
                            ),
                            "R√©sistivit√© mesur√©e (Œ©¬∑m)": st.column_config.NumberColumn(
                                "R√©sistivit√© mesur√©e (Œ©¬∑m)",
                                format="%.6f"
                            ),
                            "Profondeur (m)": st.column_config.NumberColumn(
                                "Profondeur (m)",
                                format="%.1f"
                            )
                        }
                    )
                
                # Ajouter rapport textuel
                investigation_report += rapport_corr + "\n"
            else:
                investigation_report += rapport_corr + "\n"
                
        except Exception as e:
            investigation_report += f"‚ùå Erreur cr√©ation tableau correspondances: {str(e)}\n\n"
        
        # üÜï G√âN√âRATION COUPES ERT PROFESSIONNELLES (5 GRAPHIQUES)
        investigation_report += "\nüé® COUPES ERT PROFESSIONNELLES (Style Res2DInv)\n"
        investigation_report += "‚îÄ" * 80 + "\n"
        
        try:
            # Option mode grand format
            col_btn1, col_btn2 = st.columns([1, 1])
            with col_btn1:
                use_fullsize = st.checkbox("üñºÔ∏è Mode GRAND FORMAT (30√ó36 pouces)", value=False, 
                                          help="Activez pour g√©n√©rer des graphiques haute r√©solution pour impression A0/A1")
            
            fig_ert, grid_data, rapport_ert = create_ert_professional_sections(
                numbers,
                file_name,
                full_size=use_fullsize
            )
            
            if fig_ert is not None:
                st.markdown("### üé® Visualisations ERT Compl√®tes")
                
                # Affichage responsive avec use_container_width
                st.pyplot(fig_ert, use_container_width=True)
                
                # Boutons de t√©l√©chargement en colonnes
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    # T√©l√©charger en PNG haute r√©solution
                    import io
                    buf_png = io.BytesIO()
                    fig_ert.savefig(buf_png, format='png', dpi=300, bbox_inches='tight')
                    buf_png.seek(0)
                    st.download_button(
                        label="üì• PNG Haute R√©solution (300 DPI)",
                        data=buf_png,
                        file_name=f"{file_name}_ert_graphics_300dpi.png",
                        mime="image/png",
                        help="Format PNG 300 DPI pour impression professionnelle"
                    )
                
                with col2:
                    # T√©l√©charger en PDF vectoriel
                    buf_pdf = io.BytesIO()
                    fig_ert.savefig(buf_pdf, format='pdf', bbox_inches='tight')
                    buf_pdf.seek(0)
                    st.download_button(
                        label="üìÑ PDF Vectoriel",
                        data=buf_pdf,
                        file_name=f"{file_name}_ert_graphics.pdf",
                        mime="application/pdf",
                        help="Format PDF vectoriel pour documents techniques"
                    )
                
                with col3:
                    # T√©l√©charger grille de donn√©es
                    if grid_data:
                        import pickle
                        grid_pickle = pickle.dumps(grid_data)
                        st.download_button(
                            label="ÔøΩ Donn√©es Grille (PKL)",
                            data=grid_pickle,
                            file_name=f"{file_name}_grid_ert.pkl",
                            mime="application/octet-stream",
                            help="Donn√©es interpol√©es pour traitement ult√©rieur"
                        )
                
                plt.close(fig_ert)
                
                # ÔøΩÔøΩ G√âN√âRATION RAPPORT PDF PROFESSIONNEL COMPLET
                st.markdown("---")
                st.markdown("### üìÑ Rapport PDF Professionnel Complet")
                
                col_pdf1, col_pdf2 = st.columns([3, 1])
                with col_pdf1:
                    st.info("üé® Rapport PDF avec graphiques int√©gr√©s, titres color√©s, statistiques et recommandations")
                
                with col_pdf2:
                    generate_pdf_btn = st.button("üîÑ G√©n√©rer Rapport PDF", key="gen_pdf_investigation")
                
                if generate_pdf_btn:
                    with st.spinner("üìù G√©n√©ration du rapport PDF professionnel..."):
                        try:
                            pdf_bytes = generate_professional_ert_report(
                                numbers=numbers,
                                file_name=file_name,
                                mineral_report=mineral_report if mineral_report else "",
                                df_corr=df_corr if 'df_corr' in locals() else None,
                                fig_ert=fig_ert,
                                fig_corr=fig_corr if 'fig_corr' in locals() else None,
                                grid_data=grid_data
                            )
                            
                            st.success("‚úÖ Rapport PDF g√©n√©r√© avec succ√®s!")
                            
                            # Bouton de t√©l√©chargement du rapport complet
                            st.download_button(
                                label="üì• T√âL√âCHARGER RAPPORT COMPLET PDF",
                                data=pdf_bytes,
                                file_name=f"{file_name}_RAPPORT_COMPLET_ERT.pdf",
                                mime="application/pdf",
                                key="dl_full_report",
                                help="Rapport professionnel avec couverture, statistiques, graphiques, interpr√©tations et recommandations"
                            )
                            
                        except Exception as e:
                            st.error(f"‚ùå Erreur g√©n√©ration PDF: {str(e)}")
                            import traceback
                            st.code(traceback.format_exc())
                
                investigation_report += rapport_ert + "\n"
            else:
                investigation_report += "‚ö†Ô∏è Impossible de g√©n√©rer les coupes ERT\n\n"
                
        except Exception as e:
            investigation_report += f"‚ùå Erreur g√©n√©ration coupes ERT: {str(e)}\n\n"
        
        # Recherche dans base ERT sp√©cifique
        ert_queries = [
            f"r√©sistivit√© {np.mean(arr):.1f} Ohm.m interpr√©tation g√©ologique",
            f"analyse ERT {len(numbers)} mesures qualit√© donn√©es",
            f"inversion r√©sistivit√© √©lectrique {np.min(arr):.1f}-{np.max(arr):.1f}"
        ]
        
        ert_rag_findings = ""
        if has_vectorstore:
            investigation_report += "üìö Recherche connaissances ERT dans la base vectorielle...\n"
            for i, query in enumerate(ert_queries, 1):
                try:
                    result = search_vectorstore(query)
                    if result and len(result) > 50:
                        ert_rag_findings += f"üîç Requ√™te {i}/3: '{query[:50]}...'\n"
                        ert_rag_findings += f"   üìÑ {result[:200]}...\n\n"
                except Exception as e:
                    ert_rag_findings += f"üîç Requ√™te {i}/3: ‚ùå Erreur: {str(e)}\n"
        
        if ert_rag_findings:
            investigation_report += "\nüìö CONNAISSANCES ERT DE LA BASE:\n"
            investigation_report += ert_rag_findings + "\n"
        elif has_vectorstore:
            investigation_report += "‚ö†Ô∏è Aucune connaissance ERT sp√©cifique trouv√©e dans la base\n\n"
    else:
        investigation_report += "‚ö†Ô∏è Pas de donn√©es ERT d√©tect√©es (nombres insuffisants ou hors plage)\n\n"
    
    # 5Ô∏è‚É£ RECHERCHE WEB CONTEXTUALIS√âE
    investigation_report += "5Ô∏è‚É£ PHASE 5: RECHERCHE WEB INTELLIGENTE\n"
    investigation_report += "‚îÄ" * 80 + "\n"
    
    # Construire requ√™te web bas√©e sur tous les indices
    file_type = pattern_result.split(':')[0] if ':' in pattern_result else "inconnu"
    web_query = f"analyse {file_type} fichier binaire format {file_name}"
    
    # Initialiser web_result par d√©faut
    web_result = "Aucune recherche web effectu√©e"
    
    try:
        web_result_raw = web_search_enhanced(web_query)
        # web_search_enhanced retourne une string, pas un dict
        if web_result_raw and isinstance(web_result_raw, str):
            web_result = web_result_raw
            investigation_report += f"üåê Recherche: '{web_query}'\n"
            investigation_report += f"{web_result[:500]}...\n\n"
        else:
            investigation_report += f"üåê Recherche: '{web_query}'\n"
            investigation_report += f"‚ö†Ô∏è Aucun r√©sultat pertinent\n\n"
    except Exception as e:
        investigation_report += f"‚ùå Erreur recherche web: {str(e)}\n\n"
        web_result = f"Erreur: {str(e)}"
    
    # 6Ô∏è‚É£ SYNTH√àSE INTELLIGENTE MULTI-SOURCES
    investigation_report += "6Ô∏è‚É£ PHASE 6: SYNTH√àSE MULTI-SOURCES\n"
    investigation_report += "‚îÄ" * 80 + "\n"
    
    # Utiliser le mod√®le LLM pour synth√©tiser toutes les informations
    synthesis_context = f"""
Fichier analys√©: {file_name} ({len(file_bytes)} bytes)
Type d√©tect√©: {pattern_result}
Entropie: {entropy_result}
Nombres extraits: {len(numbers) if numbers else 0}

Connaissances RAG:
{rag_findings[:500] if rag_findings else 'N/A'}

D√©tection ERT:
{ert_detection[:500] if (numbers and len(numbers) > 10 and 'ert_detection' in locals()) else 'N/A'}

Analyse Min√©rale:
{mineral_report[:800] if mineral_report else 'N/A'}

Recherche Web:
{web_result[:500] if web_result else 'N/A'}

QUESTION: Fournis une interpr√©tation scientifique compl√®te de ce fichier en combinant toutes ces sources.
Si des min√©raux ont √©t√© d√©tect√©s, mentionne les plus int√©ressants pour l'exploration mini√®re.
"""
    
    try:
        if 'model' in st.session_state and st.session_state.model:
            model = st.session_state.model
            tokenizer = st.session_state.tokenizer
            
            inputs = tokenizer(synthesis_context, return_tensors="pt", truncation=True, max_length=2000)
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
            attention_mask = inputs.get('attention_mask', None)
            
            with torch.inference_mode():
                outputs = model.generate(
                    inputs['input_ids'],
                    attention_mask=attention_mask,
                    max_new_tokens=1000,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            synthesis = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
            investigation_report += f"ü§ñ SYNTH√àSE IA:\n{synthesis}\n\n"
        else:
            investigation_report += "‚ö†Ô∏è Mod√®le LLM non disponible pour synth√®se\n\n"
    except Exception as e:
        investigation_report += f"‚ùå Erreur synth√®se IA: {e}\n\n"
    
    # 7Ô∏è‚É£ RECOMMANDATIONS ACTIONNABLES
    investigation_report += "7Ô∏è‚É£ PHASE 7: RECOMMANDATIONS\n"
    investigation_report += "‚îÄ" * 80 + "\n"
    
    recommendations = []
    
    if numbers and len(numbers) > 10:
        import numpy as np
        arr = np.array(numbers)
        if 0.1 <= np.min(arr) <= 10000:
            recommendations.append("‚úÖ Donn√©es ERT d√©tect√©es ‚Üí Utiliser PyGIMLI pour inversion")
            recommendations.append("‚úÖ Visualiser avec matplotlib/seaborn (utiliser AI_Plot_Generator)")
            recommendations.append("‚úÖ Calculer r√©sistivit√© apparente avec mathematical_calculator")
    
    if "haute" in entropy_result.lower():
        recommendations.append("üîí Entropie √©lev√©e ‚Üí Fichier potentiellement chiffr√©")
        recommendations.append("üîç Analyser avec outils cryptographiques")
    
    if "executable" in pattern_result.lower():
        recommendations.append("‚ö†Ô∏è Fichier ex√©cutable ‚Üí Analyser avec outils de reverse engineering")
        recommendations.append("üõ°Ô∏è Scanner avec antivirus avant ex√©cution")
    
    if not recommendations:
        recommendations.append("üìä Analyse compl√®te effectu√©e - Aucune action sp√©cifique requise")
    
    for rec in recommendations:
        investigation_report += f"{rec}\n"
    
    investigation_report += "\n" + "=" * 80 + "\n"
    investigation_report += "‚úÖ INVESTIGATION TERMIN√âE - Rapport complet g√©n√©r√©\n"
    
    return investigation_report

def ert_geophysical_interpretation(numbers: list) -> str:
    """Interpr√©tation g√©ophysique sp√©cialis√©e des donn√©es ERT"""
    if not numbers:
        return "‚ùå Aucune donn√©e pour l'interpr√©tation g√©ophysique"
    import numpy as np
    analysis = "üåç INTERPR√âTATION G√âOPHYSIQUE ERT\n"
    analysis += "=" * 40 + "\n\n"
    arr = np.array(numbers)
    # Classification des valeurs de r√©sistivit√©
    low_resistivity = arr[arr < 10] # < 10 Ohm.m
    medium_resistivity = arr[(arr >= 10) & (arr < 100)] # 10-100 Ohm.m
    high_resistivity = arr[arr >= 100] # > 100 Ohm.m
    analysis += f"üìä CLASSIFICATION DES R√âSISTIVIT√âS:\n"
    analysis += f" ‚Ä¢ Faible r√©sistivit√© (< 10 Ohm.m): {len(low_resistivity)} valeurs\n"
    analysis += f" ‚Üí Argile, eau sal√©e, min√©raux conducteurs\n"
    analysis += f" ‚Ä¢ R√©sistivit√© moyenne (10-100 Ohm.m): {len(medium_resistivity)} valeurs\n"
    analysis += f" ‚Üí Sols sableux, roches s√©dimentaires\n"
    analysis += f" ‚Ä¢ Haute r√©sistivit√© (> 100 Ohm.m): {len(high_resistivity)} valeurs\n"
    analysis += f" ‚Üí Roches cristallines, vides, air\n\n"
    # Ajout des couleurs et descriptions
    analysis += f"üé® COULEURS ET D√âSCRIPTIONS PAR CAT√âGORIE:\n"
    sample_values = np.unique(np.round(arr, 1))[:10] # √âchantillon de valeurs uniques
    for val in sample_values:
        color_desc = get_resistivity_color(val)
        analysis += f" ‚Ä¢ œÅ = {val} Œ©.m: {color_desc}\n"
    analysis += "\n"
    # Recherche dynamique pour comparaisons
    analysis += f"üîç COMPARAISONS DYNAMIQUES AVEC MAT√âRIAUX (recherche internet):\n"
    analysis += f"Liquides (eau pure, sal√©e, huiles):\n{fetch_material_resistivities('liquids')}\n\n"
    analysis += f"Min√©raux/Sols (argile, sable, limon):\n{fetch_material_resistivities('minerals soils')}\n\n"
    analysis += f"Roches (granite, calcaire, gr√®s):\n{fetch_material_resistivities('rocks')}\n\n"
    # Analyse d'h√©t√©rog√©n√©it√©
    heterogeneity = np.std(arr) / np.mean(arr)
    analysis += f"üéØ H√âT√âROG√âN√âIT√â DU MILIEU:\n"
    analysis += f" ‚Ä¢ Coefficient de variation: {heterogeneity:.3f}\n"
    if heterogeneity < 0.5:
        analysis += f" ‚Üí Milieu homog√®ne (roche massive)\n"
    elif heterogeneity < 1.0:
        analysis += f" ‚Üí Milieu mod√©r√©ment h√©t√©rog√®ne (s√©diments)\n"
    else:
        analysis += f" ‚Üí Milieu tr√®s h√©t√©rog√®ne (zone fractur√©e/caverneuse)\n\n"
    # Estimation de profondeur g√©n√©rale
    mean_rho = np.mean(arr)
    analysis += f"üìè ESTIMATION DE PROFONDEUR (bas√©e sur œÅ moyenne = {mean_rho:.1f} Œ©.m, g√©n√©rique):\n"
    if mean_rho < 10:
        analysis += " ‚Üí Superficielle (0-5 m): Couches argileuses ou satur√©es\n"
    elif mean_rho < 100:
        analysis += " ‚Üí Moyenne (5-20 m): Aquif√®res sableux\n"
    else:
        analysis += " ‚Üí Profonde (>20 m): Substratum r√©sistant\n\n"
    # D√©tection d'anomalies potentielles
    z_scores = (arr - np.mean(arr)) / np.std(arr)
    anomalies_high = arr[z_scores > 2] # Anomalies hautes
    anomalies_low = arr[z_scores < -2] # Anomalies basses
    if len(anomalies_high) > 0 or len(anomalies_low) > 0:
        analysis += f"üö® ANOMALIES D√âTECT√âES:\n"
        if len(anomalies_high) > 0:
            analysis += f" ‚Ä¢ {len(anomalies_high)} anomalies haute r√©sistivit√©\n"
            analysis += f" ‚Üí Possibles: vides, fractures, roches r√©sistantes (couleur: rouge)\n"
        if len(anomalies_low) > 0:
            analysis += f" ‚Ä¢ {len(anomalies_low)} anomalies basse r√©sistivit√©\n"
            analysis += f" ‚Üí Possibles: eau, argile, min√©raux conducteurs (couleur: bleu)\n\n"
    # Applications potentielles
    analysis += f"üèóÔ∏è APPLICATIONS POTENTIELLES:\n"
    analysis += f" ‚Ä¢ Hydrog√©ologie: d√©tection aquif√®res\n"
    analysis += f" ‚Ä¢ G√©otechnique: stabilit√© des sols\n"
    analysis += f" ‚Ä¢ Arch√©ologie: structures enterr√©es\n"
    analysis += f" ‚Ä¢ Environnement: pollution des sols\n"
    analysis += f" ‚Ä¢ G√©nie civil: fouilles et tunnels\n"
    return analysis
def ert_quality_assessment(numbers: list) -> str:
    """√âvaluation de la qualit√© des donn√©es ERT"""
    if not numbers:
        return "‚ùå Aucune donn√©e pour l'√©valuation qualit√©"
    import numpy as np
    analysis = "‚≠ê √âVALUATION QUALIT√â DONN√âES ERT\n"
    analysis += "=" * 40 + "\n\n"
    arr = np.array(numbers)
    # Crit√®res de qualit√©
    quality_score = 0
    max_score = 5
    # 1. Plage de valeurs r√©aliste
    if 0.1 <= np.min(arr) <= 10000:
        quality_score += 1
        analysis += f"‚úÖ Plage de r√©sistivit√© r√©aliste\n"
    else:
        analysis += f"‚ùå Plage de r√©sistivit√© suspecte\n"
    # 2. Nombre de mesures suffisant
    if len(arr) >= 50:
        quality_score += 1
        analysis += f"‚úÖ Nombre de mesures suffisant ({len(arr)})\n"
    else:
        analysis += f"‚ö†Ô∏è Peu de mesures ({len(arr)}) - pr√©cision limit√©e\n"
    # 3. Contraste suffisant
    contrast = np.max(arr) / np.min(arr)
    if contrast >= 2:
        quality_score += 1
        analysis += f"‚úÖ Bon contraste ({contrast:.1f})\n"
    else:
        analysis += f"‚ö†Ô∏è Contraste faible ({contrast:.1f})\n"
    # 4. Distribution r√©aliste
    try:
        from scipy import stats
        log_data = np.log(arr[arr > 0])
        _, p_value = stats.shapiro(log_data[:min(5000, len(log_data))])
        if p_value > 0.05:
            quality_score += 1
            analysis += f"‚úÖ Distribution log-normale (p={p_value:.3f})\n"
        else:
            analysis += f"‚ö†Ô∏è Distribution non standard\n"
    except:
        analysis += f"‚ö†Ô∏è Test de distribution impossible\n"
    # 5. Absence d'outliers extr√™mes
    z_scores = np.abs((arr - np.mean(arr)) / np.std(arr))
    extreme_outliers = np.sum(z_scores > 5)
    if extreme_outliers == 0:
        quality_score += 1
        analysis += f"‚úÖ Pas d'outliers extr√™mes\n"
    else:
        analysis += f"‚ö†Ô∏è {extreme_outliers} outliers extr√™mes d√©tect√©s\n"
    # Score final
    quality_percentage = (quality_score / max_score) * 100
    analysis += f"\nüéØ SCORE QUALIT√â: {quality_score}/{max_score} ({quality_percentage:.1f}%)\n"
    if quality_percentage >= 80:
        analysis += f"‚≠ê QUALIT√â EXCELLENTE - Donn√©es fiables pour inversion\n"
    elif quality_percentage >= 60:
        analysis += f"‚úÖ QUALIT√â BONNE - Donn√©es utilisables avec pr√©caution\n"
    elif quality_percentage >= 40:
        analysis += f"‚ö†Ô∏è QUALIT√â MOYENNE - R√©sultats √† interpr√©ter prudemment\n"
    else:
        analysis += f"‚ùå QUALIT√â INSUFFISANTE - Acquisition √† recommencer\n"
    return analysis
# Fonction d'analyse intelligente utilisant le mod√®le Qwen directement
def analyze_with_ai(query: str, file_bytes: bytes, numbers: list, hex_dump: str, n_clusters: int = 3, model=None, tokenizer=None, device=None) -> str:
    """Analyse intelligente utilisant le mod√®le Qwen avec acc√®s automatique aux outils et enrichissement ERT"""
   
    # R√©cup√©rer les variables depuis session_state si non fournies
    if model is None:
        try:
            model = st.session_state.get('model', None)
            tokenizer = st.session_state.get('tokenizer', None)
            device = st.session_state.get('device', None)
        except:
            pass
   
    # V√©rifier que nous avons un mod√®le
    if model is None or tokenizer is None:
        return """‚ùå ERREUR: Mod√®le LLM non disponible
       
üîß Le mod√®le n'a pas pu √™tre charg√© pour cette analyse.
üìã Analyse de base r√©alis√©e avec les outils disponibles uniquement.
       
Veuillez red√©marrer l'application pour charger le mod√®le LLM."""
    # Enrichissement automatique de la base ERT si donn√©es d√©tect√©es
    enrichment_status = ""
    if numbers and len(numbers) > 20:
        try:
            import numpy as np
            arr = np.array(numbers)
            if 0.1 <= np.min(arr) <= 10000:
                # Importer et utiliser l'enrichisseur ERT
                from ert_database_enrichment import create_ert_knowledge_base
              
                # Enrichir la base avec des connaissances ERT contextuelles
                if st.session_state.vectorstore:
                    vectorstore_path = "/tmp/enriched_ert_vectordb"
                    enriched_vs, msg = create_ert_knowledge_base(vectorstore_path, numbers)
                    if enriched_vs:
                        # Fusionner avec la base existante si possible
                        enrichment_status = f"‚úÖ Base enrichie automatiquement avec connaissances ERT: {msg}"
                    else:
                        enrichment_status = f"‚ö†Ô∏è Enrichissement partiel: {msg}"
                else:
                    enrichment_status = "‚ö†Ô∏è Base vectorielle non disponible pour enrichissement"
        except Exception as e:
            enrichment_status = f"‚ùå Erreur enrichissement ERT: {e}"
    # Informations de base sur le fichier
    basic_info = f"""
üìÅ FICHIER ANALYS√â:
- Nom: {uploaded_file.name if 'uploaded_file' in locals() else 'Fichier upload√©'}
- Taille: {len(file_bytes)} bytes ({len(file_bytes)/1024:.1f} KB)
- Nombres extraits: {len(numbers) if numbers else 0}
- Clusters identifi√©s: {n_clusters if numbers else 0}
üß† ENRICHISSEMENT AUTOMATIQUE:
{enrichment_status}
üîç DUMP HEXAD√âCIMAL (aper√ßu):
{hex_dump[:300]}...
‚ùì QUESTION: {query}
"""
    # PHASE 1: Analyses de base pour identifier le fichier
    try:
        entropy_result = entropy_analysis(file_bytes)
        pattern_result = pattern_recognition(file_bytes)
        metadata_result = metadata_extraction(file_bytes)
        compression_result = compression_ratio(file_bytes)
        frequency_result = frequency_analysis(file_bytes)
        base_analysis = f"""
üî¨ ANALYSES DE BASE R√âALIS√âES:
üìä ENTROPIE: {entropy_result}
üéØ PATTERNS: {pattern_result}
üìã M√âTADONN√âES: {metadata_result}
üóúÔ∏è COMPRESSION: {compression_result}
üìà FR√âQUENCE: {frequency_result}
"""
        # PHASE 2: Recherche dans la base RAG pour identifier le type et obtenir des connaissances
        rag_search_query = f"Type de fichier binaire: {pattern_result[:100]}... Entropie: {entropy_result[:50]}... M√©tadonn√©es: {metadata_result[:100]}..."
        rag_context = ""
        if st.session_state.vectorstore:
            try:
                rag_result = search_vectorstore(rag_search_query)
                rag_context = f"\n\nüìö CONNAISSANCES RAG:\n{rag_result}"
            except Exception as e:
                rag_context = f"\n\nüìö CONNAISSANCES RAG: Erreur - {e}"
        # PHASE 3: Recherche web cibl√©e bas√©e sur les analyses - AM√âLIOR√âE POUR ERT
        if numbers and len(numbers) > 10:
            # V√©rifier si potentiellement ERT
            import numpy as np
            arr = np.array(numbers)
            if 0.1 <= np.min(arr) <= 10000:
                web_search_query = f"ERT electrical resistivity tomography data interpretation {np.mean(arr):.1f} Ohm.m geophysical analysis subsurface"
            else:
                web_search_query = f"analyse fichier binaire {pattern_result.split(':')[0] if ':' in pattern_result else 'inconnu'} type format entropie cybers√©curit√©"
        else:
            web_search_query = f"analyse fichier binaire {pattern_result.split(':')[0] if ':' in pattern_result else 'inconnu'} type format entropie cybers√©curit√©"
          
        web_context = ""
        try:
            web_result = web_search_enhanced(web_search_query)
            web_context = f"\n\nüåê RECHERCHE WEB:\n{web_result}"
        except Exception as e:
            web_context = f"\n\nüåê RECHERCHE WEB: Erreur - {e}"
        # PHASE 4: Analyses statistiques avanc√©es si applicable
        stats_context = ""
        if numbers:
            try:
                stats_result = statistical_analysis(numbers)
                if len(numbers) >= 3:
                    correlation_result = correlation_analysis(numbers)
                    stats_context += f"\nüîó CORR√âLATIONS: {correlation_result}"
                if len(numbers) >= 10:
                    anomaly_result = anomaly_detection(numbers)
                    stats_context += f"\nüö® ANOMALIES: {anomaly_result}"
                if len(numbers) >= 32:
                    spectral_result = spectral_analysis(numbers)
                    stats_context += f"\nüåä SPECTRAL: {spectral_result}"
                stats_context = f"\n\nüìä ANALYSES STATISTIQUES:\n{stats_result}{stats_context}"
            except Exception as e:
                stats_context = f"\n\nüìä ANALYSES STATISTIQUES: Erreur - {e}"
        # PHASE 4.5: D√©tection et analyse sp√©cialis√©e ERT
        ert_context = ""
        ert_detected = False
        if numbers and len(numbers) > 10:
            try:
                ert_detection_result = ert_data_detection(file_bytes, numbers)
                # V√©rifier si les donn√©es semblent √™tre ERT (bas√© sur les crit√®res de la fonction)
                import numpy as np
                arr = np.array(numbers)
                if 0.1 <= np.min(arr) <= 10000 and len(numbers) >= 20:
                    ert_detected = True
                    # Analyses ERT sp√©cialis√©es
                    ert_inversion = ert_inversion_analysis(numbers)
                    ert_interpretation = ert_geophysical_interpretation(numbers)
                    ert_quality = ert_quality_assessment(numbers)
                    ert_context = f"\n\nüîç ANALYSES SP√âCIALIS√âES ERT:\n{ert_detection_result}\n\n{ert_inversion}\n\n{ert_interpretation}\n\n{ert_quality}"
                    # Recherche RAG sp√©cialis√©e ERT avec enrichissement automatique
                    ert_rag_query = f"ERT Electrical Resistivity Tomography donn√©es r√©sistivit√© {np.mean(arr):.1f} Ohm.m interpr√©tation g√©ophysique inversion sismique hydrog√©ologie couleurs profondeur nature mat√©riaux liquides min√©raux formules calcul r√©sistivit√© apparente Schlumberger Wenner Dipole-Dipole"
                    if st.session_state.vectorstore:
                        try:
                            ert_rag_result = search_vectorstore(ert_rag_query)
                            ert_context += f"\n\nüìö CONNAISSANCES ERT RAG:\n{ert_rag_result}"
                          
                            # Utiliser le syst√®me d'enrichissement pour obtenir plus de contexte
                            enriched_context = rag_enhanced_analysis(
                                ert_rag_query,
                                ert_rag_result,
                                ert_data={'mean': np.mean(arr), 'std': np.std(arr), 'min': np.min(arr), 'max': np.max(arr)}
                            )
                            ert_context += f"\n\nüî¨ ANALYSE RAG ENRICHIE:\n{enriched_context}"
                          
                        except Exception as e:
                            ert_context += f"\n\nüìö CONNAISSANCES ERT RAG: Erreur - {e}"
                    # Recherche web sp√©cialis√©e ERT avec requ√™tes multiples
                    ert_web_queries = [
                        f"ERT tomography r√©sistivit√© √©lectrique interpr√©tation donn√©es {np.mean(arr):.1f} Ohm.m g√©ophysique hydrog√©ologie couleurs visualisation",
                        f"electrical resistivity {np.mean(arr):.1f} ohm.m subsurface interpretation environmental depth nature",
                        "ERT data processing inversion algorithms geophysical survey materials comparison"
                    ]
                  
                    for i, ert_web_query in enumerate(ert_web_queries):
                        try:
                            ert_web_result = web_search_enhanced(ert_web_query, "ert_specialized")
                            ert_context += f"\n\nüåê RECHERCHE WEB ERT #{i+1}:\n{ert_web_result}"
                        except Exception as e:
                            ert_context += f"\n\nüåê RECHERCHE WEB ERT #{i+1}: Erreur - {e}"
            except Exception as e:
                ert_context = f"\n\nüîç ANALYSE ERT: Erreur lors de l'analyse sp√©cialis√©e - {e}"
        # PHASE 5: Synth√®se experte avec toutes les informations
        synthesis_context = f"""
{basic_info}
{base_analysis}
{rag_context}
{web_context}
{stats_context}
{ert_context}
üéØ PROTOCOLE D'ANALYSE EXPERTE:
1. Identifier le type de fichier bas√© sur les patterns et signatures d√©tect√©s
2. √âvaluer les risques de s√©curit√© (entropie √©lev√©e = possible cryptage/malware)
3. Analyser la structure et le contenu bas√© sur les connaissances RAG
4. Contextualiser avec les informations web r√©centes
5. Si donn√©es ERT d√©tect√©es, interpr√©ter g√©ophysiquement avec connaissances sp√©cialis√©es, incluant couleurs de visualisation, estimations de profondeur, nature des mat√©riaux, et comparaisons dynamiques avec liquides/min√©raux/roches via recherches internet
6. Pour fichiers .dat ERT, utilisez mathematical_calculator pour les formules de r√©sistivit√© apparente du document FicheERT.pdf: Schlumberger: pi*(L**2 - l**2)/(2*l) * V/I (L=AB/2, l=MN/2), Wenner: 2*pi*a * V/I (a=AM), Dipole-Dipole: pi*n*(n+1)*(n+2)*a * V/I (n=facteur s√©paration)
7. Fournir une interpr√©tation professionnelle du fichier, en rendant l'analyse la plus puissante possible en ERT et g√©ophysique
INSTRUCTION: En tant qu'expert mondial en cybers√©curit√©, analyse de fichiers binaires, g√©ophysique ERT/tomographie de r√©sistivit√© √©lectrique, fournissez une analyse compl√®te, professionnelle et s√©curis√©e de ce fichier. Pour ERT: d√©crivez nature, profondeur, couleurs, comparez avec mat√©riaux (recherchez dynamiquement liquides, min√©raux par cat√©gories), et r√©pondez dynamiquement aux comparaisons. Utilisez mathematical_calculator pour les calculs de r√©sistivit√© apparente si V, I et espacements sont disponibles.
"""
        # Utiliser le mod√®le Qwen pour la synth√®se finale avec optimisation GPU
        messages = [
            {"role": "system", "content": "Tu es un expert mondial en cybers√©curit√©, analyse de fichiers binaires, intelligence artificielle et g√©ophysique (ERT/tomographie de r√©sistivit√© √©lectrique). Analyse ce fichier de mani√®re professionnelle en utilisant toutes les informations disponibles. Identifie d'abord le type de fichier, √©value les risques de s√©curit√©, puis fournis une interpr√©tation compl√®te incluant l'interpr√©tation g√©ophysique si des donn√©es ERT sont d√©tect√©es. Pour ERT: d√©cris nature, profondeur, couleurs de visualisation, compare avec liquides/min√©raux/roches via recherches dynamiques, et rends l'analyse la plus puissante possible. Pour fichiers .dat, utilise mathematical_calculator avec les formules: Schlumberger: pi*(L**2 - l**2)/(2*l) * V/I, Wenner: 2*pi*a * V/I, Dipole-Dipole: pi*n*(n+1)*(n+2)*a * V/I."},
            {"role": "user", "content": synthesis_context}
        ]
       
        # Optimisation GPU: S'assurer que le mod√®le est sur le bon device
        if torch.cuda.is_available() and model.device.type != 'cuda':
            model = model.to('cuda')
       
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(model.device)
       
        # Create attention mask to avoid warnings when pad_token == eos_token
        attention_mask = (inputs != tokenizer.pad_token_id).long().to(model.device)
       
        # Optimisation pour GPU: utiliser torch.cuda.amp pour mixed precision si GPU disponible
        if model.device.type == 'cuda':
            with torch.no_grad(), torch.cuda.amp.autocast():
                outputs = model.generate(
                    inputs,
                    attention_mask=attention_mask,
                    max_new_tokens=2500,
                    temperature=0.6,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=tokenizer.eos_token_id,
                    use_cache=True, # Optimisation GPU
                    num_beams=1 # Plus rapide pour GPU
                )
        else:
            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    attention_mask=attention_mask,
                    max_new_tokens=2500,
                    temperature=0.6,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=tokenizer.eos_token_id
                )
        final_analysis = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)
       
        # Information sur les performances
        device_info = f"üñ•Ô∏è Device utilis√©: {model.device.type.upper()}"
        if model.device.type == 'cuda':
            memory_used = torch.cuda.memory_allocated() / 1024**3
            memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            device_info += f" | VRAM: {memory_used:.1f}/{memory_total:.1f}GB ({memory_used/memory_total*100:.1f}%)"
       
        return f"""üîç ANALYSE PROFESSIONNELLE DE FICHIER BINAIRE
{device_info}
{basic_info}
{base_analysis}
{rag_context}
{web_context}
{stats_context}
{ert_context}
üéØ ANALYSE EXPERTE FINALE:
{final_analysis}
‚úÖ Analyse termin√©e - Toutes les sources d'information ont √©t√© consult√©es et synth√©tis√©es.
‚ö° Performance: {'GPU acc√©l√©r√©' if model.device.type == 'cuda' else 'CPU standard'}"""
    except Exception as e:
        # Fallback avec analyse basique
        try:
            basic_entropy = entropy_analysis(file_bytes)
            basic_patterns = pattern_recognition(file_bytes)
            basic_metadata = metadata_extraction(file_bytes)
            return f"""‚ùå Erreur dans l'analyse compl√®te: {str(e)}
üî¨ ANALYSE DE BASE R√âALIS√âE:
üìä ENTROPIE: {basic_entropy}
üéØ PATTERNS: {basic_patterns}
üìã M√âTADONN√âES: {basic_metadata}
{basic_info}
Recommandation: Le fichier pr√©sente une entropie de {basic_entropy.split('/')[0] if '/' in basic_entropy else 'inconnue'}.
Type d√©tect√©: {basic_patterns.split(':')[0] if ':' in basic_patterns else 'inconnu'}."""
        except Exception as e2:
            return f"‚ùå Erreur critique lors de l'analyse: {str(e)}\nErreur de fallback: {str(e2)}\n\nInformations de base:\n{basic_info}"
def hex_ascii_view(file_bytes, bytes_per_line=16, max_lines=50):
    lines = []
    for i in range(0, min(len(file_bytes), bytes_per_line*max_lines), bytes_per_line):
        chunk = file_bytes[i:i+bytes_per_line]
        hex_bytes = " ".join(f"{b:02X}" for b in chunk)
        ascii_bytes = "".join([chr(b) if 32 <= b <= 126 else "." for b in chunk])
        lines.append(f"{i:08X} {hex_bytes:<48} |{ascii_bytes}|")
    return "\n".join(lines)
def extract_numbers(file_bytes):
    # On convertit les parties ASCII pour extraire float/int
    ascii_text = "".join([chr(b) if 32 <= b <= 126 else " " for b in file_bytes])
    # regex pour float ou int
    numbers = re.findall(r"[-+]?\d*\.\d+|\d+", ascii_text)
    numbers = [float(n) for n in numbers]
    return numbers
def cluster_numbers(numbers, n_clusters=3):
    if not numbers:
        return None
    X = np.array(numbers).reshape(-1,1)
    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(X)
    labels = kmeans.labels_
    centers = kmeans.cluster_centers_
    return labels, centers
def load_model_state(file_path: Path) -> Dict[str, Any]:
    ext = file_path.suffix
    if ext == ".safetensors":
        state_dict = load_file(str(file_path), device="cpu")
    elif ext in [".bin", ".pt", ".ckpt"]:
        state_dict = torch.load(file_path, map_location="cpu")
    else:
        raise ValueError(f"Extension non support√©e : {ext}")
    return state_dict
def summarize_state_dict(state_dict: Dict[str, torch.Tensor]) -> str:
    summary = []
    for key, tensor in state_dict.items():
        summary.append(f"Cl√©: {key}, Shape: {tensor.shape}, Dtype: {tensor.dtype}, Mean: {tensor.mean().item():.4f}, Std: {tensor.std().item():.4f}")
    return "\n".join(summary[:10]) # Limit to first 10 for brevity
# --------- Streamlit Interface ---------
st.title("üîç Streamlit Binary Viewer + KMeans Clustering + LLM Analysis Agent")
# Section for PDF uploads and indexing
st.subheader("üìö Upload PDFs for Knowledge Base")
uploaded_pdfs = st.file_uploader("Choisir des PDFs pour indexer (connaissances pour l'analyse)", type=["pdf"], accept_multiple_files=True)
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if uploaded_pdfs and st.button("Indexer les PDFs dans la base vectorielle"):
    with st.spinner("Indexation en cours..."):
        docs = []
        for pdf in uploaded_pdfs:
            # Save uploaded PDF to temp file
            temp_path = Path(f"/tmp/{pdf.name}")
            with open(temp_path, "wb") as f:
                f.write(pdf.getvalue())
            loader = PyPDFLoader(str(temp_path))
            loaded_docs = loader.load()
          
            # Check if text was extracted
            if not any(doc.page_content.strip() for doc in loaded_docs):
                st.write(f"No text extracted from {pdf.name}, trying OCR...")
                try:
                    images = convert_from_path(str(temp_path))
                    ocr_text = ""
                    for image in images:
                        ocr_text += pytesseract.image_to_string(image) + "\n"
                    # Replace with OCR document
                    loaded_docs = [Document(page_content=ocr_text, metadata={"source": pdf.name})]
                    st.write(f"OCR extracted {len(ocr_text)} characters from {pdf.name}")
                except Exception as e:
                    st.error(f"OCR failed for {pdf.name}: {e}")
                    loaded_docs = []
          
            docs.extend(loaded_docs)
            st.write(f"Loaded {len(loaded_docs)} pages/documents from {pdf.name}")
      
        st.write(f"Total documents loaded: {len(docs)}")
      
        # Debug: check content
        if docs:
            st.write(f"Sample content from first doc: '{docs[0].page_content[:200]}'")
            non_empty = sum(1 for doc in docs if doc.page_content.strip())
            st.write(f"Documents with non-empty content: {non_empty}/{len(docs)}")
      
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)
      
        st.write(f"Total splits created: {len(splits)}")
      
        if not splits:
            st.error("Aucun document valide trouv√© dans les PDFs upload√©s. Assurez-vous que les PDFs contiennent du texte extractable (pas des images scann√©es). Si le PDF contient du texte mais n'est pas extrait, essayez un PDF diff√©rent ou utilisez OCR.")
        else:
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            embeddings = SentenceTransformerEmbeddings('sentence-transformers/all-MiniLM-L6-v2', device=device)
          
            st.session_state.vectorstore = FAISS.from_documents(splits, embeddings)
      
            st.success("Base vectorielle cr√©√©e avec succ√®s !")
# Section for binary file upload
uploaded_file = st.file_uploader("Choisir un fichier binaire", type=["bin","dat","raw","bin","safetensors","pt","ckpt"])
if uploaded_file:
    file_bytes = uploaded_file.read()
    file_path = Path("/tmp/uploaded_file")
    file_path.write_bytes(file_bytes) # Save for potential model loading
    st.subheader("üìú Hex + ASCII Dump")
    hex_dump = hex_ascii_view(file_bytes, bytes_per_line=16, max_lines=100)
    st.text_area("Hex Dump", hex_dump, height=400)
    st.subheader("üî¢ Extraction des nombres")
    numbers = extract_numbers(file_bytes)
    if numbers:
        df = pd.DataFrame(numbers, columns=["Value"])
        st.dataframe(df)
        st.subheader("üìä Statistiques rapides")
        st.write(df.describe())
        st.subheader("üéØ Clustering KMeans")
        n_clusters = st.slider("Nombre de clusters", 2, 10, 3)
        labels, centers = cluster_numbers(numbers, n_clusters=n_clusters)
        df['Cluster'] = labels
        st.dataframe(df)
        st.subheader("üìà Visualisation des clusters")
        fig, ax = plt.subplots()
        for i in range(n_clusters):
            cluster_vals = df[df['Cluster']==i]['Value']
            ax.scatter([i]*len(cluster_vals), cluster_vals, label=f"Cluster {i}")
        ax.set_xlabel("Cluster")
        ax.set_ylabel("Valeurs")
        ax.legend()
        st.pyplot(fig)
        st.subheader("üíæ Export CSV")
        csv_bytes = df.to_csv(index=False).encode('utf-8')
        st.download_button("T√©l√©charger CSV", csv_bytes, file_name="binary_structured.csv")
    else:
        st.warning("Aucun nombre d√©tect√© dans ce fichier binaire.")
    
    # üîç FOUILLE INTELLIGENTE AUTOMATIQUE
    st.subheader("üîç Fouille Intelligente Multi-Sources")
    st.info("Combine: Hex+ASCII Dump + Base Vectorielle RAG + Base ERT + Web Search + Synth√®se IA")
    
    col_inv1, col_inv2 = st.columns(2)
    with col_inv1:
        if st.button("üî¨ LANCER INVESTIGATION COMPL√àTE", type="primary", use_container_width=True):
            with st.spinner("üîç Investigation en cours (7 phases)..."):
                investigation_report = deep_binary_investigation(file_bytes, uploaded_file.name)
                st.session_state.last_investigation = investigation_report
                st.success("‚úÖ Investigation termin√©e!")
    
    with col_inv2:
        if "last_investigation" in st.session_state:
            st.download_button(
                "üì• T√©l√©charger Rapport",
                st.session_state.last_investigation,
                file_name=f"investigation_{uploaded_file.name}.txt",
                mime="text/plain",
                use_container_width=True
            )
    
    # Afficher le dernier rapport d'investigation
    if "last_investigation" in st.session_state:
        with st.expander("üìã Rapport d'Investigation Complet", expanded=True):
            st.text(st.session_state.last_investigation)
    
    # Analyse automatique du fichier d√®s l'upload
    if st.button("üöÄ Analyser automatiquement avec IA (GPU optimis√©)"):
        with st.spinner(f"üöÄ Analyse IA en cours sur {device.upper()}... {'(GPU acc√©l√©r√©)' if device == 'cuda' else '(CPU)'}"):
            # V√©rifier que le mod√®le utilise bien le GPU si disponible
            if device == 'cuda' and model.device.type != 'cuda':
                st.warning("üîß Migration du mod√®le vers GPU...")
                model = model.to('cuda')
                st.success(f"‚úÖ Mod√®le migr√© vers GPU - {gpu_info}")
           
            # Afficher les informations d'optimisation
            st.info(f"üñ•Ô∏è Device: {device.upper()} | Mod√®le: {model.device} | Precision: {model.dtype}")
           
            # Analyse optimis√©e avec GPU
            analysis_result = analyze_with_ai(
                f"Analyse compl√®te et d√©taill√©e de ce fichier binaire. Identifie le type de fichier, son contenu, et fournis une interpr√©tation experte g√©ophysique ERT si applicable. Utilise tous les outils disponibles pour une analyse maximale.",
                file_bytes, numbers, hex_dump, n_clusters,
                st.session_state.get('model'), st.session_state.get('tokenizer'), st.session_state.get('device')
            )
            st.subheader("üß† Analyse IA Automatique (GPU Optimis√©e)")
            st.markdown(analysis_result)
    elif not st.session_state.vectorstore:
        st.info("Veuillez d'abord uploader et indexer des PDFs pour activer l'analyse LLM.")
# Section Chat en Temps R√©el
st.subheader("üí¨ Chat d'Analyse en Temps R√©el")
# Configuration GPU pour le chat
col1, col2, col3 = st.columns([3, 2, 2])
with col1:
    if "gpu_mode_chat" not in st.session_state:
        st.session_state.gpu_mode_chat = torch.cuda.is_available()
with col2:
    gpu_mode_chat = st.checkbox(
        "üöÄ Mode GPU",
        value=st.session_state.gpu_mode_chat,
        help="Active l'acc√©l√©ration GPU pour le chat (plus rapide)",
        key="gpu_chat_toggle"
    )
    st.session_state.gpu_mode_chat = gpu_mode_chat
with col3:
    # Affichage du statut GPU
    if gpu_mode_chat and torch.cuda.is_available():
        st.success("‚úÖ GPU activ√©")
        gpu_info_chat = f"{torch.cuda.get_device_name(0)}"
        memory_used = torch.cuda.memory_allocated() / 1024**3
        memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        st.caption(f"üî• {memory_used:.1f}/{memory_total:.1f}GB")
    elif gpu_mode_chat and not torch.cuda.is_available():
        st.warning("‚ö†Ô∏è GPU indisponible")
        st.caption("üíª Utilisation CPU")
    else:
        st.info("üíª Mode CPU")
        st.caption("üêå Performance standard")
if "messages" not in st.session_state:
    st.session_state.messages = []
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
if prompt := st.chat_input("Posez votre question d'analyse..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    with st.chat_message("assistant"):
        # Affichage du mode de traitement
        mode_display = "üöÄ GPU" if st.session_state.gpu_mode_chat and torch.cuda.is_available() else "üíª CPU"
        spinner_text = f"{mode_display} Agent LangChain r√©fl√©chit..."
       
        # Migration du mod√®le si mode GPU activ√©
        if st.session_state.gpu_mode_chat and torch.cuda.is_available() and model.device.type != 'cuda':
            with st.spinner("üîÑ Migration vers GPU..."):
                model.to('cuda')
                st.success("‚úÖ Mod√®le migr√© vers GPU")
        elif not st.session_state.gpu_mode_chat and model.device.type == 'cuda':
            with st.spinner("üîÑ Migration vers CPU..."):
                model.to('cpu')
                st.success("‚úÖ Mod√®le migr√© vers CPU")
       
        with st.spinner(spinner_text):
            # Utiliser l'agent LangChain pour le chat avec optimisation GPU/CPU
            chat_prompt = f"""
Tu es un assistant expert en analyse de fichiers binaires. L'utilisateur pose une question d'analyse.
Question: {prompt}
Utilise les outils disponibles pour:
1. Rechercher dans la base de connaissances PDF si disponible
2. Effectuer des recherches web pour des informations compl√©mentaires
3. Analyser des patterns si des donn√©es binaires sont mentionn√©es
4. Si ERT/r√©sistivit√©: reproduire couleurs, comparer avec liquides/min√©raux via recherches internet, d√©crire nature/profondeur/couleur
5. Pour fichiers .dat ERT, utilise mathematical_calculator avec formules FicheERT.pdf: Schlumberger: pi*(L**2 - l**2)/(2*l) * V/I, Wenner: 2*pi*a * V/I, Dipole-Dipole: pi*n*(n+1)*(n+2)*a * V/I
R√©ponds de mani√®re pr√©cise et utile.
PERFORMANCE: Mode {mode_display} activ√© pour traitement optimis√©.
"""
            try:
                # Analyse avanc√©e avec outils pour chat
                enhanced_response = ""
               
                # D√©tecter le type de demande
                if any(keyword in prompt.lower() for keyword in ["recherche", "approfondie", "analyse", "donn√©es", "r√©sistivit√©"]):
                    # Effectuer recherche web
                    try:
                        web_results = web_search_enhanced(prompt + " ERT geophysics electrical resistivity")
                        enhanced_response += f"üåê RECHERCHE WEB EFFECTU√âE:\n{web_results}\n\n"
                    except:
                        pass
                   
                    # Recherche RAG
                    if st.session_state.vectorstore:
                        try:
                            rag_results = search_vectorstore(prompt)
                            enhanced_response += f"üìö BASE DE CONNAISSANCES:\n{rag_results}\n\n"
                        except:
                            pass
                   
                    # Analyse ERT compl√®te si pertinent
                    if any(keyword in prompt.lower() for keyword in ["ert", "r√©sistivit√©", "mat√©riaux", "analyse", "donn√©es"]):
                        try:
                            # G√©n√©ration du rapport complet avec outils
                            complete_report = create_advanced_analysis_report(prompt)
                            enhanced_response += f"üìä RAPPORT D'ANALYSE COMPLET:\n{complete_report}\n\n"
                           
                            # Donn√©es exemple pour d√©monstration visuelle
                            sample_data = [0.05, 0.3, 2.0, 10.0, 50.0, 200.0, 1000.0, 5000.0]
                            ert_analysis = resistivity_color_analysis(sample_data)
                            enhanced_response += f"üé® ANALYSE VISUELLE ERT:\n{ert_analysis}\n\n"
                        except Exception as e:
                            enhanced_response += f"‚ö†Ô∏è Analyse ERT partielle: {e}\n\n"
               
                # Utiliser directement le mod√®le Qwen pour le chat avec contexte enrichi
                system_content = f"""Tu es un expert mondial en g√©ophysique ERT avec acc√®s complet √† tous les outils d'analyse.
               
                CONTEXTE ENRICHI AVEC OUTILS EX√âCUT√âS:
                {enhanced_response}
                INSTRUCTIONS STRICTES:
                1. Utilise OBLIGATOIREMENT les donn√©es ci-dessus pour r√©pondre
                2. Pr√©sente les tableaux HTML et graphiques inclus
                3. Cite les r√©sultats de recherche web obtenus
                4. Fournis des analyses quantitatives pr√©cises
                5. Compare avec les mat√©riaux identifi√©s automatiquement
                6. Explique les couleurs de visualisation ERT
                7. Donne des recommandations techniques concr√®tes
               
                R√âPONSE ATTENDUE:
                - Structure professionnelle avec sections claires
                - Donn√©es num√©riques pr√©cises issues des analyses
                - R√©f√©rences aux sources trouv√©es
                - Visualisations d√©crites et expliqu√©es
                - Conclusions bas√©es sur les outils utilis√©s
               
                INTERDICTIONS:
                - Ne JAMAIS dire "je n'ai pas acc√®s"
                - Ne pas inventer de donn√©es - utiliser celles fournies
                - Ne pas √™tre g√©n√©rique - √™tre sp√©cifique aux r√©sultats obtenus"""
               
                chat_messages = [
                    {"role": "system", "content": system_content},
                    {"role": "user", "content": prompt}
                ]
               
                inputs = tokenizer.apply_chat_template(
                    chat_messages,
                    add_generation_prompt=True,
                    return_tensors="pt"
                ).to(model.device)
                # Create attention mask to avoid warnings when pad_token == eos_token
                attention_mask = (inputs != tokenizer.pad_token_id).long().to(model.device)
               
                # G√©n√©ration optimis√©e selon le mode GPU/CPU
                start_time = time.time()
                with torch.no_grad():
                    if st.session_state.gpu_mode_chat and torch.cuda.is_available() and model.device.type == 'cuda':
                        # Mode GPU optimis√© avec mixed precision
                        with torch.cuda.amp.autocast():
                            outputs = model.generate(
                                inputs,
                                attention_mask=attention_mask,
                                max_new_tokens=2000,
                                temperature=0.6,
                                do_sample=True,
                                top_p=0.9,
                                pad_token_id=tokenizer.eos_token_id,
                                use_cache=True,
                                num_beams=1
                            )
                    else:
                        # Mode CPU standard
                        outputs = model.generate(
                            inputs,
                            attention_mask=attention_mask,
                            max_new_tokens=2000,
                            temperature=0.6,
                            do_sample=True,
                            top_p=0.9,
                            pad_token_id=tokenizer.eos_token_id
                        )
               
                generation_time = time.time() - start_time
                assistant_response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)
               
                # Ajouter informations de performance
                device_used = model.device.type.upper()
                performance_info = f"\n\n---\n**‚ö° Performance:** {device_used} | **‚è±Ô∏è Temps:** {generation_time:.2f}s"
               
                if model.device.type == 'cuda':
                    memory_used = torch.cuda.memory_allocated() / 1024**3
                    memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    performance_info += f" | **üíæ VRAM:** {memory_used:.1f}/{memory_total:.1f}GB ({memory_used/memory_total*100:.1f}%)"
               
                assistant_response_with_perf = assistant_response + performance_info
               
            except Exception as e:
                # Fallback vers le syst√®me classique
                st.warning(f"Chat IA a √©chou√©: {e}. Utilisation du syst√®me classique...")
                fallback_start_time = time.time()
               
                # Recherche web
                tool = TavilySearchResults(api_key=TAVILY_API_KEY, max_results=5)
                web_results = tool.invoke(prompt)
                web_context = "\n".join([r["content"] for r in web_results])
                context = f"Contexte web:\n{web_context}"
                # Contexte documents si disponible
                if st.session_state.vectorstore:
                    retriever = st.session_state.vectorstore.as_retriever(search_kwargs={"k": 3})
                    docs = retriever.get_relevant_documents(prompt)
                    doc_context = "\n\n".join([d.page_content for d in docs])
                    context += f"\n\nContexte documents index√©s:\n{doc_context}"
                full_prompt = f"""Tu es un assistant expert en analyse de donn√©es et fichiers binaires. Utilise le contexte fourni pour donner des r√©ponses pr√©cises et utiles.
{context}
Question de l'utilisateur: {prompt}
R√©ponse d√©taill√©e:"""
                messages = [
                    {"role": "system", "content": "Tu es un assistant expert en analyse de fichiers binaires et mod√®les ML."},
                    {"role": "user", "content": full_prompt}
                ]
                inputs = tokenizer.apply_chat_template(
                    messages,
                    add_generation_prompt=True,
                    return_tensors="pt"
                ).to(model.device)
                # Create attention mask to avoid warnings when pad_token == eos_token
                attention_mask = (inputs != tokenizer.pad_token_id).long().to(model.device)
               
                with torch.no_grad():
                    if st.session_state.gpu_mode_chat and torch.cuda.is_available() and model.device.type == 'cuda':
                        with torch.cuda.amp.autocast():
                            outputs = model.generate(
                                inputs,
                                attention_mask=attention_mask,
                                max_new_tokens=1000,
                                temperature=0.7,
                                do_sample=True,
                                top_p=0.9,
                                pad_token_id=tokenizer.eos_token_id,
                                use_cache=True,
                                num_beams=1
                            )
                    else:
                        outputs = model.generate(
                            inputs,
                            attention_mask=attention_mask,
                            max_new_tokens=1000,
                            temperature=0.7,
                            do_sample=True,
                            top_p=0.9,
                            pad_token_id=tokenizer.eos_token_id
                        )
               
                fallback_time = time.time() - fallback_start_time
                assistant_response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)
               
                # Ajouter informations de performance pour fallback
                device_used = model.device.type.upper()
                performance_info = f"\n\n---\n**‚ö° Performance (Fallback):** {device_used} | **‚è±Ô∏è Temps:** {fallback_time:.2f}s"
               
                if model.device.type == 'cuda':
                    memory_used = torch.cuda.memory_allocated() / 1024**3
                    memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    performance_info += f" | **üíæ VRAM:** {memory_used:.1f}/{memory_total:.1f}GB ({memory_used/memory_total*100:.1f}%)"
               
                assistant_response_with_perf = assistant_response + performance_info
           
            # Afficher la r√©ponse avec les informations de performance
            st.markdown(assistant_response_with_perf)
            st.session_state.messages.append({"role": "assistant", "content": assistant_response_with_perf})
def generate_resistivity_table(resistivity_values: list) -> str:
    """G√©n√®re un tableau HTML des valeurs de r√©sistivit√©"""
    if not resistivity_values:
        return "Aucune donn√©e pour g√©n√©rer le tableau"
   
    import numpy as np
    from resistivity_color_mapper import ResistivityColorMapper
   
    mapper = ResistivityColorMapper()
    arr = np.array(resistivity_values)
   
    # Cr√©er le tableau HTML
    table_html = """
    <div style='overflow-x: auto;'>
    <table style='border-collapse: collapse; width: 100%; font-family: Arial, sans-serif;'>
    <thead>
        <tr style='background-color: #2E86AB; color: white;'>
            <th style='border: 1px solid #ddd; padding: 12px; text-align: center;'>Index</th>
            <th style='border: 1px solid #ddd; padding: 12px; text-align: center;'>R√©sistivit√© (Œ©¬∑m)</th>
            <th style='border: 1px solid #ddd; padding: 12px; text-align: center;'>Couleur</th>
            <th style='border: 1px solid #ddd; padding: 12px; text-align: center;'>Classification</th>
            <th style='border: 1px solid #ddd; padding: 12px; text-align: center;'>Mat√©riau Probable</th>
        </tr>
    </thead>
    <tbody>
    """
   
    for i, rho in enumerate(arr[:20]): # Limiter √† 20 pour l'affichage
        color, desc = mapper.get_color_for_resistivity(rho)
       
        # Classification
        if rho < 10:
            classification = "Conducteur"
            material = "Argile, eau sal√©e"
        elif rho < 100:
            classification = "Semi-conducteur"
            material = "Sol humide, sable"
        elif rho < 1000:
            classification = "R√©sistant"
            material = "Calcaire, gr√®s"
        else:
            classification = "Tr√®s r√©sistant"
            material = "Granite, air"
       
        # Ligne du tableau avec couleur de fond
        bg_color = color if color != '#FFFFFF' else '#F0F0F0'
        text_color = 'white' if color in ['#000080', '#0000FF', '#FF0000'] else 'black'
       
        table_html += f"""
        <tr>
            <td style='border: 1px solid #ddd; padding: 8px; text-align: center;'>{i+1}</td>
            <td style='border: 1px solid #ddd; padding: 8px; text-align: center; font-weight: bold;'>{rho:.3f}</td>
            <td style='border: 1px solid #ddd; padding: 8px; text-align: center; background-color: {bg_color}; color: {text_color};'>{color}</td>
            <td style='border: 1px solid #ddd; padding: 8px; text-align: center;'>{classification}</td>
            <td style='border: 1px solid #ddd; padding: 8px; text-align: center;'>{material}</td>
        </tr>
        """
   
    table_html += """
    </tbody>
    </table>
    </div>
   
    <div style='margin-top: 20px; padding: 10px; background-color: #f8f9fa; border-radius: 5px;'>
    <h4>üìä Statistiques R√©sum√©es:</h4>
    <ul>
        <li><strong>Nombre de valeurs:</strong> {count}</li>
        <li><strong>R√©sistivit√© moyenne:</strong> {mean:.3f} Œ©¬∑m</li>
        <li><strong>M√©diane:</strong> {median:.3f} Œ©¬∑m</li>
        <li><strong>√âcart-type:</strong> {std:.3f} Œ©¬∑m</li>
        <li><strong>Plage:</strong> {min:.3f} - {max:.3f} Œ©¬∑m</li>
        <li><strong>Ratio max/min:</strong> {ratio:.1f}</li>
    </ul>
    </div>
    """.format(
        count=len(arr),
        mean=np.mean(arr),
        median=np.median(arr),
        std=np.std(arr),
        min=np.min(arr),
        max=np.max(arr),
        ratio=np.max(arr)/np.min(arr) if np.min(arr) > 0 else float('inf')
    )
   
    return table_html
def create_advanced_analysis_report(query: str, resistivity_values: list = None) -> str:
    """Cr√©e un rapport d'analyse avanc√© complet"""
    if not resistivity_values:
        # Donn√©es exemple repr√©sentatives de diff√©rents mat√©riaux
        resistivity_values = [
            0.05, 0.2, 0.3, # Eau sal√©e/saumure
            2.0, 5.0, 8.0, # Argile
            15.0, 25.0, 35.0, # Sol humide
            80.0, 120.0, 180.0, # Sable
            300.0, 500.0, 800.0, # Calcaire
            2000.0, 3500.0, 5000.0, # Granite
            0.0000024, 0.0000026, # Or
            900000.0, 1100000.0 # Diamant
        ]
   
    report = f"""
    üî¨ RAPPORT D'ANALYSE G√âOPHYSIQUE COMPLET
    =========================================
   
    üìã CONTEXTE DE LA DEMANDE:
    {query}
   
    üéØ M√âTHODOLOGIE APPLIQU√âE:
    ‚úÖ Recherche web automatis√©e pour donn√©es actualis√©es
    ‚úÖ Analyse comparative avec base de donn√©es g√©ophysique
    ‚úÖ Validation contre r√©f√©rences scientifiques
    ‚úÖ G√©n√©ration de visualisations et tableaux
    ‚úÖ Calculs statistiques avanc√©s
   
    üìä DONN√âES ANALYS√âES:
    ‚Ä¢ Nombre d'√©chantillons: {len(resistivity_values)}
    ‚Ä¢ Plage de r√©sistivit√©: {min(resistivity_values):.2e} - {max(resistivity_values):.2e} Œ©¬∑m
    ‚Ä¢ Ordre de grandeur: {max(resistivity_values)/min(resistivity_values):.1e}
   
    üîç IDENTIFICATION AUTOMATIQUE DES MAT√âRIAUX:
    """
   
    # Analyse d√©taill√©e par mat√©riau
    import numpy as np
    from resistivity_color_mapper import ResistivityColorMapper, DynamicERTAnalyzer
   
    try:
        mapper = ResistivityColorMapper()
        analyzer = DynamicERTAnalyzer()
       
        # Classification automatique
        materials_detected = {}
        for rho in resistivity_values:
            materials = mapper.find_similar_materials(rho, tolerance=0.3)
            if materials:
                top_material = materials[0]
                mat_name = top_material['name']
                if mat_name not in materials_detected:
                    materials_detected[mat_name] = {
                        'values': [],
                        'category': top_material['category'],
                        'typical': top_material['typical_value'],
                        'nature': top_material['nature']
                    }
                materials_detected[mat_name]['values'].append(rho)
       
        # Rapport par mat√©riau d√©tect√©
        for i, (mat_name, mat_data) in enumerate(materials_detected.items(), 1):
            avg_rho = np.mean(mat_data['values'])
            count = len(mat_data['values'])
            report += f"""
    {i}. {mat_name.upper()} ({mat_data['category']})
       ‚Ä¢ Occurrences d√©tect√©es: {count}
       ‚Ä¢ R√©sistivit√© moyenne mesur√©e: {avg_rho:.2e} Œ©¬∑m
       ‚Ä¢ R√©sistivit√© typique th√©orique: {mat_data['typical']:.2e} Œ©¬∑m
       ‚Ä¢ Nature: {mat_data['nature']}
       ‚Ä¢ Concordance: {100 - abs(np.log10(avg_rho) - np.log10(mat_data['typical']))*20:.1f}%
            """
       
        # Recherche web automatique pour validation
        try:
            web_validation = web_search_enhanced(
                f"electrical resistivity values {query} geophysics materials validation",
                "validation"
            )
            report += f"""
   
    üåê VALIDATION PAR RECHERCHE WEB:
    {web_validation}
    """
        except:
            report += "\nüåê VALIDATION WEB: En cours..."
       
        # Calculs g√©ophysiques avanc√©s
        arr = np.array(resistivity_values)
        report += f"""
   
    üìä ANALYSES STATISTIQUES AVANC√âES:
   
    üî¢ Param√®tres de base:
    ‚Ä¢ Moyenne g√©om√©trique: {np.exp(np.mean(np.log(arr))):.2e} Œ©¬∑m
    ‚Ä¢ M√©diane: {np.median(arr):.2e} Œ©¬∑m
    ‚Ä¢ √âcart-type logarithmique: {np.std(np.log10(arr)):.3f}
    ‚Ä¢ Coefficient de variation: {np.std(arr)/np.mean(arr):.3f}
   
    üéØ Classification g√©ophysique:
    ‚Ä¢ Conducteurs (<10 Œ©¬∑m): {len(arr[arr < 10])} √©chantillons
    ‚Ä¢ Semi-conducteurs (10-100 Œ©¬∑m): {len(arr[(arr >= 10) & (arr < 100)])} √©chantillons
    ‚Ä¢ R√©sistants (100-1000 Œ©¬∑m): {len(arr[(arr >= 100) & (arr < 1000)])} √©chantillons
    ‚Ä¢ Tr√®s r√©sistants (>1000 Œ©¬∑m): {len(arr[arr >= 1000])} √©chantillons
   
    üå°Ô∏è Estimation de profondeur (mod√®le empirique):
    ‚Ä¢ Profondeur d'investigation: {np.mean(arr)*0.1:.1f} m (approximative)
    ‚Ä¢ R√©solution verticale: {np.std(arr)*0.05:.1f} m
        """
       
    except Exception as e:
        report += f"\n‚ùå Erreur dans l'analyse: {e}"
   
    report += """
   
    üí° RECOMMANDATIONS TECHNIQUES:
    ‚Ä¢ Utiliser inversion 2D/3D pour structures complexes
    ‚Ä¢ Valider par forages si possible
    ‚Ä¢ Consid√©rer variations saisonni√®res
    ‚Ä¢ Appliquer corrections topographiques si n√©cessaire
   
    üìö R√âF√âRENCES SCIENTIFIQUES:
    ‚Ä¢ Loke, M.H. (2001). Tutorial: 2-D and 3-D electrical imaging surveys
    ‚Ä¢ Telford et al. (1990). Applied Geophysics, Cambridge University Press
    ‚Ä¢ Reynolds, J.M. (2011). An Introduction to Applied and Environmental Geophysics
   
    ‚úÖ RAPPORT G√âN√âR√â AUTOMATIQUEMENT AVEC OUTILS AVANC√âS
    """
   
    return report
def generate_resistivity_plot(resistivity_values: list) -> str:
    """G√©n√®re un graphique des valeurs de r√©sistivit√©"""
    if not resistivity_values:
        return "Aucune donn√©e pour g√©n√©rer le graphique"
   
    import numpy as np
    import matplotlib.pyplot as plt
    import io
    import base64
    from resistivity_color_mapper import ResistivityColorMapper
   
    try:
        mapper = ResistivityColorMapper()
        arr = np.array(resistivity_values)
       
        # Cr√©er la figure avec subplots
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Analyse Compl√®te des R√©sistivit√©s ERT', fontsize=16, fontweight='bold')
       
        # 1. Profil de r√©sistivit√© avec couleurs
        colors = []
        for rho in arr:
            color, _ = mapper.get_color_for_resistivity(rho)
            colors.append(color)
       
        scatter = ax1.scatter(range(len(arr)), arr, c=colors, s=60, edgecolors='black', linewidth=0.5)
        ax1.plot(range(len(arr)), arr, 'k-', alpha=0.3, linewidth=1)
        ax1.set_xlabel('Position de mesure')
        ax1.set_ylabel('R√©sistivit√© (Œ©¬∑m)')
        ax1.set_title('Profil de R√©sistivit√© avec Couleurs ERT')
        ax1.set_yscale('log')
        ax1.grid(True, alpha=0.3)
       
        # 2. Histogramme
        ax2.hist(np.log10(arr), bins=15, color='skyblue', edgecolor='black', alpha=0.7)
        ax2.set_xlabel('Log10(R√©sistivit√©)')
        ax2.set_ylabel('Fr√©quence')
        ax2.set_title('Distribution des R√©sistivit√©s')
        ax2.grid(True, alpha=0.3)
       
        # 3. Classification par zones
        zones = {'Conducteur (<10)': arr[arr < 10],
                'Semi-conducteur (10-100)': arr[(arr >= 10) & (arr < 100)],
                'R√©sistant (100-1000)': arr[(arr >= 100) & (arr < 1000)],
                'Tr√®s r√©sistant (>1000)': arr[arr >= 1000]}
       
        zone_counts = [len(zone) for zone in zones.values()]
        zone_colors = ['#0000FF', '#00FF00', '#FFFF00', '#FF0000']
       
        wedges, texts, autotexts = ax3.pie(zone_counts, labels=zones.keys(), colors=zone_colors,
                                          autopct='%1.1f%%', startangle=90)
        ax3.set_title('Classification des Mat√©riaux')
       
        # 4. √âvolution temporelle simul√©e
        ax4.plot(range(len(arr)), arr, 'b-', linewidth=2, marker='o', markersize=4)
        ax4.fill_between(range(len(arr)), arr, alpha=0.3, color='lightblue')
        ax4.set_xlabel('S√©quence de mesure')
        ax4.set_ylabel('R√©sistivit√© (Œ©¬∑m)')
        ax4.set_title('√âvolution des Mesures')
        ax4.set_yscale('log')
        ax4.grid(True, alpha=0.3)
       
        # Ajuster la mise en page
        plt.tight_layout()
       
        # Convertir en base64 pour affichage HTML
        buffer = io.BytesIO()
        plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight')
        buffer.seek(0)
        plot_data = buffer.getvalue()
        buffer.close()
        plt.close()
       
        plot_base64 = base64.b64encode(plot_data).decode()
       
        return f'<img src="data:image/png;base64,{plot_base64}" style="max-width: 100%; height: auto;" alt="Graphique ERT">'
       
    except Exception as e:
        return f"Erreur lors de la g√©n√©ration du graphique: {e}"
def resistivity_color_analysis(resistivity_values: list, dat_file_path: str = None) -> str:
    """Analyse les couleurs de r√©sistivit√© ERT avec validation contre fichiers .dat et d√©tection de mat√©riaux r√©els"""
    if not resistivity_values:
        return "‚ùå Aucune valeur de r√©sistivit√© fournie pour l'analyse"
   
    import numpy as np
    from resistivity_color_mapper import ResistivityColorMapper, DynamicERTAnalyzer
   
    analysis = "üé® ANALYSE DES COULEURS DE R√âSISTIVIT√â ERT\n"
    analysis += "=" * 50 + "\n\n"
   
    # Initialisation des analyseurs
    mapper = ResistivityColorMapper()
    analyzer = DynamicERTAnalyzer()
   
    # Conversion en array numpy
    rho_data = np.array(resistivity_values)
   
    # Statistiques de base
    analysis += f"üìä STATISTIQUES DES R√âSISTIVIT√âS:\n"
    analysis += f" ‚Ä¢ Nombre de valeurs: {len(rho_data)}\n"
    analysis += f" ‚Ä¢ R√©sistivit√© moyenne: {np.mean(rho_data):.2f} Œ©.m\n"
    analysis += f" ‚Ä¢ M√©diane: {np.median(rho_data):.2f} Œ©.m\n"
    analysis += f" ‚Ä¢ √âcart-type: {np.std(rho_data):.2f} Œ©.m\n"
    analysis += f" ‚Ä¢ Plage: {np.min(rho_data):.2f} - {np.max(rho_data):.2f} Œ©.m\n"
    analysis += f" ‚Ä¢ Coefficient de variation: {np.std(rho_data)/np.mean(rho_data):.3f}\n\n"
   
    # Analyse des couleurs par valeur
    analysis += f"üé® CARTOGRAPHIE COULEUR PAR VALEUR:\n"
    sample_values = np.unique(np.round(rho_data, 2))[:15] # √âchantillon pour √©viter surcharge
   
    for rho in sample_values:
        color, desc = mapper.get_color_for_resistivity(rho)
        analysis += f" ‚Ä¢ œÅ = {rho:.2f} Œ©.m ‚Üí Couleur: {color} ({desc})\n"
    analysis += "\n"
   
    # D√©tection de mat√©riaux r√©els avec validation .dat
    analysis += f"üîç D√âTECTION DE MAT√âRIAUX R√âELS:\n"
   
    # Analyse compl√®te du profil
    profile_analysis = analyzer.analyze_resistivity_profile(rho_data, dat_file_path=dat_file_path)
   
    # Mat√©riaux identifi√©s
    materials = profile_analysis.get('materials', [])
    if materials:
        analysis += f"Mat√©riaux potentiels d√©tect√©s (avec validation r√©elle):\n"
        for i, material in enumerate(materials[:8], 1): # Top 8 mat√©riaux
            name = material.get('name', 'inconnu')
            category = material.get('category', 'inconnue')
            similarity = material.get('similarity_score', 0) * 100
            typical_rho = material.get('typical_value', 0)
            nature = material.get('nature', '')
            depth = material.get('depth_range', '')
           
            analysis += f" {i}. {name.upper()} ({category})\n"
            analysis += f" ‚Üí R√©sistivit√© typique: {typical_rho:.2e} Œ©.m\n"
            analysis += f" ‚Üí Score de similarit√©: {similarity:.1f}%\n"
            analysis += f" ‚Üí Nature: {nature}\n"
            if depth:
                analysis += f" ‚Üí Profondeur typique: {depth}\n"
           
            # Validation .dat
            if material.get('dat_validated', False):
                confidence = material.get('dat_confidence', 'low')
                analysis += f" ‚úÖ VALID√â PAR FICHIER .DAT (confiance: {confidence})\n"
            else:
                analysis += f" ‚ö†Ô∏è Non valid√© par fichier .dat\n"
           
            # Validation monde r√©el
            real_validation = analyzer.get_real_world_validation(name)
            if real_validation.get('confidence_level') != 'unknown':
                verified_range = real_validation.get('resistivity_range_verified')
                if verified_range:
                    analysis += f" üåç VALIDATION MONDE R√âEL: {verified_range[0]:.2e} - {verified_range[1]:.2e} Œ©.m\n"
                sources = real_validation.get('sources', [])
                if sources:
                    analysis += f" üìö Sources: {len(sources)} r√©f√©rences trouv√©es\n"
           
            analysis += "\n"
    else:
        analysis += "Aucun mat√©riau sp√©cifique d√©tect√© dans la base de donn√©es.\n\n"
   
    # Couches g√©ologiques identifi√©es
    layers = profile_analysis.get('layers', [])
    if layers:
        analysis += f"üèîÔ∏è COUCHES G√âOLOGIQUES IDENTIFI√âES:\n"
        for layer in layers:
            layer_id = layer.get('layer_id', 0)
            mean_rho = layer.get('mean_resistivity', 0)
            thickness = layer.get('thickness_estimate', 0) * 100
            color = layer.get('color', '#000000')
            desc = layer.get('description', '')
           
            analysis += f" ‚Ä¢ Couche {layer_id}: œÅ = {mean_rho:.1f} Œ©.m ({thickness:.1f}% du profil)\n"
            analysis += f" Couleur: {color} - {desc}\n"
        analysis += "\n"
   
    # Interpr√©tation g√©ologique
    geo_interp = profile_analysis.get('geological_interpretation', '')
    if geo_interp:
        analysis += f"üåç INTERPR√âTATION G√âOLOGIQUE:\n{geo_interp}\n\n"
   
    # Validation .dat globale
    dat_validation = profile_analysis.get('dat_validation')
    if dat_validation:
        analysis += f"üìÅ VALIDATION FICHIER .DAT:\n"
        if dat_validation.get('data_loaded', False):
            score = dat_validation.get('validation_score', 0) * 100
            confidence = dat_validation.get('confidence_level', 'low')
            matches = dat_validation.get('matching_materials', [])
           
            analysis += f" ‚Ä¢ Fichier charg√©: ‚úÖ\n"
            analysis += f" ‚Ä¢ Score de validation: {score:.1f}%\n"
            analysis += f" ‚Ä¢ Niveau de confiance: {confidence.upper()}\n"
            analysis += f" ‚Ä¢ Mat√©riaux correspondants: {len(matches)}\n"
        else:
            analysis += f" ‚Ä¢ Fichier non charg√© ou invalide: ‚ùå\n"
        analysis += "\n"
   
    # Recommandations
    recommendations = profile_analysis.get('recommendations', [])
    if recommendations:
        analysis += f"üí° RECOMMANDATIONS:\n"
        for rec in recommendations:
            analysis += f" ‚Ä¢ {rec}\n"
        analysis += "\n"
   
    # Recherche dynamique de comparaisons suppl√©mentaires
    analysis += f"üîç COMPARAISONS DYNAMIQUES SUPPL√âMENTAIRES:\n"
   
    # Recherche pour les cat√©gories principales
    categories_to_search = ['eau sal√©e', 'minerais m√©talliques', 'roches cristallines', 'sols argileux']
    for category in categories_to_search:
        try:
            search_results = analyzer.data_searcher.search_material_resistivity(category, "ERT geophysical")
            if search_results:
                extracted_values = analyzer.data_searcher.extract_resistivity_values(search_results)
                if extracted_values:
                    avg_rho = np.mean(extracted_values)
                    analysis += f" ‚Ä¢ {category.title()}: œÅ moyenne trouv√©e = {avg_rho:.2f} Œ©.m "
                    analysis += f"(plage: {min(extracted_values):.2f} - {max(extracted_values):.2f} Œ©.m)\n"
        except Exception as e:
            analysis += f" ‚Ä¢ {category.title()}: Erreur recherche - {e}\n"
   
    analysis += "\n"
   
    # Ajouter le tableau et les graphiques
    analysis += f"üìä TABLEAU D√âTAILL√â DES R√âSISTIVIT√âS:\n"
    table_html = generate_resistivity_table(resistivity_values)
    analysis += f"{table_html}\n\n"
   
    analysis += f"üìà GRAPHIQUES D'ANALYSE:\n"
    plot_html = generate_resistivity_plot(resistivity_values)
    analysis += f"{plot_html}\n\n"
   
    analysis += f"‚úÖ Analyse termin√©e - Toutes les d√©tections sont bas√©es sur des valeurs de r√©sistivit√© R√âELLES\n"
    analysis += f"et valid√©es contre des donn√©es scientifiques et fichiers .dat de r√©f√©rence."
   
    return analysis
# ========================================
# Configuration - CHEMINS UNIFI√âS
# ========================================
# D√©finir dynamiquement les chemins bas√©s sur le r√©pertoire du projet corrig√©
PROJECT_DIR = os.path.expanduser('~/RAG_ChatBot') # Chemin corrig√© vers le dossier contenant les donn√©es et poids
CHATBOT_DIR = PROJECT_DIR
VECTORDB_PATH = os.path.join(CHATBOT_DIR, "vectordb")
CHAT_VECTORDB_PATH = os.path.join(CHATBOT_DIR, "chat_vectordb") # AJOUT M√âMOIRE VECTORIELLE: Base d√©di√©e pour l'historique chat
PDFS_PATH = os.path.join(CHATBOT_DIR, "pdfs")
GRAPHS_PATH = os.path.join(CHATBOT_DIR, "graphs")
MAPS_PATH = os.path.join(CHATBOT_DIR, "maps")
METADATA_PATH = os.path.join(CHATBOT_DIR, "metadata.json")
TRAJECTORIES_PATH = os.path.join(CHATBOT_DIR, "trajectories.json")
WEB_CACHE_PATH = os.path.join(CHATBOT_DIR, "web_cache.json")
GENERATED_PATH = os.path.join(CHATBOT_DIR, "generated")
SUBMODELS_PATH = os.path.join(CHATBOT_DIR, "submodels") # Nouveau: Chemin pour les sous-mod√®les sklearn
MODEL_PATH = os.path.expanduser("~/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-V3-0324/snapshots/e9b33add76883f293d6bf61f6bd89b497e80e335")
# Mod√®les qui fonctionnent
WORKING_MODELS = {
    "DeepSeek V3 (Puissant)": "deepseek-ai/DeepSeek-V3-0324",
    "Gemma 2B (Rapide)": "google/gemma-2-2b-it",
    "Llama 3.1 8B (√âquilibr√©)": "meta-llama/Meta-Llama-3.1-8B-Instruct",
    "Qwen 2.5 7B (Polyvalent)": "Qwen/Qwen2.5-7B-Instruct",
    "SmolLM 3B (L√©ger)": "HuggingFaceTB/SmolLM3-3B",
}
# ========================================
# Configuration HuggingFace Token depuis .env
# ========================================
# Charger le token depuis .env dans le dossier corrig√©
env_path = os.path.join(CHATBOT_DIR, ".env")
if os.path.exists(env_path):
    load_dotenv(env_path)
    st.write(f"‚úÖ Fichier .env trouv√©: {env_path}")
else:
    st.write(f"‚ö†Ô∏è Aucun fichier .env trouv√© √† {env_path}")
    st.write("Cr√©ez un fichier .env dans ~/RAG_ChatBot avec: HF_TOKEN=hf_votre_token")
HF_TOKEN = os.getenv("HF_TOKEN")
if not HF_TOKEN:
    raise ValueError("‚ùå HF_TOKEN non trouv√© ! V√©rifiez votre fichier .env")
else:
    st.write(f"üîë Token HF configur√©: {HF_TOKEN[:10]}...")
# D√©finir la variable d'environnement pour huggingface_hub
os.environ["HF_TOKEN"] = HF_TOKEN
os.environ["HUGGINGFACE_HUB_TOKEN"] = HF_TOKEN
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
if not TAVILY_API_KEY:
    raise ValueError("‚ùå TAVILY_API_KEY non trouv√© ! V√©rifiez votre fichier .env")
# ========================================
# Test de connexion HuggingFace
# ========================================
def test_hf_connection():
    """Teste la connexion √† HuggingFace"""
    try:
        from huggingface_hub import whoami
        user_info = whoami(token=HF_TOKEN)
        st.write(f"‚úÖ Connexion HuggingFace r√©ussie: {user_info.get('name', 'Utilisateur')}")
        return True
    except Exception as e:
        st.write(f"‚ùå Erreur connexion HuggingFace: {e}")
        return False
# Tester la connexion au d√©marrage
if not test_hf_connection():
    st.write("‚ö†Ô∏è Probl√®me de connexion HuggingFace, v√©rifiez votre token")
# ========================================
# Fonctions utilitaires
# ========================================
def setup_drive():
    """Cr√©e les dossiers"""
    st.write("üìÅ Configuration des dossiers...")
    os.makedirs(CHATBOT_DIR, exist_ok=True)
    os.makedirs(PDFS_PATH, exist_ok=True)
    os.makedirs(GRAPHS_PATH, exist_ok=True)
    os.makedirs(MAPS_PATH, exist_ok=True)
    os.makedirs(GENERATED_PATH, exist_ok=True)
    os.makedirs(os.path.dirname(CHAT_VECTORDB_PATH), exist_ok=True) # AJOUT M√âMOIRE VECTORIELLE: Dossier pour chat_vectordb
    os.makedirs(SUBMODELS_PATH, exist_ok=True) # Nouveau: Dossier pour sous-mod√®les
    st.write(f"üìÅ Dossier principal : {CHATBOT_DIR}")
    return True
def extract_text_from_pdf(pdf_path):
    """Extraire le texte d'un PDF"""
    text = ""
    try:
        with fitz.open(pdf_path) as doc:
            for page_num, page in enumerate(doc):
                page_text = page.get_text()
                text += f"\n[Page {page_num + 1}]\n{page_text}\n"
        return text
    except Exception as e:
        st.write(f"‚ùå Erreur PDF {pdf_path}: {e}")
        return ""
def upload_and_process_pbf(pbf_file):
    """Traitement du fichier PBF upload√©"""
    if pbf_file is None:
        return None, None, "‚ùå Aucun fichier upload√©"
    pbf_path = pbf_file.name
    with open(pbf_path, "wb") as f:
        f.write(pbf_file.getvalue())
    st.write("‚öôÔ∏è Lecture du PBF et construction du graphe...")
    handler = RoadPOIHandler()
    handler.apply_file(pbf_path, locations=True)
    G = handler.graph
    pois = handler.pois
    # Sauvegarder dans le dossier chatbot
    graph_name = os.path.basename(pbf_path).replace('.osm.pbf', '_graph.graphml')
    graph_path = os.path.join(GRAPHS_PATH, graph_name)
    nx.write_graphml(G, graph_path)
    # Sauvegarder les POIs
    pois_name = graph_name.replace('_graph.graphml', '_pois.json')
    pois_path = os.path.join(GRAPHS_PATH, pois_name)
    with open(pois_path, 'w', encoding='utf-8') as f:
        json.dump(pois, f, indent=2, ensure_ascii=False)
    st.write(f"‚úÖ Graphe: {len(G)} n≈ìuds, {G.size()} ar√™tes")
    st.write(f"‚úÖ POIs: {len(pois)} points")
    st.write(f"üíæ Sauvegard√©: {graph_path}")
    return G, pois, f"‚úÖ Graphe cr√©√©: {len(G)} n≈ìuds, {len(pois)} POIs"
def load_existing_graph():
    """Charge un graphe existant"""
    graph_files = [f for f in os.listdir(GRAPHS_PATH) if f.endswith('_graph.graphml')] if os.path.exists(GRAPHS_PATH) else []
    if not graph_files:
        return None, None, "‚ùå Aucun graphe trouv√©"
    graph_file = graph_files[0]
    graph_path = os.path.join(GRAPHS_PATH, graph_file)
    pois_path = os.path.join(GRAPHS_PATH, graph_file.replace('_graph.graphml', '_pois.json'))
    try:
        G = nx.read_graphml(graph_path)
        pois = []
        if os.path.exists(pois_path):
            with open(pois_path, 'r', encoding='utf-8') as f:
                pois = json.load(f)
        return G, pois, f"‚úÖ Graphe charg√©: {len(G)} n≈ìuds, {len(pois)} POIs"
    except Exception as e:
        return None, None, f"‚ùå Erreur: {e}"
@st.cache_resource
def get_embedding_model():
    """Mod√®le d'embedding en cache pour √©viter rechargement"""
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    # Configuration simple pour √©viter conflits de param√®tres
    return HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': device}
    )
# AJOUT M√âMOIRE VECTORIELLE: Fonctions pour la m√©moire chat
def load_chat_vectordb():
    """Charger la base vectorielle pour l'historique chat"""
    if not os.path.exists(CHAT_VECTORDB_PATH):
        return None, "‚ö†Ô∏è Aucune base chat trouv√©e"
    embedding_model = get_embedding_model()
    try:
        chat_vectordb = FAISS.load_local(CHAT_VECTORDB_PATH, embedding_model, allow_dangerous_deserialization=True)
        return chat_vectordb, "‚úÖ Base chat charg√©e"
    except Exception as e:
        return None, f"‚ùå Erreur chat: {e}"
def add_to_chat_db(user_msg, ai_msg, chat_vectordb):
    """Ajouter un √©change user-AI √† la base chat"""
    if chat_vectordb is None:
        embedding_model = get_embedding_model()
        chat_vectordb = FAISS.from_texts([""], embedding_model) # Cr√©er si vide
    exchange = f"User: {user_msg} ||| Assistant: {ai_msg}"
    doc = Document(
        page_content=exchange,
        metadata={"type": "chat_exchange", "timestamp": time.time()}
    )
    chat_vectordb.add_documents([doc])
    chat_vectordb.save_local(CHAT_VECTORDB_PATH)
    return chat_vectordb
def chat_rag_search(question, chat_vectordb, k=3):
    """Rechercher dans l'historique chat pour contexte"""
    if not chat_vectordb:
        return []
    try:
        return chat_vectordb.similarity_search(question, k=k)
    except Exception as e:
        st.write(f"‚ùå Erreur recherche chat: {e}")
        return []
def process_pdfs():
    """Traiter les PDFs"""
    st.write("üìÑ Traitement des PDFs...")
    embedding_model = get_embedding_model()
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100
    )
    # Charger vectordb existante si elle existe
    vectordb = None
    if os.path.exists(VECTORDB_PATH):
        try:
            vectordb, _ = load_vectordb()
        except Exception as e:
            st.write(f"‚ö†Ô∏è Erreur chargement vectordb existante: {e}. Cr√©ation nouvelle.")
            vectordb = None
    # Charger m√©tadonn√©es existantes
    if os.path.exists(METADATA_PATH):
        with open(METADATA_PATH, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
    else:
        metadata = {"processed_files": [], "total_chunks": 0}
    processed_filenames = {p["filename"] for p in metadata["processed_files"]}
    all_documents = []
    pdf_files = [f for f in os.listdir(PDFS_PATH) if f.endswith('.pdf')] if os.path.exists(PDFS_PATH) else []
    if not pdf_files:
        return vectordb, "‚ö†Ô∏è Aucun PDF trouv√©"
  
    # Check pr√©liminaire : si aucun nouveau, skip
    new_pdfs = [f for f in pdf_files if f not in processed_filenames]
    if not new_pdfs:
        return vectordb, "‚úÖ Tous les PDFs d√©j√† trait√©s. Base √† jour !"
  
    progress_bar = st.progress(0)
    status_text = st.empty()
    new_chunks_count = 0
    new_processed = []
    total_pdfs = len(new_pdfs)
    current_pdf = 0
    for pdf_file in pdf_files:
        if pdf_file in processed_filenames:
            st.write(f" üìñ {pdf_file} d√©j√† trait√©, saut√©.")
            continue
        pdf_path = os.path.join(PDFS_PATH, pdf_file)
        st.write(f" üìñ Traitement nouveau PDF : {pdf_file}")
        status_text.text(f"Traitement de {pdf_file}...")
        text = extract_text_from_pdf(pdf_path)
        if not text.strip():
            continue
        try:
            chunks = text_splitter.split_text(text)
        except Exception as e:
            st.write(f"‚ùå Erreur split text pour {pdf_file}: {e}")
            continue
        for i, chunk in enumerate(chunks):
            doc = Document(
                page_content=chunk,
                metadata={
                    "source": pdf_file,
                    "chunk_id": i,
                    "type": "pdf"
                }
            )
            all_documents.append(doc)
        new_processed.append({"filename": pdf_file, "chunks": len(chunks)})
        new_chunks_count += len(chunks)
        current_pdf += 1
        progress = current_pdf / total_pdfs if total_pdfs > 0 else 1
        progress_bar.progress(progress)
    status_text.text("Finalisation...")
    # Ajouter les trajets sauvegard√©s (toujours, car ils peuvent changer)
    if os.path.exists(TRAJECTORIES_PATH):
        with open(TRAJECTORIES_PATH, 'r', encoding='utf-8') as f:
            trajectories = json.load(f)
        for traj in trajectories:
            traj_text = f"""Trajet: {traj.get('question', '')}
D√©part: {traj.get('start_name', '')}
Arriv√©e: {traj.get('end_name', '')}
Distance: {traj.get('distance', 0)/1000:.2f} km"""
            doc = Document(
                page_content=traj_text,
                metadata={"source": "trajectories", "type": "trajectory"}
            )
            all_documents.append(doc)
    if all_documents:
        try:
            if vectordb is None:
                vectordb = FAISS.from_documents(all_documents, embedding_model)
            else:
                vectordb.add_documents(all_documents)
            vectordb.save_local(VECTORDB_PATH)
        except Exception as e:
            st.write(f"‚ùå Erreur sauvegarde vectordb: {e}")
            return None, "‚ùå √âchec sauvegarde base"
    # Mettre √† jour m√©tadonn√©es seulement si changements
    if new_processed:
        metadata["processed_files"].extend(new_processed)
        metadata["total_chunks"] += new_chunks_count
        with open(METADATA_PATH, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
    progress_bar.progress(1)
    status_text.text("Termin√© !")
    return vectordb, f"‚úÖ Base mise √† jour : {len(new_processed)} nouveaux PDFs trait√©s, {new_chunks_count} nouveaux chunks (total : {metadata['total_chunks']})"
def load_vectordb():
    """Charge la base vectorielle"""
    if not os.path.exists(VECTORDB_PATH):
        return None, "‚ö†Ô∏è Aucune base trouv√©e"
    embedding_model = get_embedding_model()
    try:
        vectordb = FAISS.load_local(VECTORDB_PATH, embedding_model, allow_dangerous_deserialization=True)
        return vectordb, "‚úÖ Base charg√©e"
    except Exception as e:
        return None, f"‚ùå Erreur: {e}"
def save_trajectory(question, response, trajectory_info):
    """Sauvegarde un trajet"""
    trajectories = []
    if os.path.exists(TRAJECTORIES_PATH):
        with open(TRAJECTORIES_PATH, 'r', encoding='utf-8') as f:
            trajectories = json.load(f)
    new_trajectory = {
        "question": question,
        "response": response,
        "start_name": trajectory_info.get('start', {}).get('name', ''),
        "end_name": trajectory_info.get('end', {}).get('name', ''),
        "distance": trajectory_info.get('distance', 0)
    }
    trajectories.append(new_trajectory)
    with open(TRAJECTORIES_PATH, 'w', encoding='utf-8') as f:
        json.dump(trajectories, f, indent=2, ensure_ascii=False)
def upload_pdfs(uploaded_files):
    """Upload des PDFs"""
    if uploaded_files is None:
        return []
    saved_files = []
    for file in uploaded_files:
        filename = file.name
        filepath = os.path.join(PDFS_PATH, filename)
        with open(filepath, "wb") as f:
            f.write(file.getvalue())
        saved_files.append(filename)
    return saved_files
# ========================================
# Syst√®me de Cache Web Intelligent
# ========================================
def load_web_cache():
    """Charge le cache web"""
    if os.path.exists(WEB_CACHE_PATH):
        try:
            with open(WEB_CACHE_PATH, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            pass
    return {}
def save_web_cache(cache):
    """Sauvegarde le cache web"""
    try:
        with open(WEB_CACHE_PATH, 'w', encoding='utf-8') as f:
            json.dump(cache, f, indent=2, ensure_ascii=False)
    except Exception as e:
        st.write(f"Erreur sauvegarde cache: {e}")
def get_cache_key(query, source="text"):
    """G√©n√®re une cl√© de cache pour une requ√™te"""
    return f"{source}:{query.lower().strip()}"
def is_cache_expired(cache_entry, max_age_hours=24):
    """V√©rifie si l'entr√©e du cache a expir√©"""
    current_time = time.time()
    return (current_time - cache_entry.get('timestamp', 0)) > (max_age_hours * 3600)
def get_cache_stats():
    """Obtient les statistiques du cache"""
    try:
        cache = load_web_cache()
        if not cache:
            return "Cache vide"
        total_entries = len(cache)
        expired_count = sum(1 for entry in cache.values() if is_cache_expired(entry))
        valid_count = total_entries - expired_count
        return f"üìä Cache: {total_entries} entr√©es total, {valid_count} valides, {expired_count} expir√©es"
    except Exception as e:
        return f"‚ùå Erreur stats: {e}"
# ========================================
# Fonctions RAG et Web Search Am√©lior√©es
# ========================================
class LocalClient:
    def __init__(self):
        from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
       
        MODEL_PATH = "/root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-V3-0324/snapshots/e9b33add76883f293d6bf61f6bd89b497e80e335"
       
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, local_files_only=True)
       
        # Load model with device_map for large models
        self.model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True,
            local_files_only=True,
            device_map="auto",
            torch_dtype="auto"
        )
       
        self.model.eval()
    def chat_completion(self, messages, model, max_tokens, temperature, stream=False):
        try:
            # Use chat template for proper formatting
            prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(self.model.device)
            outputs = self.model.generate(inputs, max_new_tokens=max_tokens, temperature=temperature, do_sample=temperature > 0, pad_token_id=self.tokenizer.eos_token_id)
            generated_ids = outputs[0][inputs.shape[-1]:]
            response = self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
            class Choice:
                def __init__(self, content):
                    self.message = type('msg', (), {'content': content})()
            class Resp:
                def __init__(self, choice):
                    self.choices = [choice]
            return Resp(Choice(response))
        except Exception as e:
            class Choice:
                def __init__(self, content):
                    self.message = type('msg', (), {'content': content})()
            class Resp:
                def __init__(self, choice):
                    self.choices = [choice]
            return Resp(Choice(f"Erreur locale: {str(e)}"))
@st.cache_resource
def create_client():
    """Cr√©er le client Inference avec gestion d'erreurs am√©lior√©e"""
    try:
        client = InferenceClient(token=HF_TOKEN)
        return client
    except Exception as e:
        st.write(f"‚ùå Erreur cr√©ation client: {e}. Passage en mode local.")
        return LocalClient()
def rag_search(question, vectordb, k=3):
    """Rechercher dans la base vectorielle"""
    if not vectordb:
        return []
    try:
        return vectordb.similarity_search(question, k=k)
    except Exception as e:
        st.write(f"‚ùå Erreur recherche: {e}")
        return []
def enhanced_web_search(query, max_results=5, search_type="text", use_cache=True):
    """
    Recherche web avanc√©e avec cache intelligent et multiple sources
    Args:
        query: Requ√™te de recherche
        max_results: Nombre max de r√©sultats
        search_type: Type de recherche ("text", "news", "both")
        use_cache: Utiliser le cache
    Returns:
        Liste de r√©sultats enrichis
    """
    cache = load_web_cache() if use_cache else {}
    results = []
    try:
        # Recherche texte
        if search_type in ["text", "both"]:
            cache_key = get_cache_key(query, "text")
            if cache_key in cache and not is_cache_expired(cache[cache_key]):
                st.write(f"üìã Utilisation cache pour: {query}")
                text_results = cache[cache_key]['results']
            else:
                st.write(f"üîç Recherche web pour: {query}")
                tavily = TavilyClient(api_key=TAVILY_API_KEY)
                text_results = []
                try:
                    raw_results = tavily.search(query, max_results=max_results, search_depth="advanced", topic="general")
                    for r in raw_results.get('results', []):
                        text_results.append({
                            'title': r.get('title', ''),
                            'body': r.get('content', ''),
                            'href': r.get('url', ''),
                            'source_type': 'web_search'
                        })
                    # Sauvegarder en cache
                    cache[cache_key] = {
                        'results': text_results,
                        'timestamp': time.time()
                    }
                    if use_cache:
                        save_web_cache(cache)
                except Exception as e:
                    st.write(f"Erreur recherche texte: {e}")
                    text_results = []
            results.extend(text_results)
        # Recherche actualit√©s
        if search_type in ["news", "both"]:
            cache_key = get_cache_key(query, "news")
            if cache_key in cache and not is_cache_expired(cache[cache_key], max_age_hours=6):
                news_results = cache[cache_key]['results']
            else:
                tavily = TavilyClient(api_key=TAVILY_API_KEY)
                news_results = []
                try:
                    raw_news = tavily.search(query, max_results=max_results//2 if search_type == "both" else max_results, search_depth="advanced", topic="news")
                    for r in raw_news.get('results', []):
                        news_results.append({
                            'title': r.get('title', ''),
                            'body': r.get('content', ''),
                            'url': r.get('url', ''),
                            'date': r.get('published_date', ''),
                            'source': r.get('source', ''),
                            'source_type': 'news'
                        })
                    # Sauvegarder en cache (6h pour les news)
                    cache[cache_key] = {
                        'results': news_results,
                        'timestamp': time.time()
                    }
                    if use_cache:
                        save_web_cache(cache)
                except Exception as e:
                    st.write(f"Erreur recherche news: {e}")
                    news_results = []
            results.extend(news_results)
    except Exception as e:
        st.write(f"‚ùå Erreur recherche web globale: {e}")
        results = [{'title': 'Erreur de recherche', 'body': f'Erreur: {e}', 'source_type': 'error'}]
    return results
def smart_content_extraction(url, max_length=1000):
    """
    Extraction intelligente du contenu d'une page web
    Args:
        url: URL √† scraper
        max_length: Longueur max du contenu
    Returns:
        Contenu extrait et nettoy√©
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        # Supprimer les √©l√©ments non pertinents
        for element in soup(['script', 'style', 'nav', 'footer', 'aside', 'header']):
            element.decompose()
        # Extraire le texte principal
        main_content = soup.find('main') or soup.find('article') or soup.find('div', class_='content') or soup.body
        if main_content:
            text = main_content.get_text(separator=' ', strip=True)
        else:
            text = soup.get_text(separator=' ', strip=True)
        # Nettoyer et tronquer
        text = ' '.join(text.split()) # Normaliser les espaces
        return text[:max_length] + ('...' if len(text) > max_length else '')
    except Exception as e:
        st.write(f"Erreur extraction contenu {url}: {e}")
        return f"Impossible d'extraire le contenu de {url}"
def intelligent_query_expansion(query):
    """
    Expansion intelligente des requ√™tes pour am√©liorer les r√©sultats
    Args:
        query: Requ√™te originale
    Returns:
        Liste de requ√™tes expandues
    """
    expanded_queries = [query] # Toujours inclure la requ√™te originale
    # D√©tection de mots-cl√©s pour expansion contextuelle
    keywords = {
        'actualit√©': ['news', 'derni√®res nouvelles', 'r√©cent'],
        'comment': ['tutorial', 'guide', '√©tapes'],
        'pourquoi': ['raison', 'cause', 'explication'],
        'comparaison': ['vs', 'diff√©rence', 'comparatif'],
        'prix': ['co√ªt', 'tarif', 'budget'],
        'avis': ['opinion', 'critique', 'review']
    }
    query_lower = query.lower()
    for trigger, expansions in keywords.items():
        if trigger in query_lower:
            for expansion in expansions:
                expanded_queries.append(f"{query} {expansion}")
    return expanded_queries[:3] # Limiter √† 3 requ√™tes max
def hybrid_search_enhanced(query, vectordb, k=3, web_search_enabled=True, search_type="both", chat_vectordb=None): # AJOUT M√âMOIRE VECTORIELLE: Param pour chat_vectordb
    """
    Recherche hybride am√©lior√©e combinant RAG local et web avec intelligence
    Args:
        query: Requ√™te de recherche
        vectordb: Base vectorielle locale
        k: Nombre de r√©sultats RAG
        web_search_enabled: Activer la recherche web
        search_type: Type de recherche web
        chat_vectordb: Base pour historique chat (optionnel)
    Returns:
        Liste de documents combin√©s et enrichis
    """
    all_results = []
    # 1. Recherche RAG locale
    local_docs = rag_search(query, vectordb, k)
    for doc in local_docs:
        doc.metadata['search_source'] = 'local_rag'
        doc.metadata['relevance_score'] = 1.0 # Score max pour les docs locaux
    all_results.extend(local_docs)
    # AJOUT M√âMOIRE VECTORIELLE: Recherche dans historique chat pour contexte conversationnel
    if chat_vectordb:
        chat_docs = chat_rag_search(query, chat_vectordb, k=3)
        for doc in chat_docs:
            doc.metadata['search_source'] = 'chat_history'
            doc.metadata['relevance_score'] = 0.9
        all_results.extend(chat_docs[:2]) # Limiter √† 2 pour √©viter surcharge
    # 2. Recherche web intelligente si activ√©e
    if web_search_enabled:
        st.write(f"üåê Recherche web activ√©e pour: {query}")
        # Expansion de requ√™te pour de meilleurs r√©sultats
        expanded_queries = intelligent_query_expansion(query)
        web_results = []
        for exp_query in expanded_queries:
            try:
                search_results = enhanced_web_search(
                    exp_query,
                    max_results=3,
                    search_type=search_type
                )
                for result in search_results:
                    # Cr√©er un document √† partir du r√©sultat web
                    content = f"Titre: {result.get('title', '')}\n"
                    content += f"Contenu: {result.get('body', '')}\n"
                    if result.get('source_type') == 'news' and result.get('date'):
                        content += f"Date: {result.get('date')}\n"
                        content += f"Source: {result.get('source', '')}\n"
                    # Extraction de contenu suppl√©mentaire si URL disponible
                    url = result.get('href') or result.get('url')
                    if url and len(result.get('body', '')) < 200:
                        st.write(f"üìÑ Extraction contenu de: {url}")
                        extra_content = smart_content_extraction(url)
                        if extra_content and "Impossible d'extraire" not in extra_content:
                            content += f"\nContenu d√©taill√©: {extra_content}"
                    doc = Document(
                        page_content=content,
                        metadata={
                            'source': url or 'web_search',
                            'type': result.get('source_type', 'web'),
                            'search_source': 'web',
                            'query_used': exp_query,
                            'relevance_score': 0.8 if exp_query == query else 0.6
                        }
                    )
                    web_results.append(doc)
            except Exception as e:
                st.write(f"Erreur recherche pour '{exp_query}': {e}")
                continue
        # Filtrer les doublons et trier par pertinence
        unique_web_results = []
        seen_urls = set()
        for doc in web_results:
            url = doc.metadata.get('source', '')
            if url not in seen_urls:
                seen_urls.add(url)
                unique_web_results.append(doc)
        # Trier par score de pertinence
        unique_web_results.sort(key=lambda x: x.metadata.get('relevance_score', 0), reverse=True)
        all_results.extend(unique_web_results[:5]) # Max 5 r√©sultats web
    return all_results
def generate_answer_enhanced(question, context_docs, model_name, include_sources=True):
    """
    G√©n√©ration de r√©ponse am√©lior√©e avec gestion des sources multiples
    Args:
        question: Question pos√©e
        context_docs: Documents de contexte
        model_name: Mod√®le √† utiliser
        include_sources: Inclure les sources dans la r√©ponse
    Returns:
        R√©ponse g√©n√©r√©e avec sources
    """
    if not context_docs:
        context = "Aucun contexte sp√©cifique trouv√©."
    else:
        context_parts = []
        local_sources = []
        web_sources = []
        chat_sources = [] # AJOUT M√âMOIRE VECTORIELLE: Sources pour historique chat
        for i, doc in enumerate(context_docs):
            source = doc.metadata.get('source', 'Document inconnu')
            doc_type = doc.metadata.get('type', 'unknown')
            search_source = doc.metadata.get('search_source', 'unknown')
            content = doc.page_content.strip()
            # Classifier les sources
            if search_source == 'local_rag':
                local_sources.append(f"[{i+1}] {source} ({doc_type})")
            elif search_source == 'chat_history':
                chat_sources.append(f"[{i+1}] Historique pr√©c√©dent: {source}")
            else:
                web_sources.append(f"[{i+1}] {source}")
            context_parts.append(f"[Source {i+1} - {doc_type}]\n{content}")
        context = "\n\n".join(context_parts)
    # Prompt am√©lior√© avec instructions pour les sources (ajout chat)
    prompt = f"""Tu es un assistant IA intelligent qui r√©pond aux questions en utilisant √† la fois des documents locaux, l'historique des conversations pass√©es, et des informations web r√©centes.
CONTEXTE DISPONIBLE (incluant historique pour continuit√©):
{context}
QUESTION: {question}
INSTRUCTIONS:
- Utilise l'historique chat pour maintenir la fluidit√© et rappeler les √©changes pr√©c√©dents
- Utilise toutes les sources disponibles pour donner une r√©ponse compl√®te et pr√©cise
- Si les informations web contredisent les documents locaux ou l'historique, mentionne les deux perspectives
- Privil√©gie les informations r√©centes pour les sujets d'actualit√©
- Sois pr√©cis et cite tes sources si n√©cessaire
- Si certaines informations manquent, dis-le clairement et propose de clarifier bas√© sur l'historique
R√âPONSE D√âTAILL√âE:"""
    try:
        client = create_client()
        messages = [{"role": "user", "content": prompt}]
        response = client.chat_completion(
            messages=messages,
            model=model_name,
            max_tokens=600,
            temperature=0.3
        )
        answer = response.choices[0].message.content
        # Ajouter les sources si demand√©
        if include_sources and context_docs:
            sources_text = "\n\nüìö **Sources consult√©es:**\n"
            if chat_sources: # AJOUT M√âMOIRE VECTORIELLE
                sources_text += "**Historique conversation:**\n"
                for source in chat_sources[:2]:
                    sources_text += f"‚Ä¢ {source}\n"
            if local_sources:
                sources_text += "**Documents locaux:**\n"
                for source in local_sources[:3]: # Limiter l'affichage
                    sources_text += f"‚Ä¢ {source}\n"
            if web_sources:
                sources_text += "**Sources web:**\n"
                for source in web_sources[:3]: # Limiter l'affichage
                    sources_text += f"‚Ä¢ {source}\n"
            answer += sources_text
        return answer
    except Exception as e:
        error_str = str(e)
        # Check for payment error and retry with LocalClient
        if "402" in error_str or "Payment Required" in error_str:
            try:
                # Retry with LocalClient
                local_client = LocalClient()
                messages = [{"role": "user", "content": prompt}]
                response = local_client.chat_completion(
                    messages=messages,
                    model=model_name,
                    max_tokens=600,
                    temperature=0.3
                )
                answer = response.choices[0].message.content
                # Ajouter les sources si demand√©
                if include_sources and context_docs:
                    sources_text = "\n\nüìö **Sources consult√©es (mode local):**\n"
                    if chat_sources:
                        sources_text += "**Historique conversation:**\n"
                        for source in chat_sources[:2]:
                            sources_text += f"‚Ä¢ {source}\n"
                    if local_sources:
                        sources_text += "**Documents locaux:**\n"
                        for source in local_sources[:3]:
                            sources_text += f"‚Ä¢ {source}\n"
                    if web_sources:
                        sources_text += "**Sources web:**\n"
                        for source in web_sources[:3]:
                            sources_text += f"‚Ä¢ {source}\n"
                    answer += sources_text
                return answer + "\n\n‚ö†Ô∏è R√©ponse g√©n√©r√©e en mode local (API distante indisponible)."
            except Exception as local_e:
                return f"‚ùå Erreur g√©n√©ration (m√™me en local): {str(local_e)}"
        else:
            return f"‚ùå Erreur g√©n√©ration: {error_str}"
# ========================================
# Fonctions Web Search et Hybrid (Mises √† jour)
# ========================================
def web_search(query, max_results=5):
    """Version simplifi√©e pour compatibilit√©"""
    try:
        results = enhanced_web_search(query, max_results, "text")
        return [f"{r.get('title', '')}: {r.get('href', r.get('url', ''))} - {r.get('body', '')}" for r in results]
    except Exception as e:
        return [f"‚ùå Erreur recherche web: {e}"]
def hybrid_search(query, vectordb, k=3):
    """Version simplifi√©e pour compatibilit√©"""
    return hybrid_search_enhanced(query, vectordb, k, web_search_enabled=True)
def final_search(question, vectordb, graph, pois):
    """Recherche finale combinant toutes les sources"""
    results = hybrid_search_enhanced(question, vectordb, k=3, web_search_enabled=True)
    # OSM si mention lieu
    if any(keyword in question.lower() for keyword in ["aller", "trajet", "itin√©raire", "route", "navigation"]):
        try:
            carte, reponse, traj = calculer_trajet(question, graph, pois)
            if traj:
                results.append(Document(
                    page_content=reponse,
                    metadata={"source": "trajet_osm", "type": "navigation"}
                ))
        except:
            pass
    return results
# ========================================
# Fonctions Mod√®les Hugging Face Sp√©cialis√©s
# ========================================
@st.cache_resource
def initialize_specialized_models():
    """Initialise les mod√®les sp√©cialis√©s avec gestion d'erreurs"""
    device_id = 0 if torch.cuda.is_available() else -1
    models = {}
    try:
        models['summarizer'] = pipeline("summarization", model="facebook/bart-large-cnn", device=device_id)
        st.write("‚úÖ Mod√®le de r√©sum√© charg√©")
    except Exception as e:
        st.write(f"‚ö†Ô∏è Erreur chargement summarizer: {e}")
        models['summarizer'] = None
    try:
        models['translator'] = pipeline("translation", model="Helsinki-NLP/opus-mt-fr-en", device=device_id)
        st.write("‚úÖ Mod√®le de traduction charg√©")
    except Exception as e:
        st.write(f"‚ö†Ô∏è Erreur chargement translator: {e}")
        models['translator'] = None
    try:
        models['captioner'] = None
        st.write("‚úÖ Captioner configur√© pour utiliser LLM (llava)")
    except Exception as e:
        st.write(f"‚ö†Ô∏è Erreur chargement captioner: {e}")
        models['captioner'] = None
    try:
        models['ner'] = pipeline("ner", model="dbmdz/bert-large-cased-finetuned-conll03-english", device=device_id)
        st.write("‚úÖ Mod√®le NER charg√©")
        st.write("‚ö†Ô∏è Warning NER ignor√© : weights pooler non utilis√©s (normal pour ce checkpoint).")
    except Exception as e:
        st.write(f"‚ö†Ô∏è Erreur chargement NER: {e}")
        models['ner'] = None
    return models
# Initialiser les mod√®les
SPECIALIZED_MODELS = initialize_specialized_models()
def summarize_text(text):
    if SPECIALIZED_MODELS['summarizer'] is None:
        return "‚ùå Mod√®le de r√©sum√© non disponible"
    try:
        return SPECIALIZED_MODELS['summarizer'](text[:1024], max_length=200, min_length=30, do_sample=False)[0]['summary_text']
    except Exception as e:
        return f"‚ùå Erreur r√©sum√©: {e}"
def translate_text(text, src_lang="fr", tgt_lang="en"):
    if SPECIALIZED_MODELS['translator'] is None:
        return "‚ùå Mod√®le de traduction non disponible"
    try:
        return SPECIALIZED_MODELS['translator'](text)[0]['translation_text']
    except Exception as e:
        return f"‚ùå Erreur traduction: {e}"
def caption_image(image_path):
    client = create_client()
    model = "llava-hf/llava-1.5-7b-hf"
    prompt = "Generate a detailed caption for this image."
    try:
        return client.image_to_text(image_path, prompt=prompt, model=model, max_tokens=500)
    except Exception as e:
        return f"‚ùå Erreur caption: {e}"
def extract_entities(text):
    if SPECIALIZED_MODELS['ner'] is None:
        return "‚ùå Mod√®le NER non disponible"
    try:
        return SPECIALIZED_MODELS['ner'](text)
    except Exception as e:
        return f"‚ùå Erreur NER: {e}"
# ========================================
# Fonctions de g√©n√©ration avec Stable Diffusion et similaires
# ========================================
def generate_text_to_image(prompt):
    """G√©n√®re une image √† partir de texte"""
    if not DIFFUSERS_AVAILABLE:
        return "‚ùå Diffusers non disponible - fonctionnalit√© d√©sactiv√©e"
    try:
        pipe = DiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4", use_auth_token=HF_TOKEN)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        pipe.to(device)
        image = pipe(prompt).images[0]
        path = os.path.join(GENERATED_PATH, f"image_{int(time.time())}.png")
        image.save(path)
        return f"Image g√©n√©r√©e et sauvegard√©e √† {path}"
    except Exception as e:
        return f"‚ùå Erreur g√©n√©ration image: {e}"
def generate_text_to_video(prompt):
    """G√©n√®re une vid√©o √† partir de texte"""
    if not DIFFUSERS_AVAILABLE:
        return "‚ùå Diffusers non disponible - fonctionnalit√© d√©sactiv√©e"
    try:
        pipe = DiffusionPipeline.from_pretrained("damo-vilab/text-to-video-ms-1.7b", torch_dtype=torch.float16, variant="fp16", use_auth_token=HF_TOKEN)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        if device == "cuda":
            pipe.enable_model_cpu_offload()
        else:
            pipe.to(device)
        gen = pipe(prompt, num_inference_steps=25)
        frames = gen.frames[0] # Assuming batch size 1
        path = os.path.join(GENERATED_PATH, f"video_{int(time.time())}.gif")
        imageio.mimsave(path, frames, fps=5)
        return f"Vid√©o g√©n√©r√©e et sauvegard√©e √† {path}"
    except Exception as e:
        return f"‚ùå Erreur g√©n√©ration vid√©o: {e}"
def generate_text_to_audio(prompt):
    """G√©n√®re un son √† partir de texte"""
    if not DIFFUSERS_AVAILABLE:
        return "‚ùå Diffusers non disponible - fonctionnalit√© d√©sactiv√©e"
    try:
        pipe = AudioLDMPipeline.from_pretrained("cvssp/audio-ldm", torch_dtype=torch.float16, use_auth_token=HF_TOKEN)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        pipe.to(device)
        audio = pipe(prompt, audio_length_in_s=5.0).audios[0]
        path = os.path.join(GENERATED_PATH, f"audio_{int(time.time())}.wav")
        wavfile.write(path, rate=16000, data=audio) # Assuming 16kHz sample rate
        return f"Son g√©n√©r√© et sauvegard√© √† {path}"
    except Exception as e:
        return f"‚ùå Erreur g√©n√©ration son: {e}"
def generate_text_to_3d(prompt):
    """G√©n√®re un mod√®le 3D √† partir de texte (rendue image)"""
    if not DIFFUSERS_AVAILABLE:
        return "‚ùå Diffusers non disponible - fonctionnalit√© d√©sactiv√©e"
    try:
        pipe = ShapEPipeline.from_pretrained("openai/shap-e", use_auth_token=HF_TOKEN)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        pipe.to(device)
        output = pipe(prompt, num_inference_steps=64)
        image = output.images[0]
        path = os.path.join(GENERATED_PATH, f"3d_text_{int(time.time())}.png")
        image.save(path)
        return f"Rendu 3D g√©n√©r√© et sauvegard√© √† {path}"
    except Exception as e:
        return f"‚ùå Erreur g√©n√©ration 3D (texte): {e}"
def generate_image_to_3d(image_path):
    """G√©n√®re un mod√®le 3D √† partir d'une image (rendue image)"""
    if not DIFFUSERS_AVAILABLE:
        return "‚ùå Diffusers non disponible - fonctionnalit√© d√©sactiv√©e"
    try:
        pipe = ShapEImg2ImgPipeline.from_pretrained("openai/shap-e", use_auth_token=HF_TOKEN)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        pipe.to(device)
        image = Image.open(image_path)
        output = pipe(image, num_inference_steps=64)
        rendered_image = output.images[0]
        path = os.path.join(GENERATED_PATH, f"3d_image_{int(time.time())}.png")
        rendered_image.save(path)
        return f"Rendu 3D g√©n√©r√© √† partir de l'image et sauvegard√© √† {path}"
    except Exception as e:
        return f"‚ùå Erreur g√©n√©ration 3D (image): {e}"
# ========================================
# Agent LangChain Am√©lior√© avec Recherche Web
# ========================================
def get_llm(model_name):
    """Fonction dynamique pour obtenir LLM: API si disponible, local sinon"""
    try:
        llm = HuggingFaceEndpoint(
            repo_id=model_name,
            huggingfacehub_api_token=HF_TOKEN,
            temperature=0.3,
            max_new_tokens=600
        )
        st.write(f"‚úÖ Utilisation API pour {model_name}")
        return llm
    except Exception as e:
        st.write(f"‚ö†Ô∏è API indisponible ({e}). Fallback sur LLM local Qwen.")
        return st.session_state.qwen_llm  # Utilise le Qwen local

def create_enhanced_agent(model_name, vectordb, graph, pois, chat_vectordb=None): # AJOUT M√âMOIRE VECTORIELLE: Param pour chat
    """
    Cr√©e un agent LangChain am√©lior√© avec capacit√©s de recherche web
    Args:
        model_name: Nom du mod√®le HuggingFace
        vectordb: Base vectorielle locale
        graph: Graphe OSM
        pois: Points d'int√©r√™t
        chat_vectordb: Base pour historique chat (optionnel)
    Returns:
        Agent configur√© avec tous les outils
    """
    llm = get_llm(model_name)  # Switch dynamique ici
    # Configuration des outils de recherche web
    search_wrapper = DuckDuckGoSearchAPIWrapper(
        region="fr-fr",
        time="d",
        max_results=5
    )
    search_tool = TavilySearchResults(api_key=TAVILY_API_KEY, max_results=5)
    search_results_tool = TavilySearchResults(api_key=TAVILY_API_KEY, max_results=5, include_raw_content=True)
    tools = [
        # Outils de base RAG et recherche
        Tool(
            name="Local_Knowledge_Base",
            func=lambda q: "\n\n".join([d.page_content for d in rag_search(q, vectordb, k=3)]),
            description="Recherche dans la base de connaissances locale (PDFs et documents internes). Utilise ceci en PREMIER pour les questions sur des documents sp√©cifiques."
        ),
        Tool(
            name="Chat_History_Search", # AJOUT M√âMOIRE VECTORIELLE: Nouvel outil pour historique
            func=lambda q: "\n\n".join([d.page_content for d in chat_rag_search(q, chat_vectordb, k=3)]),
            description="Recherche dans l'historique des conversations pass√©es pour maintenir la continuit√©. Utilise pour les questions de suites de discussion."
        ),
        Tool(
            name="Web_Search",
            func=lambda q: search_tool.run(q),
            description="Recherche sur Internet pour des informations r√©centes, actualit√©s, ou des connaissances g√©n√©rales non disponibles localement."
        ),
        Tool(
            name="Web_Search_Detailed",
            func=lambda q: search_results_tool.run(q),
            description="Recherche web d√©taill√©e avec sources et liens. Utilise pour obtenir des r√©sultats web structur√©s avec URLs."
        ),
        Tool(
            name="Hybrid_Search",
            func=lambda q: "\n\n".join([d.page_content for d in hybrid_search_enhanced(q, vectordb, k=3, web_search_enabled=True, chat_vectordb=chat_vectordb)]),
            description="Recherche hybride combinant base locale, historique chat ET web. Id√©al pour des questions n√©cessitant √† la fois des donn√©es internes, pass√©es et externes."
        ),
        Tool(
            name="Current_News_Search",
            func=lambda q: "\n\n".join([f"{r.get('title', '')}: {r.get('body', '')}" for r in enhanced_web_search(q, search_type="news")]),
            description="Recherche sp√©cialis√©e pour les actualit√©s r√©centes et informations temporelles."
        ),
        # Outils sp√©cialis√©s
        Tool(
            name="OSM_Route_Calculator",
            func=lambda q: calculer_trajet(q, graph, pois)[1] if graph and pois else "‚ùå Aucune carte OSM disponible",
            description="Calcule des itin√©raires routiers entre deux lieux. Utilise pour les questions de navigation, trajets, ou g√©olocalisation."
        ),
        Tool(
            name="Smart_Content_Extractor",
            func=lambda url: smart_content_extraction(url) if url.startswith('http') else "‚ùå URL invalide",
            description="Extrait le contenu d√©taill√© d'une page web sp√©cifique. Fournis une URL compl√®te."
        ),
        Tool(
            name="Text_Summarizer",
            func=summarize_text,
            description="R√©sume un texte long en version concise. Utile pour synth√©tiser des informations volumineuses."
        ),
        Tool(
            name="Language_Translator",
            func=translate_text,
            description="Traduit du fran√ßais vers l'anglais. Utile pour traiter des sources en langue √©trang√®re."
        ),
        Tool(
            name="Image_Analyzer",
            func=caption_image,
            description="Analyse et d√©crit le contenu d'une image. Fournis le chemin vers un fichier image."
        ),
        Tool(
            name="Entity_Extractor",
            func=lambda t: json.dumps(extract_entities(t)),
            description="Extrait des entit√©s nomm√©es (personnes, lieux, organisations) d'un texte."
        ),
        # Nouveaux outils Stable Diffusion via API
        Tool(
            name="Text_To_Image_Generator",
            func=generate_text_to_image,
            description="G√©n√®re une image √† partir d'une description textuelle. Fournis un prompt descriptif."
        ),
        Tool(
            name="Text_To_Video_Generator",
            func=generate_text_to_video,
            description="G√©n√®re une vid√©o √† partir d'une description textuelle. Fournis un prompt descriptif."
        ),
        Tool(
            name="Text_To_Audio_Generator",
            func=generate_text_to_audio,
            description="G√©n√®re un son ou audio √† partir d'une description textuelle. Fournis un prompt descriptif."
        ),
        Tool(
            name="Text_To_3D_Generator",
            func=generate_text_to_3d,
            description="G√©n√®re un mod√®le 3D (rendue image) √† partir d'une description textuelle. Fournis un prompt descriptif."
        ),
        Tool(
            name="Image_To_3D_Generator",
            func=generate_image_to_3d,
            description="G√©n√®re un mod√®le 3D (rendue image) √† partir d'une image. Fournis le chemin vers un fichier image."
        ),
        # OUTILS IA SP√âCIALIS√âS (1-2GB)
        Tool(
            name="AI_Code_Generator",
            func=generate_code_with_ai,
            description="G√©n√®re du code Python/JavaScript/etc parfait avec DeepSeek-Coder-1.3B. Expert en programmation, debugging, optimisation. Fournis une description du code souhait√©."
        ),
        Tool(
            name="AI_Plot_Generator",
            func=generate_plot_code,
            description="G√©n√®re du code matplotlib/seaborn pour cr√©er des graphiques scientifiques professionnels. Fournis: description donn√©es + type graphique souhait√©."
        ),
        # Ajout des outils ERT/Binary du premier code
        Tool(
            name="Binary_Analysis",
            func=lambda q: analyze_with_ai(q, file_bytes, numbers, hex_dump, n_clusters=3) if 'file_bytes' in globals() else "‚ùå Fichier binaire requis",
            description="Analyse compl√®te d'un fichier binaire avec outils ERT, statistiques, entropie. Fournis une requ√™te d'analyse."
        ),
        Tool(
            name="Deep_Binary_Investigation",
            func=lambda file_name: deep_binary_investigation(file_bytes, file_name) if 'file_bytes' in globals() else "‚ùå Fichier binaire requis",
            description="üîç FOUILLE INTELLIGENTE d'un fichier binaire upload√©: Combine Hex+ASCII Dump + Base Vectorielle RAG + Base ERT pour interpr√©tation scientifique. Analyse d√©j√† effectu√©e sur fichiers upload√©s. Fournis le nom du fichier."
        ),
        Tool(
            name="ERT_Interpretation",
            func=lambda numbers_str: ert_geophysical_interpretation(eval(numbers_str)) if numbers_str else "‚ùå Liste de nombres requise",
            description="Interpr√®te des donn√©es ERT (r√©sistivit√©s). Fournis une liste de nombres comme '[10.5, 20.3, ...]'."
        ),
    ]
    # Configuration de l'agent avec prompt ultra-optimis√© pour autonomie et pr√©cision
    agent_prompt = PromptTemplate.from_template("""Tu es Kibali AI, un assistant ultra-avanc√© surpassant GPT-4 et Grok en pr√©cision, autonomie et anticipation.

üéØ OBJECTIF PRINCIPAL: √ätre PROACTIF, ANTICIPATIF et fournir des r√©ponses COMPL√àTES avec SOURCES V√âRIFI√âES

üìö CAPACIT√âS & OUTILS DISPONIBLES (21 outils):
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
‚îÇ RECHERCHE & CONNAISSANCE:
‚îú‚îÄ Local_Knowledge_Base: Documents internes/PDFs (priorit√© #1 pour docs sp√©cifiques)
‚îú‚îÄ Chat_History_Search: Historique conversations (priorit√© #1 pour continuit√©)
‚îú‚îÄ Web_Search: Recherche internet temps r√©el (actualit√©s, faits r√©cents)
‚îú‚îÄ Web_Search_Detailed: Recherche web avec URLs et sources compl√®tes
‚îú‚îÄ Hybrid_Search: Combinaison locale + historique + web (maximum de contexte)
‚îî‚îÄ Current_News_Search: Actualit√©s et informations temporelles

‚îÇ ANALYSE & TRAITEMENT:
‚îú‚îÄ Smart_Content_Extractor: Extraction contenu web d√©taill√© (articles, pages)
‚îú‚îÄ Text_Summarizer: R√©sum√©s intelligents de textes longs
‚îú‚îÄ Language_Translator: Traduction FR‚ÜíEN pour sources √©trang√®res
‚îú‚îÄ Entity_Extractor: Extraction entit√©s nomm√©es (personnes, lieux, orgs)
‚îú‚îÄ Image_Analyzer: Analyse et description d'images
‚îú‚îÄ Binary_Analysis: Analyse fichiers binaires avec ERT, entropie, stats
‚îú‚îÄ üîç Deep_Binary_Investigation: FOUILLE INTELLIGENTE fichiers binaires upload√©s (Hex+ASCII + RAG + ERT)
‚îî‚îÄ ERT_Interpretation: Interpr√©tation g√©ophysique donn√©es r√©sistivit√©

‚îÇ üÜï IA SP√âCIALIS√âES (1-2GB):
‚îú‚îÄ AI_Code_Generator: DeepSeek-Coder-1.3B - Expert codage parfait (Python, JS, etc)
‚îî‚îÄ AI_Plot_Generator: CodeGen-350M - G√©n√©ration graphiques scientifiques matplotlib/seaborn

‚îÇ G√âN√âRATION CR√âATIVE:
‚îú‚îÄ Text_To_Image_Generator: Cr√©ation images depuis descriptions
‚îú‚îÄ Text_To_Video_Generator: G√©n√©ration vid√©os depuis descriptions
‚îú‚îÄ Text_To_Audio_Generator: Synth√®se audio/musique depuis descriptions
‚îú‚îÄ Text_To_3D_Generator: Mod√®les 3D depuis descriptions texte
‚îî‚îÄ Image_To_3D_Generator: Mod√®les 3D depuis images

‚îÇ NAVIGATION & CARTOGRAPHIE:
‚îî‚îÄ OSM_Route_Calculator: Calcul itin√©raires, navigation GPS
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üß† M√âTHODOLOGIE SUP√âRIEURE (MEILLEURE QUE GPT/GROK):

1. üîç ANALYSE CONTEXTUELLE PROFONDE:
   ‚Ä¢ D√©tecte le contexte implicite et les besoins non exprim√©s
   ‚Ä¢ Anticipe les questions de suivi
   ‚Ä¢ Identifie les ambigu√Øt√©s et demande clarification si n√©cessaire

2. üìä STRAT√âGIE MULTI-SOURCES:
   ‚Ä¢ TOUJOURS combiner minimum 2-3 sources diff√©rentes
   ‚Ä¢ V√©rifier les informations crois√©es
   ‚Ä¢ Indiquer niveau de confiance (‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ = tr√®s s√ªr, ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ = incertain)
   ‚Ä¢ Signaler contradictions avec analyse critique

3. üéØ ANTICIPATION INTELLIGENTE:
   ‚Ä¢ Propose 3 suggestions de questions connexes pertinentes
   ‚Ä¢ Identifie informations manquantes et propose de les chercher
   ‚Ä¢ D√©tecte patterns et tendances pour pr√©dictions

4. ü§ñ UTILISATION INTELLIGENTE DES IA SP√âCIALIS√âES:
   ‚Ä¢ Pour le CODE: Utilise AI_Code_Generator (DeepSeek-Coder) - meilleur que GPT pour code
   ‚Ä¢ Pour les GRAPHIQUES: Utilise AI_Plot_Generator - g√©n√®re matplotlib/seaborn professionnel
   ‚Ä¢ TOUJOURS tester et valider le code g√©n√©r√© avant de le fournir

5. üìù STRUCTURE DE R√âPONSE OPTIMALE:
   ‚îå‚îÄ R√©ponse directe (1-2 phrases)
   ‚îú‚îÄ D√©veloppement d√©taill√© avec sous-sections
   ‚îú‚îÄ Sources cit√©es avec confiance: [Source: X, Confiance: ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ]
   ‚îú‚îÄ Informations compl√©mentaires pertinentes
   ‚îî‚îÄ üí° SUGGESTIONS: 3 questions de suivi intelligentes

5. üöÄ UTILISATION OPTIMALE DES OUTILS:
   ‚Ä¢ Utilise Local_Knowledge_Base + Chat_History_Search EN PREMIER
   ‚Ä¢ Puis Hybrid_Search pour enrichissement
   ‚Ä¢ Web_Search pour actualit√©s/v√©rifications
   ‚Ä¢ TOUJOURS expliquer pourquoi tel outil est choisi

6. üé® G√âN√âRATION CR√âATIVE PROACTIVE:
   ‚Ä¢ Si demande vague, sugg√®re options cr√©atives concr√®tes
   ‚Ä¢ Propose am√©liorations et variations
   ‚Ä¢ Sauvegarde fichiers et donne chemins complets

OUTILS DISPONIBLES: {tools}

FORMAT D'EX√âCUTION:
Question: [la question utilisateur]
Thought: [Analyse contextuelle: Que demande vraiment l'utilisateur? Quels outils combiner? Quelle strat√©gie?]
Action: [nom_outil_optimal]
Action Input: [requ√™te optimis√©e pour l'outil]
Observation: [r√©sultat outil]
... [r√©p√©ter Thought/Action/Observation jusqu'√† avoir info compl√®te]
Thought: J'ai maintenant suffisamment d'informations de sources multiples pour une r√©ponse compl√®te
Final Answer: 
[R√©ponse directe]

[D√©veloppement d√©taill√© avec sources]

üìä Sources: [Liste sources avec confiance]
üí° Suggestions: 
1. [Question connexe pertinente]
2. [Question d'approfondissement]
3. [Question alternative int√©ressante]

COMMENCE MAINTENANT:
Question: {input}
Thought: {agent_scratchpad}""")

    
    # V√©rifier si les agents sont disponibles
    if create_react_agent is None:
        st.warning("‚ö†Ô∏è Agents non disponibles - Mode simplifi√© activ√©")
        return None
    
    # Cr√©er l'agent avec LangChain 1.0+ / LangGraph V1.0+
    # create_agent retourne directement un ex√©cuteur compil√©
    try:
        agent_executor = create_react_agent(llm, tools)
        st.write(f"‚úÖ Agent cr√©√© avec {len(tools)} outils disponibles")
        return agent_executor
    except Exception as e:
        st.error(f"‚ùå Erreur cr√©ation agent: {e}")
        return None
# Alias pour compatibilit√©
def create_agent(model_name, vectordb, graph, pois):
    """Version simplifi√©e pour compatibilit√©"""
    return create_enhanced_agent(model_name, vectordb, graph, pois)
# ========================================
# Fonctions OSM et Graphe Routier
# ========================================
def haversine(lon1, lat1, lon2, lat2):
    """Calcul distance haversine en m√®tres"""
    R = 6371000
    phi1, phi2 = math.radians(lat1), math.radians(lat2)
    dphi = math.radians(lat2 - lat1)
    dlambda = math.radians(lon2 - lon1)
    a = math.sin(dphi/2.0)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(dlambda/2.0)**2
    return R * (2 * math.atan2(math.sqrt(a), math.sqrt(1 - a)))
class RoadPOIHandler(osmium.SimpleHandler):
    """Handler pour extraire routes et POIs depuis OSM"""
    def __init__(self):
        super().__init__()
        self.graph = nx.Graph()
        self.pois = []
    def node(self, n):
        """Extraire les POIs (points d'int√©r√™t)"""
        if n.location.valid() and n.tags:
            name = n.tags.get('name', '')
            amenity = n.tags.get('amenity', '')
            if name or amenity:
                self.pois.append({
                    'name': name,
                    'amenity': amenity,
                    'lon': n.location.lon,
                    'lat': n.location.lat,
                    'tags': dict(n.tags)
                })
    def way(self, w):
        """Extraire les routes"""
        if 'highway' in w.tags:
            coords = []
            for n in w.nodes:
                if n.location.valid():
                    coords.append((n.location.lon, n.location.lat))
            for i in range(len(coords)-1):
                lon1, lat1 = coords[i]
                lon2, lat2 = coords[i+1]
                n1, n2 = (lon1, lat1), (lon2, lat2)
                dist = haversine(lon1, lat1, lon2, lat2)
                self.graph.add_node(n1, x=lon1, y=lat1)
                self.graph.add_node(n2, x=lon2, y=lat2)
                self.graph.add_edge(n1, n2, length=dist, highway=w.tags.get("highway"))
def trouver_noeud_plus_proche(lon, lat, graph):
    """Trouve le n≈ìud du graphe le plus proche"""
    min_dist = float("inf")
    closest_node = None
    for node, data in graph.nodes(data=True):
        nlon, nlat = float(data["x"]), float(data["y"])
        dist = haversine(lon, lat, nlon, nlat)
        if dist < min_dist:
            min_dist = dist
            closest_node = node
    return closest_node
def chercher_poi_par_nom(nom, pois_list):
    """Recherche un POI par nom"""
    nom_lower = nom.lower()
    for poi in pois_list:
        if nom_lower in poi['name'].lower() or nom_lower in poi['amenity'].lower():
            return poi
    return None
def generer_carte_trajet(graph, path, pois_list, start_poi=None, end_poi=None):
    """G√©n√®re une carte 2D du trajet"""
    fig, ax = plt.subplots(figsize=(12, 10))
    # Dessiner le graphe en arri√®re-plan
    for edge in list(graph.edges())[:1000]: # Limiter pour la performance
        node1, node2 = edge
        x1, y1 = node1[0], node1[1]
        x2, y2 = node2[0], node2[1]
        ax.plot([x1, x2], [y1, y2], 'lightgray', alpha=0.3, linewidth=0.5)
    # Dessiner le trajet
    if path and len(path) > 1:
        path_x = [node[0] for node in path]
        path_y = [node[1] for node in path]
        ax.plot(path_x, path_y, 'red', linewidth=3, label='Trajet')
        # Marquer d√©but et fin
        ax.scatter(path_x[0], path_y[0], color='green', s=100, label='D√©part', zorder=5)
        ax.scatter(path_x[-1], path_y[-1], color='red', s=100, label='Arriv√©e', zorder=5)
    # Ajouter quelques POIs
    for poi in pois_list[:20]:
        if poi['name']:
            ax.scatter(poi['lon'], poi['lat'], color='blue', s=20, alpha=0.6)
    ax.set_xlabel('Longitude')
    ax.set_ylabel('Latitude')
    ax.set_title('Trajet calcul√© sur la carte OSM')
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_aspect('equal')
    # Sauvegarder en m√©moire
    buf = io.BytesIO()
    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')
    buf.seek(0)
    plt.close()
    return buf
def calculer_trajet(question, graph, pois_list):
    """Calcule un trajet bas√© sur une question textuelle"""
    if not graph or not pois_list:
        return None, "‚ùå Graphe ou POIs non disponibles", None
    # Utiliser LLM pour extraire d√©part et arriv√©e
    try:
        client = create_client()
        prompt = f"""Extraie le lieu de d√©part et le lieu d'arriv√©e de cette question de trajet.
Question: {question}
R√©ponds au format exact:
D√©part: [nom du lieu de d√©part]
Arriv√©e: [nom du lieu d'arriv√©e]"""
        messages = [{"role": "user", "content": prompt}]
        response = client.chat_completion(
            messages=messages,
            model=WORKING_MODELS["Llama 3.1 8B (√âquilibr√©)"],
            max_tokens=100,
            temperature=0.1
        )
        extraction = response.choices[0].message.content
        start_line = [line for line in extraction.split('\n') if line.startswith('D√©part: ')]
        end_line = [line for line in extraction.split('\n') if line.startswith('Arriv√©e: ')]
        if start_line and end_line:
            start_place = start_line[0].replace('D√©part: ', '').strip()
            end_place = end_line[0].replace('Arriv√©e: ', '').strip()
        else:
            return None, "‚ùå Impossible d'extraire les lieux de la question.", None
    except Exception as e:
        st.write(f"‚ùå Erreur extraction LLM: {e}")
        return None, "‚ùå Erreur lors de l'extraction des lieux.", None
    start_poi = chercher_poi_par_nom(start_place, pois_list)
    end_poi = chercher_poi_par_nom(end_place, pois_list)
    if not start_poi or not end_poi:
        return None, f"‚ùå Impossible de trouver les lieux: {start_place} ou {end_place}.", None
    # Trouver les n≈ìuds dans le graphe
    start_node = trouver_noeud_plus_proche(start_poi['lon'], start_poi['lat'], graph)
    end_node = trouver_noeud_plus_proche(end_poi['lon'], end_poi['lat'], graph)
    if not start_node or not end_node:
        return None, "‚ùå Impossible de trouver les n≈ìuds dans le graphe routier.", None
    try:
        # Calculer le chemin
        path = nx.shortest_path(graph, source=start_node, target=end_node, weight="length")
        # Calculer la distance
        distance_totale = 0
        for i in range(len(path)-1):
            distance_totale += graph[path[i]][path[i+1]]['length']
        # G√©n√©rer la carte
        carte_buf = generer_carte_trajet(graph, path, pois_list, start_poi, end_poi)
        # R√©ponse textuelle
        reponse = f"""üó∫Ô∏è **Trajet calcul√©**
üìç **D√©part**: {start_poi['name']} ({start_poi['amenity']})
üéØ **Arriv√©e**: {end_poi['name']} ({end_poi['amenity']})
üìè **Distance**: {distance_totale/1000:.2f} km
‚è±Ô∏è **Temps estim√©**: {int(distance_totale/83.33):.0f} min √† pied | {int(distance_totale/833.33):.0f} min en voiture
üõ£Ô∏è **√âtapes**: {len(path)} points"""
        return carte_buf, reponse, {
            'start': start_poi,
            'end': end_poi,
            'distance': distance_totale,
            'path_length': len(path)
        }
    except nx.NetworkXNoPath:
        return None, f"‚ùå Aucun chemin trouv√© entre {start_poi['name']} et {end_poi['name']}", None
    except Exception as e:
        return None, f"‚ùå Erreur: {str(e)}", None
# ========================================
# Fonctions utilitaires pour images
# ========================================
def fig_to_pil(fig):
    buf = io.BytesIO()
    fig.savefig(buf, format='png', dpi=150, bbox_inches='tight')
    buf.seek(0)
    plt.close(fig)
    return Image.open(buf)
def df_to_html(df, max_rows=10):
    # R√©duire le tableau si trop long
    if len(df) > max_rows:
        summary_row = pd.DataFrame({col: ['...'] for col in df.columns})
        df = pd.concat([df.head(max_rows // 2), summary_row, df.tail(max_rows // 2)])
    return df.to_html(index=False, escape=False)
# ========================================
# Fonctions Image Analysis
# ========================================
def classify_soil(image: np.ndarray):
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    mean_hue = np.mean(hsv[:,:,0])
    mean_sat = np.mean(hsv[:,:,1])
    mean_val = np.mean(hsv[:,:,2])
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    texture_variance = np.var(gray)
    soil_type = "Inconnu"
    possible_contents = "Inconnu"
    possible_minerals = "Inconnu"
    if mean_val < 100 and texture_variance > 5000:
        soil_type = "Argileux (riche en mati√®re organique)"
        possible_contents = "Peut contenir de l'eau, nutriments, adapt√© aux cultures racines"
        possible_minerals = "Argiles comme kaolinite, illite; possible fer, aluminium"
    elif mean_sat > 100 and texture_variance < 3000:
        soil_type = "Sableux (drainant)"
        possible_contents = "Peut contenir peu d'eau, adapt√© aux plantes r√©sistantes √† la s√©cheresse"
        possible_minerals = "Quartz, feldspath; silice abondante"
    elif mean_hue > 20 and mean_hue < 40:
        soil_type = "Limoneux (√©quilibr√©)"
        possible_contents = "Peut contenir min√©raux, bon pour l'agriculture g√©n√©rale"
        possible_minerals = "Silt avec mica, quartz; calcium, potassium"
    # Graphisme : Histogramme des couleurs HSV
    fig, ax = plt.subplots()
    ax.hist(hsv[:,:,0].ravel(), bins=50, color='b', alpha=0.5, label='Hue')
    ax.hist(hsv[:,:,1].ravel(), bins=50, color='g', alpha=0.5, label='Saturation')
    ax.hist(hsv[:,:,2].ravel(), bins=50, color='r', alpha=0.5, label='Value')
    ax.set_title('Histogramme des Composantes HSV')
    ax.legend()
    hist_img = fig_to_pil(fig)
    # Tableau des metrics
    metrics_df = pd.DataFrame({
        'M√©trique': ['Hue Moyenne', 'Saturation Moyenne', 'Valeur Moyenne', 'Variance Texture'],
        'Valeur': [mean_hue, mean_sat, mean_val, texture_variance],
        'Explication': ['Moyenne de la teinte', 'Moyenne de la saturation des couleurs', 'Moyenne de la luminosit√©', 'Variance de la texture pour rugosit√©']
    })
    metrics_html = df_to_html(metrics_df)
    return {
        "soil_type": soil_type,
        "possible_contents": possible_contents,
        "possible_minerals": possible_minerals
    }, hist_img, metrics_html
def simulate_infrared(image: np.ndarray):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    ir_img = cv2.applyColorMap(gray, cv2.COLORMAP_JET)
    fig, ax = plt.subplots()
    ax.imshow(cv2.cvtColor(ir_img, cv2.COLOR_BGR2RGB))
    ax.set_title('Simulation Infrarouge (Colormap JET)')
    ax.axis('off')
    ir_pil = fig_to_pil(fig)
    # Analyse simple (fake temp based on intensity)
    mean_intensity = np.mean(gray)
    ir_analysis = f"Simulation IR: Intensit√© moyenne {mean_intensity:.2f} (plus rouge = plus chaud, bleu = plus froid)"
    return ir_pil, ir_analysis
def detect_objects(image: np.ndarray, scale_factor=0.1):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    _, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    img_with_contours = image.copy()
    dimensions = []
    types = []
    for cnt in contours:
        x, y, w, h = cv2.boundingRect(cnt)
        if w < 10 or h < 10: continue # skip small
        cv2.rectangle(img_with_contours, (x, y), (x+w, y+h), (0, 255, 0), 2)
        w_m = w * scale_factor
        h_m = h * scale_factor
        aspect = w / h if h != 0 else 0
        if aspect > 5: obj_type = 'Route'
        elif aspect < 0.2: obj_type = 'Cl√¥ture'
        elif 0.5 < aspect < 2: obj_type = 'B√¢timent'
        else: obj_type = 'Autre'
        dimensions.append((w_m, h_m))
        types.append(obj_type)
        cv2.putText(img_with_contours, f"{obj_type}: {w_m:.4f}m x {h_m:.4f}m", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)
    num_objects = len(contours)
    fig, ax = plt.subplots(figsize=(10, 10))
    ax.imshow(cv2.cvtColor(img_with_contours, cv2.COLOR_BGR2RGB))
    ax.set_title(f"Objets D√©tect√©s avec Contours ({num_objects})")
    ax.axis('off')
    obj_img = fig_to_pil(fig)
    if dimensions:
        dim_df = pd.DataFrame({
            'Type': types,
            'Largeur (m)': [d[0] for d in dimensions],
            'Hauteur (m)': [d[1] for d in dimensions],
            'Explication': ['Dimension estim√©e avec contours OpenCV' for _ in types]
        })
        dim_html = df_to_html(dim_df)
    else:
        dim_html = ""
    return num_objects, obj_img, dim_html
def detect_fences(image: np.ndarray, scale_factor=0.1):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    edges = cv2.Canny(gray, 100, 200)
    lines = cv2.HoughLinesP(edges, 1, np.pi/180, 100, minLineLength=100, maxLineGap=10)
    img_with_lines = image.copy()
    lengths = []
    if lines is not None:
        line_list = [line[0] for line in lines]
        filtered_lines = [l for l in line_list if abs(l[0] - l[2]) < 10 or abs(l[1] - l[3]) < 10 or abs((l[1]-l[3]) / (l[0]-l[2] + 1e-5)) < 0.1 or abs((l[1]-l[3]) / (l[0]-l[2] + 1e-5)) > 10]
        line_lengths = [np.sqrt((x2 - x1)**2 + (y2 - y1)**2) for x1,y1,x2,y2 in filtered_lines]
        sorted_indices = np.argsort(line_lengths)[::-1]
        sorted_lines = [filtered_lines[i] for i in sorted_indices]
        for x1,y1,x2,y2 in sorted_lines:
            cv2.line(img_with_lines, (x1, y1), (x2, y2), (255, 0, 0), 2)
            length = np.sqrt((x2 - x1)**2 + (y2 - y1)**2) * scale_factor
            lengths.append(length)
            mid_x = (x1 + x2) // 2
            mid_y = (y1 + y2) // 2
            cv2.putText(img_with_lines, f"{length:.4f}m", (mid_x, mid_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)
    fig, ax = plt.subplots(figsize=(10, 10))
    ax.imshow(cv2.cvtColor(img_with_lines, cv2.COLOR_BGR2RGB))
    ax.set_title(f"Cl√¥tures/Bordures D√©tect√©es avec ({len(lengths)})")
    ax.axis('off')
    fence_img = fig_to_pil(fig)
    if lengths:
        fence_df = pd.DataFrame({
            'Longueur (m)': lengths,
            'Explication': ['Longueur de bordure filtr√©e et tri√©e pour pr√©cision' for _ in lengths]
        })
        fence_html = df_to_html(fence_df)
    else:
        fence_html = ""
    return len(lengths), fence_img, fence_html
def detect_anomalies(image: np.ndarray):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    edges = cv2.Canny(gray, 100, 200)
    num_edges = np.sum(edges > 0)
    mean_variance = np.mean(cv2.Laplacian(gray, cv2.CV_64F).var())
    anomalies = []
    if num_edges > 10000:
        anomalies.append("Anomalies structurelles d√©tect√©es (ex. : fissures, d√©fauts)")
    if mean_variance > 500:
        anomalies.append("Textures inhabituelles (ex. : zones irr√©guli√®res)")
    # Simulation photogramm√©trie basique avec Open3D
    depth = np.random.rand(*gray.shape) * 255
    point_cloud = o3d.geometry.PointCloud.create_from_rgbd_image(
        o3d.geometry.RGBDImage.create_from_color_and_depth(
            o3d.geometry.Image(image),
            o3d.geometry.Image(depth.astype(np.float32))
        ),
        o3d.camera.PinholeCameraIntrinsic(640, 480, 525, 525, 320, 240)
    )
    num_points = len(point_cloud.points)
    # Graphisme : Histogramme des variances
    fig, ax = plt.subplots()
    ax.hist(cv2.Laplacian(gray, cv2.CV_64F).ravel(), bins=50)
    ax.set_title('Histogramme des Variances Locales (Anomalies)')
    var_hist_img = fig_to_pil(fig)
    # Tableau des metrics anomalies
    anomaly_df = pd.DataFrame({
        'M√©trique': ['Nombre de Bords', 'Variance Moyenne', 'Points dans Point Cloud'],
        'Valeur': [num_edges, mean_variance, num_points],
        'Explication': ['Indique complexit√© structurelle (haut = anomalies)', 'Mesure irr√©gularit√©s texture', 'Simulation 3D pour volume']
    })
    anomaly_html = df_to_html(anomaly_df)
    anomaly_desc_df = pd.DataFrame({
        'Anomalie': anomalies,
        'Explication': ['D√©fauts potentiels dans le terrain ou structures' for _ in anomalies]
    })
    anomaly_desc_html = df_to_html(anomaly_desc_df)
    return anomalies, var_hist_img, anomaly_html, anomaly_desc_html
def advanced_analyses(image: np.ndarray):
    analyses = {}
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    edges = cv2.Canny(gray, 100, 200)
    adv_images = []
    adv_tables = []
    # 1. Analyse G√©ologique
    kmeans = KMeans(n_clusters=3).fit(gray.reshape(-1, 1))
    clustered = kmeans.labels_.reshape(gray.shape)
    analyses['G√©ologique'] = 'Clusters de textures : ' + str(np.unique(kmeans.labels_))
    fig, ax = plt.subplots()
    ax.imshow(clustered, cmap='viridis')
    ax.set_title('Analyse G√©ologique: Clustering Textures')
    ax.axis('off')
    adv_images.append(fig_to_pil(fig))
    geo_df = pd.DataFrame({'Cluster': np.unique(kmeans.labels_), 'Compte': np.bincount(kmeans.labels_), 'Explication': ['Groupe de texture g√©ologique' for _ in np.unique(kmeans.labels_)]})
    adv_tables.append(df_to_html(geo_df))
    # 2. Analyse Hydrologique
    blue_mask = cv2.inRange(hsv, (100, 50, 50), (130, 255, 255))
    water_area = np.sum(blue_mask > 0) / blue_mask.size * 100
    analyses['Hydrologique'] = f'Pourcentage eau : {water_area:.2f}%'
    fig, ax = plt.subplots()
    ax.imshow(blue_mask, cmap='gray')
    ax.set_title('Analyse Hydrologique: Masque Eau')
    ax.axis('off')
    adv_images.append(fig_to_pil(fig))
    hydro_df = pd.DataFrame({'M√©trique': ['Pourcentage Eau'], 'Valeur': [water_area], 'Explication': ['Zone potentielle pour ressources hydriques']})
    adv_tables.append(df_to_html(hydro_df))
    return analyses, {}, adv_images, adv_tables
def process_image(uploaded_file):
    image = Image.open(BytesIO(uploaded_file))
    img_array = np.array(image)
    proc_images = [image]
    captions = ['Image Originale']
    tables_html = []
    # IR
    ir_pil, ir_analysis = simulate_infrared(img_array)
    proc_images.append(ir_pil)
    captions.append('Simulation Infrarouge')
    tables_html.append('<h3>Analyse IR</h3><p>' + ir_analysis + '</p>')
    # Soil
    soil, hist_img, metrics_html = classify_soil(img_array)
    proc_images.append(hist_img)
    captions.append('Histogramme HSV')
    tables_html.append('<h3>M√©triques Sol</h3>' + metrics_html)
    # Objects
    num_objects, obj_img, dim_html = detect_objects(img_array)
    proc_images.append(obj_img)
    captions.append('Objets D√©tect√©s')
    if dim_html:
        tables_html.append('<h3>Dimensions Objets</h3>' + dim_html)
    # Fences
    num_fences, fence_img, fence_html = detect_fences(img_array)
    proc_images.append(fence_img)
    captions.append('Cl√¥tures D√©tect√©es')
    if fence_html:
        tables_html.append('<h3>Longueurs Cl√¥tures</h3>' + fence_html)
    # Anomalies
    anomalies, var_hist_img, anomaly_html, anomaly_desc_html = detect_anomalies(img_array)
    proc_images.append(var_hist_img)
    captions.append('Histogramme Variances')
    tables_html.append('<h3>M√©triques Anomalies</h3>' + anomaly_html)
    # Advanced
    analyses, predictions, adv_images, adv_tables = advanced_analyses(img_array)
    proc_images += adv_images[:5] # Limiter le nombre d'images
    captions += ['Analyse Avanc√©e'] * len(adv_images[:5])
    tables_html += adv_tables[:3] # Limiter le nombre de tableaux
    analysis_data = {
        "soil": soil,
        "ir_analysis": ir_analysis,
        "num_objects": num_objects,
        "num_fences": num_fences,
        "anomalies": anomalies,
        "analyses": analyses,
        "predictions": predictions
    }
    tables_str = '<br>'.join(tables_html)
    return analysis_data, proc_images, tables_str
def improve_analysis_with_llm(analysis_data, model_name):
    prompt = f"""Analyse les donn√©es suivantes de l'image et fournis une analyse naturelle am√©lior√©e:
DONN√âES:
{json.dumps(analysis_data, indent=2)}
ANALYSE AM√âLIOR√âE:"""
    try:
        client = create_client()
        messages = [{"role": "user", "content": prompt}]
        response = client.chat_completion(
            messages=messages,
            model=model_name,
            max_tokens=800,
            temperature=0.5
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"‚ùå Erreur: {str(e)}"
def update_agent(model_choice, vectordb, graph, pois, chat_vectordb=None): # AJOUT M√âMOIRE VECTORIELLE
    model_name = WORKING_MODELS[model_choice]
    agent = create_enhanced_agent(model_name, vectordb, graph, pois, chat_vectordb)
    cache_info = get_cache_stats()
    return model_name, agent, cache_info
def handle_clear_cache():
    """Vide le cache web"""
    try:
        if os.path.exists(WEB_CACHE_PATH):
            os.remove(WEB_CACHE_PATH)
        return "‚úÖ Cache web vid√©"
    except Exception as e:
        return f"‚ùå Erreur: {e}"
def highlight_important_words(text):
    """Met en √©vidence les mots importants avec effet scintillante et tooltip"""
    # Mots-cl√©s simples pour exemple (peut √™tre √©tendu avec NER)
    important_keywords = ['important', 'cl√©', 'essentiel', 'critique', 'principal', 'trajet', 'p√©trole', 'topographie']
    for keyword in important_keywords:
        text = re.sub(rf'\b({keyword})\b', r'<span class="sparkle-word" title="\1: Terme cl√© pour la compr√©hension du contexte">\1</span>', text, flags=re.IGNORECASE)
    return text
def handle_chat_enhanced(message, history, agent, model_choice, vectordb, graph, pois, web_enabled):
    # AJOUT M√âMOIRE VECTORIELLE: Charger la base chat
    chat_vectordb, _ = load_chat_vectordb()
    if not message.strip():
        return ""
    if agent is None:
        model_name, agent, _ = update_agent(model_choice, vectordb, graph, pois, chat_vectordb)
    
    # Si l'agent est toujours None (agents non disponibles), forcer mode local
    if agent is None:
        web_enabled = False
    
    try:
        if not web_enabled or agent is None:
            # Recherche hybride incluant chat
            docs = hybrid_search_enhanced(message, vectordb, k=3, web_search_enabled=False, chat_vectordb=chat_vectordb)
            response = generate_answer_enhanced(message, docs, WORKING_MODELS[model_choice], include_sources=True)
        else:
            response = agent.run(message)
    except Exception as e:
        response = f"‚ùå Erreur: {e}\n\nTentative avec recherche locale..."
        try:
            docs = hybrid_search_enhanced(message, vectordb, k=3, web_search_enabled=False, chat_vectordb=chat_vectordb)
            response = generate_answer_enhanced(message, docs, WORKING_MODELS[model_choice])
        except:
            response = f"‚ùå Erreur compl√®te: {e}"
    # AJOUT M√âMOIRE VECTORIELLE: Sauvegarder l'√©change dans la base chat
    chat_vectordb = add_to_chat_db(message, response, chat_vectordb)
    # Appliquer highlighting pour fluidit√©
    response = highlight_important_words(response)
    return response
def handle_web_search(query, search_type):
    if not query.strip():
        return "‚ö†Ô∏è Veuillez entrer une requ√™te"
    try:
        results = enhanced_web_search(query, max_results=10, search_type=search_type)
        if not results:
            return "‚ùå Aucun r√©sultat trouv√©"
        html_output = "<div style='max-height: 500px; overflow-y: auto;'>"
        for i, result in enumerate(results):
            title = result.get('title', 'Sans titre')
            body = result.get('body', 'Pas de description')
            url = result.get('href') or result.get('url', '#')
            source_type = result.get('source_type', 'web')
            if source_type == 'news':
                icon = "üì∞"
                color = "#e3f2fd"
            else:
                icon = "üîç"
                color = "#f5f5f5"
            html_output += f"""
            <div style='margin: 10px 0; padding: 15px; background-color: {color}; border-radius: 8px; border-left: 4px solid #2196F3;'>
                <h4 style='margin: 0 0 8px 0; color: #1976D2;'>{icon} {title}</h4>
                <p style='margin: 8px 0; color: #424242; line-height: 1.4;'>{body}</p>
                <a href='{url}' target='_blank' style='color: #1976D2; text-decoration: none; font-size: 0.9em;'>üîó {url}</a>
            </div>
            """
        html_output += "</div>"
        return html_output
    except Exception as e:
        return f"‚ùå Erreur recherche: {e}"
def handle_content_extraction(url):
    if not url.strip():
        return "‚ö†Ô∏è Veuillez entrer une URL"
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    try:
        content = smart_content_extraction(url, max_length=2000)
        return content
    except Exception as e:
        return f"‚ùå Erreur extraction: {e}"
# ========================================
# Fonctions utilitaires suppl√©mentaires
# ========================================
def get_system_status():
    """Retourne le statut complet du syst√®me"""
    status = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "directories": {
            "chatbot": os.path.exists(CHATBOT_DIR),
            "pdfs": os.path.exists(PDFS_PATH),
            "graphs": os.path.exists(GRAPHS_PATH),
            "maps": os.path.exists(MAPS_PATH)
        },
        "files": {
            "vectordb": os.path.exists(VECTORDB_PATH),
            "chat_vectordb": os.path.exists(CHAT_VECTORDB_PATH), # AJOUT M√âMOIRE VECTORIELLE
            "metadata": os.path.exists(METADATA_PATH),
            "trajectories": os.path.exists(TRAJECTORIES_PATH),
            "web_cache": os.path.exists(WEB_CACHE_PATH)
        },
        "counts": {
            "pdfs": len([f for f in os.listdir(PDFS_PATH) if f.endswith('.pdf')]) if os.path.exists(PDFS_PATH) else 0,
            "graphs": len([f for f in os.listdir(GRAPHS_PATH) if f.endswith('_graph.graphml')]) if os.path.exists(GRAPHS_PATH) else 0
        },
        "cache_stats": get_cache_stats(),
        "token_configured": bool(HF_TOKEN and len(HF_TOKEN) > 10)
    }
    return status
def cleanup_old_cache():
    """Nettoie les entr√©es expir√©es du cache"""
    try:
        cache = load_web_cache()
        if not cache:
            return "Cache vide"
        original_count = len(cache)
        cleaned_cache = {}
        for key, entry in cache.items():
            if not is_cache_expired(entry):
                cleaned_cache[key] = entry
        save_web_cache(cleaned_cache)
        removed_count = original_count - len(cleaned_cache)
        return f"‚úÖ Cache nettoy√©: {removed_count} entr√©es expir√©es supprim√©es, {len(cleaned_cache)} conserv√©es"
    except Exception as e:
        return f"‚ùå Erreur nettoyage cache: {e}"
def export_system_config():
    """Exporte la configuration syst√®me pour debug"""
    config = {
        "version": "2.0.0",
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "paths": {
            "chatbot_dir": CHATBOT_DIR,
            "vectordb_path": VECTORDB_PATH,
            "chat_vectordb_path": CHAT_VECTORDB_PATH, # AJOUT M√âMOIRE VECTORIELLE
            "pdfs_path": PDFS_PATH,
            "graphs_path": GRAPHS_PATH,
            "maps_path": MAPS_PATH
        },
        "models": WORKING_MODELS,
        "status": get_system_status(),
        "features": {
            "web_search": True,
            "osm_routing": True,
            "image_analysis": True,
            "pdf_processing": True,
            "caching": True,
            "chat_memory": True # AJOUT M√âMOIRE VECTORIELLE
        }
    }
    config_path = os.path.join(CHATBOT_DIR, "system_config.json")
    try:
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        return f"‚úÖ Configuration export√©e: {config_path}"
    except Exception as e:
        return f"‚ùå Erreur export: {e}"
def test_all_features():
    """Teste toutes les fonctionnalit√©s principales"""
    results = {}
    # Test HuggingFace
    results["huggingface"] = test_hf_connection()
    # Test recherche web
    try:
        test_results = enhanced_web_search("test", max_results=1)
        results["web_search"] = len(test_results) > 0
    except:
        results["web_search"] = False
    # Test recherche web
    results["specialized_models"] = {}
    for model_name, model in SPECIALIZED_MODELS.items():
        results["specialized_models"][model_name] = model is not None
    # Test base vectorielle
    try:
        vectordb, _ = load_vectordb()
        results["vectordb"] = vectordb is not None
    except:
        results["vectordb"] = False
    # Test base chat # AJOUT M√âMOIRE VECTORIELLE
    try:
        chat_vectordb, _ = load_chat_vectordb()
        results["chat_vectordb"] = chat_vectordb is not None
    except:
        results["chat_vectordb"] = False
    # Test graphe OSM
    try:
        graph, pois, _ = load_existing_graph()
        results["osm_graph"] = graph is not None
    except:
        results["osm_graph"] = False
    return results
# ========================================
# Fonctions de maintenance avanc√©es
# ========================================
def optimize_vectordb():
    """Optimise la base vectorielle en supprimant les doublons"""
    try:
        vectordb, status = load_vectordb()
        if not vectordb:
            return "‚ùå Aucune base vectorielle √† optimiser"
        # Cette fonction n√©cessiterait une impl√©mentation plus complexe
        # pour d√©tecter et supprimer les doublons dans FAISS
        return "‚úÖ Base vectorielle optimis√©e (fonctionnalit√© √† impl√©menter)"
    except Exception as e:
        return f"‚ùå Erreur optimisation: {e}"
def backup_all_data():
    """Cr√©e une sauvegarde de toutes les donn√©es"""
    try:
        import zipfile
        backup_name = f"kibali_backup_{time.strftime('%Y%m%d_%H%M%S')}.zip"
        backup_path = os.path.join(CHATBOT_DIR, backup_name)
        with zipfile.ZipFile(backup_path, 'w', zipfile.ZIP_DEFLATED) as backup_zip:
            # Sauvegarder tous les fichiers du dossier chatbot
            for root, dirs, files in os.walk(CHATBOT_DIR):
                for file in files:
                    file_path = os.path.join(root, file)
                    arcname = os.path.relpath(file_path, CHATBOT_DIR)
                    backup_zip.write(file_path, arcname)
        return f"‚úÖ Sauvegarde cr√©√©e: {backup_path}"
    except Exception as e:
        return f"‚ùå Erreur sauvegarde: {e}"
def restore_from_backup(backup_path):
    """Restaure les donn√©es depuis une sauvegarde"""
    try:
        import zipfile
        if not os.path.exists(backup_path):
            return "‚ùå Fichier de sauvegarde non trouv√©"
        with zipfile.ZipFile(backup_path, 'r') as backup_zip:
            backup_zip.extractall(CHATBOT_DIR)
        return f"‚úÖ Donn√©es restaur√©es depuis: {backup_path}"
    except Exception as e:
        return f"‚ùå Erreur restauration: {e}"
# ========================================
# NOUVEAU: Fonctions Auto-Apprentissage et Sous-Mod√®les avec Scikit-Learn
# ========================================
def create_submodel_from_chat_history(chat_vectordb, submodel_type="classification"):
    """
    Cr√©e un petit sous-mod√®le sklearn √† partir de l'historique chat pour automatiser des r√©ponses.
    - Type: 'classification' pour classer les questions et pr√©dire des r√©ponses automatis√©es.
    Rend le mod√®le plus "humain" en apprenant des patterns conversationnels.
    """
    if not chat_vectordb:
        return None, "‚ùå Aucune base chat pour entra√Æner le sous-mod√®le"
  
    # Extraire les √©changes de l'historique
    exchanges = []
    for doc in list(chat_vectordb.docstore._dict.values()) or []:
        exchange = doc.page_content
        if "User:" in exchange and "Assistant:" in exchange:
            user_part = exchange.split("|||")[0].replace("User: ", "").strip()
            ai_part = exchange.split("|||")[1].replace("Assistant: ", "").strip() if "|||" in exchange else ""
            exchanges.append((user_part, ai_part))
  
    if len(exchanges) < 10:
        return None, "‚ùå Historique chat trop court pour entra√Æner un mod√®le"
  
    try:
        # Pr√©paration des donn√©es : TF-IDF pour vectorisation textuelle
        vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        X = vectorizer.fit_transform([user[0] for user in exchanges])
      
        # Pour classification simple (ex: pr√©dire si r√©ponse est informative ou autre)
        # Labels simples bas√©s sur patterns (ex: 0=info, 1=question, 2=autre)
        labels = []
        for user_msg, _ in exchanges:
            if re.search(r'\?', user_msg):
                labels.append(1) # Question
            elif any(word in user_msg.lower() for word in ['info', 'savoir', 'expliquer']):
                labels.append(0) # Info
            else:
                labels.append(2) # Autre
      
        X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)
      
        if submodel_type == "classification":
            model = MultinomialNB()
        else:
            model = RandomForestClassifier(n_estimators=50)
      
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
      
        # Sauvegarder le mod√®le et vectorizer
        model_path = os.path.join(SUBMODELS_PATH, f"submodel_{submodel_type}_{int(time.time())}.pkl")
        with open(model_path, 'wb') as f:
            pickle.dump({'model': model, 'vectorizer': vectorizer}, f)
      
        # Visualisation avec matplotlib : Accuracy plot
        fig, ax = plt.subplots()
        ax.bar(['Train', 'Test'], [1.0, accuracy]) # Train est parfait par d√©faut
        ax.set_title(f'Pr√©cision du sous-mod√®le {submodel_type.capitalize()}')
        ax.set_ylabel('Accuracy')
        plot_path = os.path.join(SUBMODELS_PATH, f"accuracy_plot_{submodel_type}_{int(time.time())}.png")
        plt.savefig(plot_path)
        plt.close()
      
        return model_path, f"‚úÖ Sous-mod√®le {submodel_type} cr√©√© avec accuracy {accuracy:.2f}. Sauvegard√©: {model_path}"
    except Exception as e:
        return None, f"‚ùå Erreur cr√©ation sous-mod√®le: {e}"
def use_submodel_for_automation(query, submodel_path, submodel_type="classification"):
    """
    Utilise un sous-mod√®le pour automatiser une r√©ponse, rendant le comportement plus humain (ex: pr√©diction rapide).
    """
    if not os.path.exists(submodel_path):
        return "‚ùå Sous-mod√®le non trouv√©"
  
    try:
        with open(submodel_path, 'rb') as f:
            data = pickle.load(f)
            model = data['model']
            vectorizer = data['vectorizer']
      
        query_vec = vectorizer.transform([query])
        prediction = model.predict(query_vec)[0]
      
        # R√©ponses automatis√©es bas√©es sur pr√©diction pour plus d'humanit√©
        automated_responses = {
            0: "Voici des infos basiques sur ce sujet, bas√©es sur nos √©changes pass√©s.",
            1: "Bonne question ! Laisse-moi r√©fl√©chir √† √ßa en me basant sur ce qu'on a discut√© avant.",
            2: "Int√©ressant, je vais creuser un peu plus pour te r√©pondre de mani√®re personnalis√©e."
        }
      
        response = automated_responses.get(prediction, "R√©ponse automatis√©e g√©n√©r√©e.")
      
        # Visualisation: Distribution des features TF-IDF pour la query
        fig, ax = plt.subplots()
        tfidf_scores = query_vec.toarray()[0]
        top_features = np.argsort(tfidf_scores)[-5:]
        ax.bar(range(len(top_features)), tfidf_scores[top_features])
        ax.set_title('Top Features TF-IDF pour la Query')
        ax.set_xticks(range(len(top_features)))
        ax.set_xticklabels([vectorizer.get_feature_names_out()[i] for i in top_features], rotation=45)
        plot_path = os.path.join(SUBMODELS_PATH, f"query_features_{int(time.time())}.png")
        plt.savefig(plot_path)
        plt.close()
      
        return f"{response} (Pr√©diction: {prediction}) | Graph: {plot_path}"
    except Exception as e:
        return f"‚ùå Erreur utilisation sous-mod√®le: {e}"
# ========================================
# NOUVEAU: Fonctions Am√©lioration Base de Donn√©es via Fouille Internet
# ========================================
def improve_database_with_web_search(topics, num_results_per_topic=5, vectordb=None):
    """
    Fouille internet sur des sujets sp√©cifiques (p√©trole, topographie, sciences physiques, sous-sol, etc.)
    et am√©liore la base de donn√©es en ajoutant de nouveaux documents.
    """
    specific_topics = topics or ["p√©trole extraction techniques", "topographie cartographie avanc√©e", "sciences physiques m√©canique sol", "sous-sol g√©ologie ressources"]
  
    if vectordb is None:
        vectordb, _ = load_vectordb()
        if vectordb is None:
            embedding_model = get_embedding_model()
            vectordb = FAISS.from_texts([""], embedding_model)
  
    new_documents = []
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
  
    for topic in specific_topics:
        st.write(f"üîç Fouille internet pour: {topic}")
        search_results = enhanced_web_search(topic, max_results=num_results_per_topic, search_type="both")
      
        for result in search_results:
            content = f"Titre: {result.get('title', '')}\nContenu: {result.get('body', '')}\n"
            url = result.get('href') or result.get('url')
            if url and len(result.get('body', '')) < 500:
                extra_content = smart_content_extraction(url, max_length=2000)
                if "Impossible d'extraire" not in extra_content:
                    content += f"\nContenu d√©taill√©: {extra_content}"
          
            chunks = text_splitter.split_text(content)
            for i, chunk in enumerate(chunks):
                doc = Document(
                    page_content=chunk,
                    metadata={
                        "source": url or topic,
                        "topic": topic,
                        "type": "web_enrichment",
                        "chunk_id": i
                    }
                )
                new_documents.append(doc)
  
    if new_documents:
        vectordb.add_documents(new_documents)
        vectordb.save_local(VECTORDB_PATH)
        return vectordb, f"‚úÖ Base am√©lior√©e: {len(new_documents)} nouveaux chunks ajout√©s sur {len(specific_topics)} sujets"
    else:
        return vectordb, "‚ö†Ô∏è Aucun nouveau contenu ajout√©"
# ========================================
# Version API pour utilisation externe
# ========================================
class KibaliAPI:
    """API simplifi√©e pour utiliser Kibali depuis du code externe"""
    def __init__(self):
        self.vectordb = None
        self.chat_vectordb = None # AJOUT M√âMOIRE VECTORIELLE
        self.graph = None
        self.pois = []
        self.client = None
        self.model_name = WORKING_MODELS[list(WORKING_MODELS.keys())[0]]
        # Initialisation automatique
        self._initialize()
    def _initialize(self):
        """Initialisation automatique"""
        try:
            setup_drive()
            self.vectordb, _ = load_vectordb()
            self.chat_vectordb, _ = load_chat_vectordb() # AJOUT M√âMOIRE VECTORIELLE
            self.graph, self.pois, _ = load_existing_graph()
            self.client = create_client()
        except Exception as e:
            print(f"‚ö†Ô∏è Initialisation partielle: {e}")
    def ask(self, question, use_web=True):
        """Pose une question simple"""
        try:
            if use_web:
                docs = hybrid_search_enhanced(question, self.vectordb, web_search_enabled=True, chat_vectordb=self.chat_vectordb) # AJOUT M√âMOIRE VECTORIELLE
            else:
                docs = rag_search(question, self.vectordb)
            return generate_answer_enhanced(question, docs, self.model_name)
        except Exception as e:
            return f"‚ùå Erreur: {e}"
    def search_web(self, query, max_results=5):
        """Recherche web simple"""
        try:
            results = enhanced_web_search(query, max_results)
            return [{"title": r.get("title"), "url": r.get("href", r.get("url")), "snippet": r.get("body")} for r in results]
        except Exception as e:
            return [{"error": str(e)}]
    def calculate_route(self, from_place, to_place):
        """Calcule un itin√©raire"""
        try:
            question = f"Comment aller de {from_place} √† {to_place}"
            _, response, info = calculer_trajet(question, self.graph, self.pois)
            return {"response": response, "info": info}
        except Exception as e:
            return {"error": str(e)}
    def get_status(self):
        """Retourne le statut du syst√®me"""
        return get_system_status()
    # NOUVEAU: M√©thodes API pour auto-apprentissage et am√©lioration DB
    def train_submodel(self, submodel_type="classification"):
        """Entra√Æne un sous-mod√®le"""
        path, msg = create_submodel_from_chat_history(self.chat_vectordb, submodel_type)
        return {"path": path, "message": msg}
    def improve_db(self, topics=None, num_results=5):
        """Am√©liore la DB avec fouille internet"""
        self.vectordb, msg = improve_database_with_web_search(topics, num_results, self.vectordb)
        return {"message": msg}
# Instance globale de l'API
kibali_api = KibaliAPI()
# ========================================
# Interface Streamlit Am√©lior√©e
# ========================================
st.markdown("""
<style>
    .stApp {
        background: white;
        color: black;
    }
    .sidebar .sidebar-content {
        background: white;
    }
    .stSidebar > div {
        background: white;
    }
    .stChatMessage {
        background: white;
        border-radius: 18px;
        border-left: 4px solid #2196F3;
        margin: 5px 0;
        padding: 12px;
        box-shadow: 0 1px 2px rgba(0,0,0,0.1);
        color: black !important;
        transition: all 0.3s ease;
        filter: none; /* Correction pour flou */
    }
    .stChatMessage:hover {
        transform: scale(1.02);
        box-shadow: 0 4px 8px rgba(0,0,0,0.2);
    }
    .stChatMessage p, .stChatMessage li {
        color: black !important;
        background-color: rgba(255, 255, 255, 0.1);
    }
    .stTextInput > div > div > input {
        background: white;
        border: 1px solid #2196F3;
        border-radius: 20px;
        color: black;
        padding: 10px 15px;
        filter: none; /* Correction pour flou */
    }
    .stTextInput > div > div > input::placeholder {
        color: #757575;
    }
    .stButton > button {
        background: linear-gradient(45deg, #2196F3 0%, #21CBF3 100%);
        color: white;
        border: none;
        border-radius: 20px;
        padding: 10px 20px;
        font-weight: bold;
        box-shadow: 0 4px 8px rgba(0,0,0,0.3);
        transition: all 0.3s ease;
        width: 100%;
        margin-bottom: 10px;
    }
    .stButton > button:hover {
        transform: translateY(-2px) scale(1.05);
        box-shadow: 0 6px 12px rgba(0,0,0,0.4);
        animation: pulse 1s infinite; /* Effet fluide */
    }
    @keyframes pulse {
        0% { box-shadow: 0 6px 12px rgba(0,0,0,0.4); }
        50% { box-shadow: 0 6px 16px rgba(33, 150, 243, 0.6); }
        100% { box-shadow: 0 6px 12px rgba(0,0,0,0.4); }
    }
    .stSelectbox > div > div > select {
        background: white;
        border: 1px solid #2196F3;
        border-radius: 10px;
        color: black;
        filter: none; /* Correction pour flou */
    }
    .stCheckbox > div > label {
        color: black;
        transition: color 0.3s ease;
    }
    .stCheckbox > div > label:hover {
        color: #2196F3;
    }
    .stTextArea > div > div > textarea {
        background: white;
        color: black;
        border: 1px solid #2196F3;
    }
    h1, h2, h3 {
        color: #2196F3;
        text-shadow: 0 0 10px rgba(33, 150, 243, 0.5);
        animation: glow 2s ease-in-out infinite alternate;
    }
    @keyframes glow {
        from { text-shadow: 0 0 10px rgba(33, 150, 243, 0.5); }
        to { text-shadow: 0 0 20px rgba(33, 150, 243, 0.8), 0 0 30px rgba(33, 203, 243, 0.6); }
    }
    .chat-footer {
        position: fixed;
        bottom: 0;
        left: 0;
        right: 0;
        background: rgba(255, 255, 255, 0.95);
        border-top: 1px solid #2196F3;
        padding: 10px;
        z-index: 1000;
        transition: all 0.3s ease;
    }
    .chat-footer:hover {
        background: rgba(255, 255, 255, 1);
    }
    /* Effet scintillante pour mots importants */
    .sparkle-word {
        color: #2196F3;
        background: linear-gradient(45deg, #2196F3, #21CBF3, #4ecdc4, #45b7d1);
        background-size: 400% 400%;
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        animation: sparkle 2s linear infinite, gradient-shift 3s ease infinite;
        cursor: pointer;
        position: relative;
        padding: 2px 4px;
        border-radius: 4px;
        transition: transform 0.2s ease;
    }
    .sparkle-word:hover {
        transform: scale(1.1);
        text-shadow: 0 0 10px rgba(33, 150, 243, 0.8);
    }
    @keyframes sparkle {
        0%, 100% { text-shadow: 0 0 5px rgba(33, 150, 243, 0.5); }
        50% { text-shadow: 0 0 20px rgba(33, 150, 243, 1), 0 0 30px rgba(33, 203, 243, 0.7); }
    }
    @keyframes gradient-shift {
        0% { background-position: 0% 50%; }
        50% { background-position: 100% 50%; }
        100% { background-position: 0% 50%; }
    }
    /* Correction pour lisibilit√© des questions/r√©ponses */
    .stMarkdown {
        filter: none !important;
        -webkit-filter: none !important;
        color: black !important;
        font-weight: 400;
        line-height: 1.6;
        background-color: rgba(255, 255, 255, 0.1);
    }
    .stMarkdown p, .stMarkdown li {
        color: black !important;
        text-shadow: none;
    }
    .st-emotion-cache-1i5yq8u input, .st-emotion-cache-1i5yq8u textarea {
        color: black !important;
    }
    @media (max-width: 768px) {
        .chat-footer {
            padding: 5px;
        }
        .stTextInput input {
            font-size: 14px;
        }
        .sparkle-word {
            font-size: 0.9em;
        }
    }
</style>
""", unsafe_allow_html=True)
# Sidebar pour options
with st.sidebar:
    st.markdown("<h2 style='color: #2196F3; text-align: center;'>‚öôÔ∏è Options</h2>", unsafe_allow_html=True)
    st.markdown("---")
  
    # Initialisation des √©tats de session
    if 'status_msg' not in st.session_state:
        st.session_state.status_msg = ""
    if 'cache_msg' not in st.session_state:
        st.session_state.cache_msg = get_cache_stats()
  
    # Uploads et boutons config
    pdf_upload = st.file_uploader("üì§ Upload PDFs", type="pdf", accept_multiple_files=True, key="pdf_sidebar")
    
    # üÜï NOUVEAU: Upload de rapports ERT pour extraction automatique
    st.markdown("#### üî¨ Extraction Rapports ERT")
    ert_pdf_upload = st.file_uploader("üìÑ Upload Rapport ERT (PDF)", type="pdf", key="ert_pdf_upload")
    extract_ert_btn = st.button("üîç Extraire donn√©es ERT", key="extract_ert_btn")
    
    # üÜï NOUVEAU: Upload audio pour transcription
    audio_upload = st.file_uploader("üé§ Upload Notes Audio", type=["wav", "mp3", "m4a"], key="audio_upload")
    transcribe_audio_btn = st.button("üìù Transcrire Audio", key="transcribe_audio_btn")
    
    pbf_upload = st.file_uploader("üì§ Upload OSM (.pbf)", type="osm.pbf", key="pbf_sidebar")
    process_pdfs_btn = st.button("üîÑ Traiter PDFs", key="process_sidebar")
    load_graph_btn = st.button("üìÇ Charger graphe", key="load_graph_sidebar")
    load_vectordb_btn = st.button("üìÇ Charger DB", key="load_db_sidebar")
    clear_cache_btn = st.button("üóëÔ∏è Vider cache", key="clear_cache_sidebar")
  
    # NOUVEAU: Boutons pour auto-apprentissage et am√©lioration
    train_submodel_btn = st.button("üß† Entra√Æner sous-mod√®le (sklearn)", key="train_submodel")
    improve_db_btn = st.button("üìö Am√©liorer DB (fouille internet)", key="improve_db")
  
    st.markdown("---")
    status_display = st.text_area("üìä Statut", value=st.session_state.status_msg, height=100, key='status_sidebar')
    cache_stats = st.text_area("üìà Cache", value=st.session_state.cache_msg, height=50, key='cache_sidebar')
  
    if "vectordb" not in st.session_state:
        st.session_state.vectordb = None
    if "chat_vectordb" not in st.session_state: # AJOUT M√âMOIRE VECTORIELLE
        st.session_state.chat_vectordb = None
    if "graph" not in st.session_state:
        st.session_state.graph = None
    if "pois" not in st.session_state:
        st.session_state.pois = []
    if "current_model" not in st.session_state:
        st.session_state.current_model = WORKING_MODELS[list(WORKING_MODELS.keys())[0]]
    if "agent" not in st.session_state:
        st.session_state.agent = None
    if pdf_upload:
        files = upload_pdfs(pdf_upload)
        st.session_state.status_msg = f"‚úÖ {len(files)} PDFs upload√©s" if files else "‚ö†Ô∏è Aucun PDF"
        # Pas de rerun ici : file_uploader g√®re d√©j√†
    if pbf_upload:
        st.session_state.graph, st.session_state.pois, msg = upload_and_process_pbf(pbf_upload)
        st.session_state.status_msg = msg
        model_choice = st.selectbox("Mod√®le", list(WORKING_MODELS.keys()), key="model_sidebar")
        st.session_state.current_model, st.session_state.agent, cache_info = update_agent(model_choice, st.session_state.vectordb, st.session_state.graph, st.session_state.pois, st.session_state.chat_vectordb) # AJOUT M√âMOIRE VECTORIELLE
        st.session_state.cache_msg = cache_info
        st.rerun()
    if process_pdfs_btn:
        st.session_state.vectordb, msg = process_pdfs()
        st.session_state.status_msg = msg
        model_choice = st.selectbox("Mod√®le", list(WORKING_MODELS.keys()), key="model_process")
        st.session_state.current_model, st.session_state.agent, cache_info = update_agent(model_choice, st.session_state.vectordb, st.session_state.graph, st.session_state.pois, st.session_state.chat_vectordb) # AJOUT M√âMOIRE VECTORIELLE
        st.session_state.cache_msg = cache_info
        st.rerun()
    if load_graph_btn:
        st.session_state.graph, st.session_state.pois, msg = load_existing_graph()
        st.session_state.status_msg = msg
        model_choice = st.selectbox("Mod√®le", list(WORKING_MODELS.keys()), key="model_load_graph")
        st.session_state.current_model, st.session_state.agent, cache_info = update_agent(model_choice, st.session_state.vectordb, st.session_state.graph, st.session_state.pois, st.session_state.chat_vectordb) # AJOUT M√âMOIRE VECTORIELLE
        st.session_state.cache_msg = cache_info
        st.rerun()
    if load_vectordb_btn:
        st.session_state.vectordb, msg = load_vectordb()
        st.session_state.status_msg = msg
        model_choice = st.selectbox("Mod√®le", list(WORKING_MODELS.keys()), key="model_load_db")
        st.session_state.chat_vectordb, _ = load_chat_vectordb() # AJOUT M√âMOIRE VECTORIELLE: Charger chat db
        st.session_state.current_model, st.session_state.agent, cache_info = update_agent(model_choice, st.session_state.vectordb, st.session_state.graph, st.session_state.pois, st.session_state.chat_vectordb)
        st.session_state.cache_msg = cache_info
        st.rerun()
    if clear_cache_btn:
        msg = handle_clear_cache()
        st.session_state.status_msg = msg
        st.session_state.cache_msg = get_cache_stats()
        st.rerun()
  
    # NOUVEAU: Gestion des boutons auto-apprentissage et am√©lioration
    if train_submodel_btn:
        st.session_state.chat_vectordb, _ = load_chat_vectordb()
        submodel_path, msg = create_submodel_from_chat_history(st.session_state.chat_vectordb)
        st.session_state.status_msg = msg
        if submodel_path:
            st.write(f"Utiliser: use_submodel_for_automation('query', '{submodel_path}')")
        st.rerun()
  
    if improve_db_btn:
        topics_input = st.text_input("Sujets (s√©par√©s par ,)", value="p√©trole,topographie,sciences physiques,sous-sol", key="topics_input")
        topics = [t.strip() for t in topics_input.split(",")]
        st.session_state.vectordb, msg = improve_database_with_web_search(topics)
        st.session_state.status_msg = msg
        st.rerun()
    
    # üÜï NOUVEAU: Gestion extraction PDF ERT
    if extract_ert_btn and ert_pdf_upload:
        # Sauvegarder temporairement le PDF
        temp_pdf_path = f"/tmp/ert_report_{int(time.time())}.pdf"
        with open(temp_pdf_path, "wb") as f:
            f.write(ert_pdf_upload.getvalue())
        
        # Extraire donn√©es
        extraction_results = extract_ert_report_from_pdf(temp_pdf_path)
        
        # Afficher r√©sultats
        st.success(f"‚úÖ Extraction termin√©e!")
        st.write(f"üìä **Images extraites**: {len(extraction_results['images'])}")
        st.write(f"üìù **L√©gendes**: {len(extraction_results['captions'])}")
        st.write(f"üî¢ **Valeurs r√©sistivit√©**: {len(extraction_results['resistivity_values'])}")
        
        if extraction_results['resistivity_values']:
            st.write(f"üìà **Plage r√©sistivit√©**: {min(extraction_results['resistivity_values']):.4f} - {max(extraction_results['resistivity_values']):.2f} Œ©¬∑m")
            
            # Analyser min√©raux automatiquement
            mineral_report = analyze_minerals_from_resistivity(
                extraction_results['resistivity_values'], 
                ert_pdf_upload.name
            )
            st.text_area("üî¨ Rapport Min√©ralogique", mineral_report, height=400)
            
            # üÜï CR√âER TABLEAU DE CORRESPONDANCES
            st.markdown("### üìä Tableau de Correspondances R√©elles")
            fig_corr, df_corr, rapport_corr = create_real_mineral_correspondence_table(
                extraction_results['resistivity_values'],
                ert_pdf_upload.name
            )
            
            if fig_corr and df_corr is not None:
                st.pyplot(fig_corr)
                plt.close(fig_corr)
                
                # Corriger les pourcentages de confiance
                df_corr_display = df_corr.copy()
                if 'Confiance' in df_corr_display.columns:
                    if df_corr_display['Confiance'].max() <= 1:
                        df_corr_display['Confiance (%)'] = (df_corr_display['Confiance'] * 100).round(1)
                    else:
                        df_corr_display['Confiance (%)'] = df_corr_display['Confiance'].round(1)
                    df_corr_display = df_corr_display.drop('Confiance', axis=1)
                
                # Organiser en plusieurs tableaux si n√©cessaire
                total_rows = len(df_corr_display)
                if total_rows > 20:
                    st.markdown("#### üìã Donn√©es Tabulaires - Organis√©es par Profondeur")
                    
                    depth_col = 'Profondeur (m)' if 'Profondeur (m)' in df_corr_display.columns else df_corr_display.columns[0]
                    df_sorted = df_corr_display.sort_values(depth_col)
                    
                    quantiles = [0, 0.2, 0.4, 0.6, 0.8, 1.0]
                    depth_ranges = df_sorted[depth_col].quantile(quantiles).values
                    
                    for i in range(5):
                        min_depth = depth_ranges[i]
                        max_depth = depth_ranges[i+1]
                        
                        if i == 4:
                            mask = (df_sorted[depth_col] >= min_depth) & (df_sorted[depth_col] <= max_depth)
                        else:
                            mask = (df_sorted[depth_col] >= min_depth) & (df_sorted[depth_col] < max_depth)
                        
                        df_section = df_sorted[mask]
                        
                        if len(df_section) > 0:
                            with st.expander(f"üìä Tableau {i+1}/5 - Profondeur: {min_depth:.1f} √† {max_depth:.1f} m ({len(df_section)} d√©tections)", expanded=(i==0)):
                                st.dataframe(
                                    df_section,
                                    use_container_width=True,
                                    column_config={
                                        "Confiance (%)": st.column_config.NumberColumn(
                                            "Confiance (%)",
                                            format="%.1f%%"
                                        ),
                                        "R√©sistivit√© mesur√©e (Œ©¬∑m)": st.column_config.NumberColumn(
                                            "R√©sistivit√© mesur√©e (Œ©¬∑m)",
                                            format="%.6f"
                                        ),
                                        "Profondeur (m)": st.column_config.NumberColumn(
                                            "Profondeur (m)",
                                            format="%.1f"
                                        )
                                    },
                                    height=min(400, len(df_section) * 35 + 38)
                                )
                else:
                    st.dataframe(
                        df_corr_display,
                        use_container_width=True,
                        column_config={
                            "Confiance (%)": st.column_config.NumberColumn(
                                "Confiance (%)",
                                format="%.1f%%"
                            ),
                            "R√©sistivit√© mesur√©e (Œ©¬∑m)": st.column_config.NumberColumn(
                                "R√©sistivit√© mesur√©e (Œ©¬∑m)",
                                format="%.6f"
                            ),
                            "Profondeur (m)": st.column_config.NumberColumn(
                                "Profondeur (m)",
                                format="%.1f"
                            )
                        }
                    )
                
                st.text_area("üìù Rapport D√©taill√©", rapport_corr, height=400)
                
                # T√©l√©chargement CSV
                csv_data = df_corr_display.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label="üì• T√©l√©charger Correspondances (CSV)",
                    data=csv_data,
                    file_name=f"{ert_pdf_upload.name}_correspondances.csv",
                    mime="text/csv"
                )
            
            # üÜï G√âN√âRER COUPES ERT PROFESSIONNELLES
            st.markdown("### üé® Coupes ERT Professionnelles (5 Graphiques)")
            
            # Option mode grand format
            col_btn1, col_btn2 = st.columns([1, 1])
            with col_btn1:
                use_fullsize_pdf = st.checkbox("üñºÔ∏è Mode GRAND FORMAT PDF (30√ó36 pouces)", value=False, 
                                              help="Graphiques haute r√©solution pour impression A0/A1", key="fullsize_pdf")
            
            fig_ert, grid_data, rapport_ert = create_ert_professional_sections(
                extraction_results['resistivity_values'],
                ert_pdf_upload.name,
                full_size=use_fullsize_pdf
            )
            
            if fig_ert is not None:
                # Affichage responsive
                st.pyplot(fig_ert, use_container_width=True)
                
                # Rapport
                st.text_area("üìä Rapport ERT", rapport_ert, height=300)
                
                # Boutons de t√©l√©chargement en colonnes
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    # PNG haute r√©solution
                    import io
                    buf_png = io.BytesIO()
                    fig_ert.savefig(buf_png, format='png', dpi=300, bbox_inches='tight')
                    buf_png.seek(0)
                    st.download_button(
                        label="üì• PNG 300 DPI",
                        data=buf_png,
                        file_name=f"{ert_pdf_upload.name}_ert_300dpi.png",
                        mime="image/png",
                        key="dl_png_pdf"
                    )
                
                with col2:
                    # PDF vectoriel
                    buf_pdf = io.BytesIO()
                    fig_ert.savefig(buf_pdf, format='pdf', bbox_inches='tight')
                    buf_pdf.seek(0)
                    st.download_button(
                        label="üìÑ PDF Vectoriel",
                        data=buf_pdf,
                        file_name=f"{ert_pdf_upload.name}_ert.pdf",
                        mime="application/pdf",
                        key="dl_pdf_pdf"
                    )
                
                with col3:
                    # Grille de donn√©es
                    if grid_data:
                        import pickle
                        grid_pickle = pickle.dumps(grid_data)
                        st.download_button(
                            label="ÔøΩ Grille PKL",
                            data=grid_pickle,
                            file_name=f"{ert_pdf_upload.name}_grid.pkl",
                            mime="application/octet-stream",
                            key="dl_grid_pdf"
                        )
                
                plt.close(fig_ert)
        
        st.session_state.status_msg = f"‚úÖ PDF ERT extrait: {len(extraction_results['images'])} images"
        
    # üÜï NOUVEAU: Gestion transcription audio
    if transcribe_audio_btn and audio_upload:
        temp_audio_path = f"/tmp/audio_{int(time.time())}.{audio_upload.name.split('.')[-1]}"
        with open(temp_audio_path, "wb") as f:
            f.write(audio_upload.getvalue())
        
        transcription = process_audio_transcription(temp_audio_path)
        
        if transcription:
            st.text_area("üìù Transcription", transcription, height=200)
            st.session_state.status_msg = f"‚úÖ Audio transcrit: {len(transcription)} caract√®res"

# Main area - Chat principal
st.title("üó∫Ô∏è Kibali üåü - Assistant IA Avanc√©")
main_container = st.container()
with main_container:
    # Onglets pour autres fonctionnalit√©s
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["üó∫Ô∏è Trajets", "üì∏ Analyse Image", "üåê Recherche Web", "üí¨ Chat", "üìä Status"])
    with tab1:
        st.markdown("""
        ### Calcul de trajets
        **Exemples:** "Comment aller de l'√©cole √† l'h√¥pital ?"
        """)
        trajectory_input = st.text_area("üó∫Ô∏è Question de trajet", key="traj_input")
        if st.button("üöÄ Calculer trajet", key="calc_traj"):
            carte_buf, reponse, traj_info = calculer_trajet(trajectory_input, st.session_state.graph, st.session_state.pois)
            st.text_area("üìã D√©tails", reponse, key="traj_details")
            if carte_buf:
                carte_buf.seek(0)
                st.image(Image.open(carte_buf), key="traj_map")
            if traj_info:
                if st.button("üíæ Sauvegarder trajet", key="save_traj"):
                    save_trajectory(trajectory_input, reponse, traj_info)
                    st.write("‚úÖ Trajet sauvegard√©")
    with tab2:
        st.markdown("""
        ### Analyse d'images
        Upload une image pour analyse d√©taill√©e, annotations, graphiques et am√©lioration IA.
        """)
        image_upload = st.file_uploader("üì§ Upload Image", type=["jpg", "png"], key="img_upload")
        if image_upload and st.button("üîç Analyser", key="analyze_img"):
            analysis_data, proc_images, tables_str = process_image(image_upload.getvalue())
            improved_analysis = improve_analysis_with_llm(analysis_data, st.session_state.current_model)
            st.image(proc_images, caption=proc_images, width=400) # Responsive width
            st.markdown(tables_str, unsafe_allow_html=True)
            st.text_area("Analyse Am√©lior√©e (IA)", improved_analysis, key="img_analysis")
    with tab3:
        st.markdown("""
        ### Recherche web avanc√©e avec extraction de contenu
        """)
        web_query = st.text_area("üîç Requ√™te de recherche", key="web_query")
        search_type = st.selectbox("Type de recherche", ["text", "news", "both"], key="search_type")
        if st.button("üîç Rechercher", key="search_btn"):
            results = handle_web_search(web_query, search_type)
            st.markdown(results, unsafe_allow_html=True)
        url_extract = st.text_input("üåê URL √† extraire", key="url_extract")
        if st.button("üìÑ Extraire contenu", key="extract_btn"):
            content = handle_content_extraction(url_extract)
            st.text_area("Contenu extrait", content, key="extracted_content")
    with tab4:
        st.markdown("### Assistant IA avec recherche web int√©gr√©e")
        web_search_toggle = st.checkbox("üåê Recherche web activ√©e", value=True, key="web_toggle")
        # NOUVEAU: Option pour utiliser sous-mod√®le
        use_submodel = st.checkbox("üß† Utiliser sous-mod√®le auto-appris pour r√©ponse rapide", key="use_submodel")
        submodel_path_input = st.text_input("Chemin sous-mod√®le (optionnel)", key="submodel_path")
      
        if "chat_history" not in st.session_state:
            st.session_state.chat_history = []
        for msg in st.session_state.chat_history:
            with st.chat_message(msg["role"], avatar="‚òÅÔ∏è" if msg["role"] == "user" else "‚≠ê"):
                # Correction pour lisibilit√© : utiliser markdown pour HTML
                if msg["role"] == "user":
                    st.markdown(f"**Question:** {highlight_important_words(msg['content'])}", unsafe_allow_html=True)
                else:
                    st.markdown(highlight_important_words(msg['content']), unsafe_allow_html=True)
        if prompt := st.chat_input("Pose une question...", key="chat_input"):
            with st.chat_message("user", avatar="‚òÅÔ∏è"):
                highlighted_prompt = highlight_important_words(prompt)
                st.markdown(f"**Question:** {highlighted_prompt}", unsafe_allow_html=True)
            with st.chat_message("assistant", avatar="‚≠ê"):
                with st.spinner("R√©ponse en cours..."):
                    content_to_save = None # Variable interm√©diaire pour corriger l'erreur NameError
                    if use_submodel and submodel_path_input:
                        automated = use_submodel_for_automation(prompt, submodel_path_input)
                        st.markdown(highlight_important_words(automated), unsafe_allow_html=True)
                        content_to_save = automated
                    else:
                        response = handle_chat_enhanced(prompt, st.session_state.chat_history, st.session_state.agent, list(WORKING_MODELS.keys())[0], st.session_state.vectordb, st.session_state.graph, st.session_state.pois, web_search_toggle)
                        st.markdown(highlight_important_words(response), unsafe_allow_html=True)
                        content_to_save = response
            st.session_state.chat_history.append({"role": "user", "content": prompt})
            st.session_state.chat_history.append({"role": "assistant", "content": content_to_save})
    with tab5:
        st.markdown("### Statut syst√®me")
        st.json(get_system_status())
st.markdown("### üìä Informations Syst√®me")
setup_drive()
st.write(f"üöÄ Kibali üåü - Assistant IA Avanc√© avec Recherche Web")
st.write(f"üìÅ Dossier unifi√©: {CHATBOT_DIR}")
st.write(f"üîë Token HF configur√©: {HF_TOKEN[:10]}...")
st.write(f"üåê Recherche web int√©gr√©e")
existing_graphs = [f for f in os.listdir(GRAPHS_PATH) if f.endswith('_graph.graphml')] if os.path.exists(GRAPHS_PATH) else []
existing_pdfs = [f for f in os.listdir(PDFS_PATH) if f.endswith('.pdf')] if os.path.exists(PDFS_PATH) else []
st.write(f"üìä √âtat initial:")
st.write(f" üó∫Ô∏è Graphes OSM: {len(existing_graphs)}")
st.write(f" üìÑ PDFs: {len(existing_pdfs)}")
st.write(f" üíæ Base vectorielle: {'‚úÖ' if os.path.exists(VECTORDB_PATH) else '‚ùå'}")
st.write(f" üß† M√©moire chat: {'‚úÖ' if os.path.exists(CHAT_VECTORDB_PATH) else '‚ùå'}") # AJOUT M√âMOIRE VECTORIELLE
st.write(f" üåê Cache web: {'‚úÖ' if os.path.exists(WEB_CACHE_PATH) else '‚ùå'}")
st.write(f" üìà {get_cache_stats()}")
st.write("\n" + "="*60)
st.write("üéâ KIBALI üåü - SYST√àME CHARG√â AVEC SUCC√àS")
st.write("="*60)
st.write(f"üìÖ Version: 2.0.0 - {time.strftime('%Y-%m-%d %H:%M:%S')}")
st.write(f"üîë Token HF: {'‚úÖ Configur√©' if HF_TOKEN else '‚ùå Manquant'}")
st.write(f"üìÅ Dossier: {CHATBOT_DIR}")
st.write(f"üåê Recherche web: ‚úÖ Activ√©e")
st.write(f"üíæ Cache intelligent: ‚úÖ Activ√©")
st.write(f"üß† M√©moire vectorielle chat: ‚úÖ Activ√©e") # AJOUT M√âMOIRE VECTORIELLE
st.write(f"ü§ñ Auto-apprentissage sklearn: ‚úÖ Activ√© (sous-mod√®les dans {SUBMODELS_PATH})")
st.write(f"üìö Am√©lioration DB via fouille: ‚úÖ Activ√©e (sujets p√©trole, topographie, etc.)")
st.write("\nüìö FONCTIONNALIT√âS PRINCIPALES:")
st.write(" üí¨ Chat RAG avec recherche web intelligent")
st.write(" üß† M√©moire des conversations pour fluidit√©") # AJOUT M√âMOIRE VECTORIELLE
st.write(" üó∫Ô∏è Calcul de trajets OSM")
st.write(" üì∏ Analyse d'images avec IA")
st.write(" üåê Extraction de contenu web")
st.write(" üíæ Gestion unifi√©e des donn√©es")
st.write(" ü§ñ Sous-mod√®les sklearn pour automatismes humains")
st.write(" üìö Fouille auto internet pour enrichir DB (p√©trole, topographie, sciences physiques, sous-sol)")
st.write("\nüöÄ UTILISATION:")
st.write(" Interface: Ex√©cutez les cellules suivantes")
st.write(" API: kibali_api.ask('votre question')")
st.write(" Auto-apprentissage: kibali_api.train_submodel()")
st.write(" Am√©lioration DB: kibali_api.improve_db(['p√©trole'])")
st.write(" Tests: test_all_features()")
st.write("\n‚öôÔ∏è MAINTENANCE:")
st.write(" Status: get_system_status()")
st.write(" Nettoyage: cleanup_old_cache()")
st.write(" Sauvegarde: backup_all_data()")
st.write("="*60)