#!/usr/bin/env python3
"""
Script CLI pour installer le mod√®le de code DeepSeek-Coder-1.3B
Utilis√© par Kibali en Mode Code Expert
"""

import os
import sys

def install_code_model():
    """Installe DeepSeek-Coder-1.3B-Instruct"""
    try:
        print("üöÄ Installation de DeepSeek-Coder-1.3B-Instruct...")
        print("üì¶ T√©l√©chargement en cours (~1.3GB)...\n")
        
        # D√©sactiver TensorFlow pour √©viter les imports lents
        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
        os.environ['TRANSFORMERS_NO_TF'] = '1'
        
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch
        
        model_name = "deepseek-ai/deepseek-coder-1.3b-instruct"
        cache_dir = "/root/.cache/huggingface/code_models"
        
        print(f"üìç Mod√®le: {model_name}")
        print(f"üìÇ Cache: {cache_dir}\n")
        
        # V√©rifier si d√©j√† install√©
        if os.path.exists(cache_dir) and os.listdir(cache_dir):
            print("‚ö†Ô∏è  Le mod√®le semble d√©j√† install√© dans le cache.")
            response = input("Voulez-vous le ret√©l√©charger ? (o/N): ")
            if response.lower() not in ['o', 'oui', 'y', 'yes']:
                print("‚úÖ Installation annul√©e")
                return True
        
        # Cr√©er le dossier cache
        os.makedirs(cache_dir, exist_ok=True)
        
        # T√©l√©charger tokenizer
        print("1Ô∏è‚É£ T√©l√©chargement du tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name, 
            cache_dir=cache_dir,
            trust_remote_code=True
        )
        print("   ‚úÖ Tokenizer t√©l√©charg√©\n")
        
        # T√©l√©charger mod√®le
        print("2Ô∏è‚É£ T√©l√©chargement du mod√®le (cela peut prendre 5-10 minutes)...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   üñ•Ô∏è  Device: {device}")
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=False,  # Changed: no custom code
            low_cpu_mem_usage=True,
            use_safetensors=True  # Use safetensors for faster loading
        )
        print("   ‚úÖ Mod√®le t√©l√©charg√©\n")
        
        # Test rapide
        print("3Ô∏è‚É£ Test du mod√®le...")
        test_prompt = "def fibonacci(n):"
        inputs = tokenizer(test_prompt, return_tensors="pt")
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                temperature=0.7,
                do_sample=True
            )
        
        generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"   Prompt: {test_prompt}")
        print(f"   G√©n√©r√©: {generated_code[:100]}...")
        print("   ‚úÖ Test r√©ussi\n")
        
        # Afficher la taille
        total_size = 0
        for root, dirs, files in os.walk(cache_dir):
            for file in files:
                filepath = os.path.join(root, file)
                total_size += os.path.getsize(filepath)
        
        size_mb = total_size / (1024 * 1024)
        print(f"üìä Taille totale du cache: {size_mb:.1f} MB")
        print(f"üìç Emplacement: {cache_dir}")
        print("\n‚úÖ ‚ú® Installation termin√©e avec succ√®s ! ‚ú®")
        print("\nüöÄ Utilisation:")
        print("   1. Ouvrez Kibali dans Streamlit")
        print("   2. S√©lectionnez 'Mode Code Expert'")
        print("   3. Le mod√®le sera automatiquement utilis√© pour g√©n√©rer du code\n")
        
        return True
        
    except ImportError as e:
        print(f"\n‚ùå Erreur: Module manquant")
        print(f"   {e}")
        print("\nüí° Solution:")
        print("   pip install transformers torch accelerate")
        return False
        
    except Exception as e:
        print(f"\n‚ùå Erreur lors de l'installation:")
        print(f"   {e}")
        print("\nüí° Suggestions:")
        print("   ‚Ä¢ V√©rifiez votre connexion Internet")
        print("   ‚Ä¢ V√©rifiez que vous avez ~2GB d'espace disque libre")
        print("   ‚Ä¢ R√©essayez avec: python install_code_model.py")
        return False

if __name__ == '__main__':
    print("=" * 60)
    print("  INSTALLATION DEEPSEEK-CODER-1.3B POUR KIBALI")
    print("=" * 60)
    print()
    
    success = install_code_model()
    
    sys.exit(0 if success else 1)
