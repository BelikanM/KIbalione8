#!/usr/bin/env python3
"""
Test des limites de tokens pour Kibali
V√©rifie que le mod√®le peut g√©n√©rer 3000 tokens
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import time

print("=" * 60)
print("TEST DES LIMITES DE TOKENS - KIBALI")
print("=" * 60)

# Chemins des mod√®les
model_name = "Qwen/Qwen2.5-1.5B-Instruct"
print(f"\nüì¶ Chargement du mod√®le: {model_name}")

try:
    # Charger le tokenizer
    print("\n1Ô∏è‚É£ Chargement du tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    print(f"   ‚úÖ Tokenizer charg√© (vocab size: {len(tokenizer)})")
    
    # Charger le mod√®le
    print("\n2Ô∏è‚É£ Chargement du mod√®le...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"   Device: {device.upper()}")
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        device_map="auto" if device == "cuda" else None,
        trust_remote_code=True
    )
    
    if device == "cpu":
        model = model.to(device)
    
    model.eval()
    print(f"   ‚úÖ Mod√®le charg√© sur {device.upper()}")
    
    # Test 1: G√©n√©ration courte (baseline)
    print("\n" + "=" * 60)
    print("TEST 1: G√©n√©ration courte (500 tokens)")
    print("=" * 60)
    
    prompt1 = """Explique-moi en d√©tail comment fonctionne la tomographie de r√©sistivit√© √©lectrique (ERT). 
D√©cris le principe physique, les √©quipements utilis√©s, et les applications en g√©ophysique."""
    
    messages1 = [
        {"role": "system", "content": "Tu es un expert en g√©ophysique."},
        {"role": "user", "content": prompt1}
    ]
    
    inputs1 = tokenizer.apply_chat_template(
        messages1,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(device)
    
    attention_mask1 = (inputs1 != tokenizer.pad_token_id).long().to(device)
    
    start1 = time.time()
    with torch.no_grad():
        outputs1 = model.generate(
            inputs1,
            attention_mask=attention_mask1,
            max_new_tokens=500,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )
    time1 = time.time() - start1
    
    response1 = tokenizer.decode(outputs1[0][inputs1.shape[1]:], skip_special_tokens=True)
    tokens1 = len(tokenizer.encode(response1))
    
    print(f"\nüìä R√©sultats:")
    print(f"   Temps: {time1:.2f}s")
    print(f"   Tokens g√©n√©r√©s: {tokens1}")
    print(f"   Longueur r√©ponse: {len(response1)} caract√®res")
    print(f"\nüìù D√©but de la r√©ponse:")
    print(f"   {response1[:300]}...")
    
    # Test 2: G√©n√©ration longue (3000 tokens)
    print("\n" + "=" * 60)
    print("TEST 2: G√©n√©ration longue (3000 tokens)")
    print("=" * 60)
    
    prompt2 = """Tu dois analyser en profondeur le fichier 'Projet Archange Ondimba 2.dat' qui contient des donn√©es de tomographie √©lectrique.

Donn√©es du fichier:
- Type: Tomographie de R√©sistivit√© √âlectrique (ERT)
- Format: Fichier .dat avec mesures de r√©sistivit√©
- Profondeur: 0 √† 50 m√®tres
- Valeurs de r√©sistivit√©: de 10 √† 5000 Ohm.m
- Points de mesure: 250 valeurs
- Localisation: Gabon, projet Archange Ondimba

Exemple de valeurs:
Profondeur 0m: 45.2, 52.1, 48.9 Ohm.m
Profondeur 5m: 78.3, 82.5, 91.2 Ohm.m
Profondeur 10m: 125.4, 132.8, 140.2 Ohm.m
Profondeur 15m: 245.6, 258.9, 267.3 Ohm.m

Fais une analyse COMPL√àTE et D√âTAILL√âE incluant:
1. Interpr√©tation g√©ologique des valeurs
2. Identification des couches et leur nature (argile, sable, roche)
3. Potentiel hydrog√©ologique 
4. Recommandations pour exploration
5. Analyse statistique des valeurs
6. Comparaison avec autres sites similaires
7. Suggestions d'investigations compl√©mentaires

Sois tr√®s d√©taill√© et explicite chaque point."""
    
    messages2 = [
        {"role": "system", "content": "Tu es un expert en g√©ophysique et hydrog√©ologie."},
        {"role": "user", "content": prompt2}
    ]
    
    inputs2 = tokenizer.apply_chat_template(
        messages2,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(device)
    
    attention_mask2 = (inputs2 != tokenizer.pad_token_id).long().to(device)
    
    print(f"\nüöÄ G√©n√©ration en cours avec max_new_tokens=3000...")
    start2 = time.time()
    with torch.no_grad():
        if device == "cuda":
            with torch.cuda.amp.autocast():
                outputs2 = model.generate(
                    inputs2,
                    attention_mask=attention_mask2,
                    max_new_tokens=3000,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=tokenizer.eos_token_id,
                    repetition_penalty=1.05,
                    use_cache=True
                )
        else:
            outputs2 = model.generate(
                inputs2,
                attention_mask=attention_mask2,
                max_new_tokens=3000,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                pad_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.05
            )
    time2 = time.time() - start2
    
    response2 = tokenizer.decode(outputs2[0][inputs2.shape[1]:], skip_special_tokens=True)
    tokens2 = len(tokenizer.encode(response2))
    
    print(f"\nüìä R√©sultats:")
    print(f"   Temps: {time2:.2f}s")
    print(f"   Tokens g√©n√©r√©s: {tokens2}")
    print(f"   Longueur r√©ponse: {len(response2)} caract√®res")
    print(f"   Ratio temps/token: {time2/tokens2*1000:.2f}ms/token")
    
    print(f"\nüìù R√©ponse COMPL√àTE:")
    print("=" * 60)
    print(response2)
    print("=" * 60)
    
    # V√©rifier si la r√©ponse est coup√©e
    if "..." in response2[-50:] or tokens2 < 2500:
        print("\n‚ö†Ô∏è  ATTENTION: La r√©ponse semble coup√©e!")
        print(f"   Tokens g√©n√©r√©s: {tokens2} (attendu: proche de 3000)")
    else:
        print("\n‚úÖ SUCCESS: R√©ponse compl√®te g√©n√©r√©e!")
        print(f"   {tokens2} tokens g√©n√©r√©s sur 3000 max")
    
    # Comparaison
    print("\n" + "=" * 60)
    print("COMPARAISON DES TESTS")
    print("=" * 60)
    print(f"Test 1 (500 tokens):  {tokens1} tokens en {time1:.2f}s")
    print(f"Test 2 (3000 tokens): {tokens2} tokens en {time2:.2f}s")
    print(f"Gain de longueur: +{tokens2-tokens1} tokens ({(tokens2/tokens1-1)*100:.1f}%)")
    
    print("\n‚úÖ Tests termin√©s avec succ√®s!")
    
except Exception as e:
    print(f"\n‚ùå ERREUR: {e}")
    import traceback
    traceback.print_exc()
